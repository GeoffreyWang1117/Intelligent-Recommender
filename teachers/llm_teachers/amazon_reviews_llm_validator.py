#!/usr/bin/env python3
"""
Amazon Reviews 2023æ•°æ®é›†å¤„ç†å’ŒLLMæ¨èéªŒè¯ç³»ç»Ÿ
Amazon Reviews 2023 Data Integration for LLM Recommendation Validation

åŠŸèƒ½:
1. é€šè¿‡Hugging Faceä¸‹è½½çœŸå®Amazon Reviews 2023æ•°æ®é›†
2. æ„å»ºç”¨æˆ·ç”»åƒå’Œå•†å“ç‰¹å¾
3. LLMæ¨èç”Ÿæˆå’ŒéªŒè¯
4. æ¨èç³»ç»Ÿæ ‡å‡†è¯„ä»·æŒ‡æ ‡éªŒè¯

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-27
"""

import pandas as pd
import numpy as np
import requests
import json
import time
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ•°æ®ä¸‹è½½å™¨
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥æ–°çš„æ•°æ®ä¸‹è½½å™¨
try:
    from data_processing.amazon_downloader import AmazonReviewsDownloader
    DOWNLOADER_AVAILABLE = True
except ImportError:
    DOWNLOADER_AVAILABLE = False
    print("âš ï¸  æ•°æ®ä¸‹è½½å™¨ä¸å¯ç”¨")

class AmazonReviewsLLMValidator:
    """åŸºäºçœŸå®Amazon Reviews 2023æ•°æ®çš„LLMæ¨èéªŒè¯å™¨"""
    
    def __init__(self, data_path: Optional[str] = None, ollama_endpoint: str = "http://localhost:11434/api/generate"):
        # ç¡®ä¿ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„data/amazonè·¯å¾„
        if data_path is None:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆä»å½“å‰æ–‡ä»¶å‘ä¸Šä¸¤çº§ï¼‰
            current_file = Path(__file__).resolve()
            project_root = current_file.parent.parent.parent
            self.data_path = project_root / "data" / "amazon"
        else:
            self.data_path = Path(data_path)
        
        self.data_path.mkdir(parents=True, exist_ok=True)
        self.ollama_endpoint = ollama_endpoint
        
        # åˆå§‹åŒ–ä¸‹è½½å™¨
        self.downloader = AmazonReviewsDownloader() if DOWNLOADER_AVAILABLE else None
        
        # LLMæ¨¡å‹é…ç½®
        self.primary_model = "llama3:latest"    # è‹±æ–‡ä¸»åŠ›
        self.secondary_model = "qwen3:latest"   # ä¸­æ–‡å¯¹ç…§
        
        # æ•°æ®å­˜å‚¨
        self.reviews = None
        self.items = None
        self.users = None
        self.user_profiles = {}
        self.item_features = {}
        
        # Amazon Reviews 2023æ”¯æŒçš„ç±»åˆ« (Hugging Face)
        self.supported_categories = [
            "All_Beauty",
            "Amazon_Fashion", 
            "Appliances",
            "Arts_Crafts_and_Sewing",
            "Automotive",
            "Baby_Products",
            "Beauty_and_Personal_Care",
            "Books",
            "CDs_and_Vinyl",
            "Cell_Phones_and_Accessories", 
            "Clothing_Shoes_and_Jewelry",
            "Digital_Music",
            "Electronics",
            "Gift_Cards",
            "Grocery_and_Gourmet_Food",
            "Handmade_Products",
            "Health_and_Household",
            "Health_and_Personal_Care",
            "Home_and_Garden", 
            "Home_and_Kitchen",
            "Industrial_and_Scientific",
            "Kindle_Store",
            "Magazine_Subscriptions",
            "Movies_and_TV",
            "Musical_Instruments",
            "Office_Products",
            "Patio_Lawn_and_Garden",
            "Pet_Supplies",
            "Software",
            "Sports_and_Outdoors",
            "Subscription_Boxes",
            "Tools_and_Home_Improvement",
            "Toys_and_Games",
            "Video_Games"
        ]
        
    def download_and_load_amazon_data(self, category: str = "Movies_and_TV", sample_size: int = 50000) -> bool:
        """
        é€šè¿‡Hugging Faceä¸‹è½½å¹¶åŠ è½½çœŸå®Amazon Reviews 2023å®Œæ•´æ•°æ®é›†
        
        Args:
            category: å•†å“ç±»åˆ«
            sample_size: é‡‡æ ·å¤§å°ï¼ˆå¢å¤§ä»¥è·å–æ›´å¤šçœŸå®æ•°æ®ï¼‰
        """
        if not DATASETS_AVAILABLE:
            print("âŒ datasetsåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install datasets")
            return False
        
        if category not in self.supported_categories:
            print(f"âŒ ä¸æ”¯æŒçš„ç±»åˆ«: {category}")
            print(f"æ”¯æŒçš„ç±»åˆ«: {self.supported_categories[:10]}...") # æ˜¾ç¤ºå‰10ä¸ª
            return False
        
        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        reviews_file = self.data_path / f"{category}_reviews.parquet"
        items_file = self.data_path / f"{category}_items.parquet"
        
        if reviews_file.exists() and items_file.exists():
            print(f"âœ… å‘ç°æœ¬åœ°ç¼“å­˜æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
            try:
                self.reviews = pd.read_parquet(reviews_file)
                self.items = pd.read_parquet(items_file)
                
                # æ„å»ºç”¨æˆ·æ•°æ®
                user_stats = self.reviews.groupby('user_id').agg({
                    'rating': ['count', 'mean', 'std'],
                    'timestamp': ['min', 'max']
                }).reset_index()
                user_stats.columns = ['user_id', 'review_count', 'avg_rating', 'rating_std', 'first_review', 'last_review']
                self.users = user_stats
                
                print(f"âœ… ä»ç¼“å­˜åŠ è½½æˆåŠŸ:")
                print(f"   ğŸ“Š è¯„è®ºæ•°: {len(self.reviews):,}")
                print(f"   ğŸ‘¥ ç”¨æˆ·æ•°: {len(self.users):,}")
                print(f"   ğŸ›ï¸  å•†å“æ•°: {len(self.items):,}")
                return True
                
            except Exception as e:
                print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶æŸåï¼Œé‡æ–°ä¸‹è½½: {e}")
        
        try:
            print(f"ğŸ”½ ä»Hugging Faceä¸‹è½½Amazon Reviews 2023å®Œæ•´æ•°æ®é›†: {category}")
            print(f"   ç›®æ ‡é‡‡æ ·æ•°: {sample_size:,} æ¡è¯„è®º")
            
            # ä¸‹è½½è¯„è®ºæ•°æ®
            print("   ğŸ”„ åŠ è½½è¯„è®ºæ•°æ®ï¼ˆæµå¼å¤„ç†ï¼‰...")
            try:
                # ä½¿ç”¨Hugging Face datasetsåº“ä¸‹è½½æ•°æ®
                dataset_name = f"McAuley-Lab/Amazon-Reviews-2023"
                
                # å°è¯•ä¸åŒçš„æ•°æ®é›†é…ç½®åç§°
                possible_names = [
                    f"raw_review_{category}",
                    f"review_{category}", 
                    category
                ]
                
                reviews_dataset = None
                for name in possible_names:
                    try:
                        reviews_dataset = load_dataset(
                            dataset_name, 
                            name=name,
                            split="full",
                            streaming=True,
                            trust_remote_code=True
                        )
                        print(f"   âœ… ä½¿ç”¨é…ç½®åç§°: {name}")
                        break
                    except Exception as e:
                        print(f"   âš ï¸  å°è¯•é…ç½® {name} å¤±è´¥: {str(e)[:100]}...")
                        continue
                
                if reviews_dataset is None:
                    print(f"âŒ æ‰€æœ‰é…ç½®éƒ½å¤±è´¥ï¼Œæ— æ³•åŠ è½½ {category} æ•°æ®")
                    return False
                
                # é‡‡æ ·æ•°æ® - ç¡®ä¿è·å–è¶³å¤Ÿçš„çœŸå®æ•°æ®
                reviews_data = []
                print(f"   ğŸ“Š å¼€å§‹é‡‡æ · {sample_size:,} æ¡è¯„è®º...")
                
                batch_size = 5000
                processed = 0
                
                for i, review in enumerate(reviews_dataset):
                    if i >= sample_size:
                        break
                    
                    # å¤„ç†è¯„è®ºæ•°æ® - æå–æ‰€æœ‰å¯ç”¨å­—æ®µ
                    review_item = {
                        'user_id': str(review.get('user_id', review.get('reviewerID', ''))),
                        'item_id': str(review.get('asin', review.get('parent_asin', ''))),
                        'rating': float(review.get('rating', review.get('overall', 0))),
                        'timestamp': review.get('timestamp', review.get('unixReviewTime', 0)),
                        'verified_purchase': review.get('verified_purchase', False),
                        'helpful_vote': review.get('helpful_vote', 0),
                        'text': str(review.get('text', review.get('reviewText', '')))[:1000]  # ä¿ç•™æ›´å¤šæ–‡æœ¬
                    }
                    
                    # åªä¿ç•™æœ‰æ•ˆçš„è¯„è®º
                    if (review_item['user_id'] and 
                        review_item['item_id'] and 
                        review_item['rating'] > 0):
                        reviews_data.append(review_item)
                        processed += 1
                    
                    if (i + 1) % batch_size == 0:
                        print(f"     å¤„ç†è¿›åº¦: {i+1:,}/{sample_size:,} ({(i+1)/sample_size*100:.1f}%), æœ‰æ•ˆè¯„è®º: {processed:,}")
                
                if not reviews_data:
                    print("âŒ æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆçš„è¯„è®ºæ•°æ®")
                    return False
                
                self.reviews = pd.DataFrame(reviews_data)
                print(f"   âœ… æˆåŠŸåŠ è½½ {len(self.reviews):,} æ¡çœŸå®è¯„è®º")
                print(f"   ğŸ“ˆ æ•°æ®è´¨é‡: ç”¨æˆ·æ•° {self.reviews['user_id'].nunique():,}, å•†å“æ•° {self.reviews['item_id'].nunique():,}")
                
            except Exception as e:
                print(f"âŒ è¯„è®ºæ•°æ®ä¸‹è½½å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False
            
            # ä¸‹è½½å•†å“å…ƒæ•°æ®
            print("   ğŸ”„ åŠ è½½å•†å“å…ƒæ•°æ®...")
            try:
                # å°è¯•ä¸åŒçš„å…ƒæ•°æ®é…ç½®
                meta_names = [
                    f"raw_meta_{category}",
                    f"meta_{category}",
                    f"{category}_meta"
                ]
                
                meta_dataset = None
                for name in meta_names:
                    try:
                        meta_dataset = load_dataset(
                            dataset_name,
                            name=name,
                            split="full",
                            streaming=True,
                            trust_remote_code=True
                        )
                        print(f"   âœ… ä½¿ç”¨å…ƒæ•°æ®é…ç½®: {name}")
                        break
                    except Exception as e:
                        print(f"   âš ï¸  å°è¯•å…ƒæ•°æ®é…ç½® {name} å¤±è´¥: {str(e)[:100]}...")
                        continue
                
                if meta_dataset is None:
                    print("âš ï¸  æ— æ³•åŠ è½½å…ƒæ•°æ®ï¼Œå°†åˆ›å»ºåŸºç¡€å•†å“ä¿¡æ¯")
                    self._create_basic_item_info()
                else:
                    # åªä¿ç•™è¯„è®ºä¸­å‡ºç°çš„å•†å“
                    item_ids = set(self.reviews['item_id'].unique())
                    items_data = []
                    
                    print(f"   ğŸ” åŒ¹é… {len(item_ids):,} ä¸ªå•†å“çš„å…ƒæ•°æ®...")
                    matched_count = 0
                    processed_meta = 0
                    
                    for i, item in enumerate(meta_dataset):
                        processed_meta += 1
                        asin = str(item.get('asin', item.get('parent_asin', '')))
                        
                        if asin in item_ids:
                            # æå–å®Œæ•´çš„å•†å“ä¿¡æ¯
                            item_info = {
                                'item_id': asin,
                                'title': str(item.get('title', ''))[:200],
                                'brand': str(item.get('brand', '')),
                                'category': item.get('category', item.get('categories', []))[:10],  # ä¿ç•™æ›´å¤šç±»åˆ«
                                'price': str(item.get('price', '')),
                                'description': str(item.get('description', []))[:500],
                                'features': item.get('features', item.get('feature', []))[:10],  # ä¿ç•™æ›´å¤šç‰¹å¾
                                'image_url': item.get('imageURL', item.get('images', [])),
                                'store': item.get('store', ''),
                                'main_category': item.get('main_category', '')
                            }
                            
                            items_data.append(item_info)
                            matched_count += 1
                            
                            if matched_count % 1000 == 0:
                                print(f"     åŒ¹é…è¿›åº¦: {matched_count:,}/{len(item_ids):,} ({matched_count/len(item_ids)*100:.1f}%)")
                        
                        # é™åˆ¶å…ƒæ•°æ®å¤„ç†æ•°é‡ä»¥é¿å…æ— é™å¾ªç¯
                        if processed_meta > len(item_ids) * 3 or matched_count >= len(item_ids) * 0.9:
                            break
                    
                    self.items = pd.DataFrame(items_data)
                    print(f"   âœ… æˆåŠŸåŒ¹é… {len(self.items):,} ä¸ªå•†å“ (åŒ¹é…ç‡: {len(self.items)/len(item_ids)*100:.1f}%)")
                    
                    # ä¸ºæœªåŒ¹é…çš„å•†å“åˆ›å»ºåŸºç¡€ä¿¡æ¯
                    if len(self.items) < len(item_ids):
                        missing_items = item_ids - set(self.items['item_id'])
                        print(f"   ğŸ“ ä¸º {len(missing_items):,} ä¸ªç¼ºå¤±å•†å“åˆ›å»ºåŸºç¡€ä¿¡æ¯...")
                        
                        basic_items = pd.DataFrame({
                            'item_id': list(missing_items),
                            'title': [f"{category.replace('_', ' ')} Product {item[-6:]}" for item in missing_items],
                            'brand': ['Unknown'] * len(missing_items),
                            'category': [[category.replace('_', ' ')]] * len(missing_items),
                            'main_category': [category.replace('_', ' ')] * len(missing_items)
                        })
                        
                        self.items = pd.concat([self.items, basic_items], ignore_index=True)
                        print(f"   ï¿½ æ€»å•†å“æ•°: {len(self.items):,}")
                
            except Exception as e:
                print(f"âš ï¸  å•†å“å…ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
                self._create_basic_item_info()
            
            # æ„å»ºç”¨æˆ·æ•°æ®
            print("   ğŸ‘¥ æ„å»ºç”¨æˆ·æ¡£æ¡ˆ...")
            user_stats = self.reviews.groupby('user_id').agg({
                'rating': ['count', 'mean', 'std'],
                'timestamp': ['min', 'max'],
                'verified_purchase': 'mean'
            }).reset_index()
            
            user_stats.columns = ['user_id', 'review_count', 'avg_rating', 'rating_std', 'first_review', 'last_review', 'verified_rate']
            self.users = user_stats
            print(f"   âœ… æ„å»ºäº† {len(self.users):,} ä¸ªç”¨æˆ·æ¡£æ¡ˆ")
            
            # è½»é‡çº§æ•°æ®æ¸…æ´— - åªç§»é™¤æ˜æ˜¾æ— æ•ˆçš„æ•°æ®
            self._minimal_clean_data()
            
            # ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
            try:
                print(f"   ğŸ’¾ ä¿å­˜æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜...")
                reviews_file = self.data_path / f"{category}_reviews.parquet"
                items_file = self.data_path / f"{category}_items.parquet"
                
                self.reviews.to_parquet(reviews_file, index=False)
                self.items.to_parquet(items_file, index=False)
                print(f"   âœ… æ•°æ®å·²ä¿å­˜åˆ° {self.data_path}")
            except Exception as e:
                print(f"   âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            
            print(f"ğŸ‰ Amazon ReviewsçœŸå®æ•°æ®åŠ è½½å®Œæˆ:")
            print(f"   ç±»åˆ«: {category}")
            print(f"   è¯„è®ºæ•°: {len(self.reviews):,}")
            print(f"   ç”¨æˆ·æ•°: {len(self.users):,}")
            print(f"   å•†å“æ•°: {len(self.items):,}")
            print(f"   è¯„åˆ†åˆ†å¸ƒ: {dict(self.reviews['rating'].value_counts().sort_index())}")
            print(f"   æ•°æ®å¯†åº¦: {len(self.reviews)/(len(self.users)*len(self.items))*100:.4f}%")
            
            return True
            
        except Exception as e:
            print(f"âŒ çœŸå®æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _create_basic_item_info(self):
        """ä¸ºå•†å“åˆ›å»ºåŸºç¡€ä¿¡æ¯"""
        unique_items = self.reviews['item_id'].unique()
        self.items = pd.DataFrame({
            'item_id': unique_items,
            'title': [f"Product {item[-6:]}" for item in unique_items],
            'brand': ['Unknown'] * len(unique_items),
            'category': [['Unknown']] * len(unique_items),
            'main_category': ['Unknown'] * len(unique_items)
        })
        print(f"   ğŸ“ åˆ›å»ºäº† {len(self.items):,} ä¸ªåŸºç¡€å•†å“ä¿¡æ¯")
    
    def _minimal_clean_data(self):
        """è½»é‡çº§æ•°æ®æ¸…æ´— - ä¿ç•™æ›´å¤šçœŸå®æ•°æ®"""
        print("ğŸ§¹ è½»é‡çº§æ•°æ®æ¸…æ´—...")
        
        if self.reviews is None or len(self.reviews) == 0:
            print("   âŒ æ²¡æœ‰è¯„è®ºæ•°æ®éœ€è¦æ¸…æ´—")
            return
        
        initial_reviews = len(self.reviews)
        
        # åªç§»é™¤æ˜æ˜¾æ— æ•ˆçš„æ•°æ®
        self.reviews = self.reviews.dropna(subset=['user_id', 'item_id', 'rating'])
        self.reviews = self.reviews[self.reviews['rating'] > 0]
        self.reviews = self.reviews[self.reviews['rating'] <= 5]  # ç¡®ä¿è¯„åˆ†åœ¨1-5èŒƒå›´å†…
        self.reviews = self.reviews[self.reviews['user_id'] != '']
        self.reviews = self.reviews[self.reviews['item_id'] != '']
        
        # éå¸¸å®½æ¾çš„è¿‡æ»¤ - åªç§»é™¤æ˜æ˜¾çš„å¼‚å¸¸æ•°æ®
        user_counts = self.reviews['user_id'].value_counts()
        item_counts = self.reviews['item_id'].value_counts()
        
        # åªç§»é™¤åªæœ‰1ä¸ªè¯„è®ºçš„æç«¯æƒ…å†µ
        active_users = user_counts[user_counts >= 1].index
        popular_items = item_counts[item_counts >= 1].index
        
        self.reviews = self.reviews[
            (self.reviews['user_id'].isin(active_users)) & 
            (self.reviews['item_id'].isin(popular_items))
        ]
        
        print(f"   æ¸…æ´—å‰: {initial_reviews:,} æ¡è¯„è®º")
        print(f"   æ¸…æ´—å: {len(self.reviews):,} æ¡è¯„è®º")
        print(f"   ä¿ç•™ç‡: {len(self.reviews)/initial_reviews*100:.1f}%")
    
    def load_amazon_data(self, category: str = "Movies_and_TV", sample_size: int = 50000) -> bool:
        """
        åŠ è½½Amazon Reviewsæ•°æ®é›† - åªä½¿ç”¨çœŸå®æ•°æ®ï¼Œä¸ç”Ÿæˆå‡æ•°æ®
        
        Args:
            category: å•†å“ç±»åˆ«
            sample_size: é‡‡æ ·å¤§å°ï¼ˆå¢å¤§ä»¥è·å–æ›´å¤šçœŸå®æ•°æ®ï¼‰
        """
        print(f"ğŸ“Š åŠ è½½Amazon ReviewsçœŸå®æ•°æ®: {category}")
        print(f"âš ï¸  æ³¨æ„: æ­¤æ–¹æ³•åªä½¿ç”¨çœŸå®æ•°æ®ï¼Œä¸ä¼šç”Ÿæˆå‡æ•°æ®")
        
        # åªå°è¯•Hugging FaceçœŸå®æ•°æ®
        success = self.download_and_load_amazon_data(category, sample_size)
        
        if not success:
            print("âŒ çœŸå®æ•°æ®åŠ è½½å¤±è´¥")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. å°è¯•ä¸åŒçš„categoryåç§°")
            print("   3. å‡å°‘sample_size")
            print(f"   4. æ”¯æŒçš„ç±»åˆ«: {self.supported_categories[:5]}...")
            return False
        
        if self.reviews is None or len(self.reviews) == 0:
            print("âŒ åŠ è½½çš„æ•°æ®ä¸ºç©º")
            return False
        
        print(f"âœ… çœŸå®Amazon Reviewsæ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"   ğŸ“Š æ•°æ®è§„æ¨¡: {len(self.reviews):,} æ¡è¯„è®º")
        print(f"   ğŸ‘¥ ç”¨æˆ·è§„æ¨¡: {len(self.users):,} ä¸ªç”¨æˆ·") 
        print(f"   ğŸ›ï¸  å•†å“è§„æ¨¡: {len(self.items):,} ä¸ªå•†å“")
        
        return True

    def _clean_data(self):
        """æ•°æ®æ¸…æ´—"""
        print("ğŸ§¹ æ•°æ®æ¸…æ´—...")
        
        if self.reviews is None or len(self.reviews) == 0:
            print("   âŒ æ²¡æœ‰è¯„è®ºæ•°æ®éœ€è¦æ¸…æ´—")
            return
        
        # ç§»é™¤ç©ºå€¼
        initial_reviews = len(self.reviews)
        self.reviews = self.reviews.dropna(subset=['user_id', 'item_id', 'rating'])
        self.reviews = self.reviews[self.reviews['rating'] > 0]  # ç§»é™¤æ— æ•ˆè¯„åˆ†
        
        # æ”¾å®½è¿‡æ»¤æ¡ä»¶ - è¿‡æ»¤æå°‘æ´»è·ƒçš„ç”¨æˆ·å’Œå•†å“
        user_counts = self.reviews['user_id'].value_counts()
        item_counts = self.reviews['item_id'].value_counts()
        
        active_users = user_counts[user_counts >= 2].index  # è‡³å°‘2ä¸ªè¯„è®º
        popular_items = item_counts[item_counts >= 1].index  # è‡³å°‘1ä¸ªè¯„è®º
        
        self.reviews = self.reviews[
            (self.reviews['user_id'].isin(active_users)) & 
            (self.reviews['item_id'].isin(popular_items))
        ]
        
        print(f"   æ¸…æ´—å‰: {initial_reviews} æ¡è¯„è®º")
        print(f"   æ¸…æ´—å: {len(self.reviews)} æ¡è¯„è®º")

    def build_user_profiles(self) -> Dict:
        """æ„å»ºç”¨æˆ·ç”»åƒ"""
        print("ğŸ‘¤ æ„å»ºç”¨æˆ·ç”»åƒ...")
        
        self.user_profiles = {}
        
        for user_id in self.users['user_id'].unique():
            user_info = self.users[self.users['user_id'] == user_id].iloc[0]
            user_reviews = self.reviews[self.reviews['user_id'] == user_id]
            
            # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            profile = {
                'user_id': user_id,
                'total_ratings': len(user_reviews),
                'avg_rating': user_reviews['rating'].mean(),
                'rating_std': user_reviews['rating'].std(),
                'purchase_history': user_reviews['item_id'].tolist(),
                'preferred_categories': [],
                'brand_preferences': {},
                'rating_distribution': user_reviews['rating'].value_counts().to_dict()
            }
            
            # å•†å“åå¥½åˆ†æ
            if len(user_reviews) > 0 and hasattr(self, 'items') and self.items is not None:
                user_items = self.items[self.items['item_id'].isin(user_reviews['item_id'])]
                
                # å“ç‰Œåå¥½
                if 'brand' in user_items.columns:
                    brand_ratings = {}
                    for _, item in user_items.iterrows():
                        brand = item['brand']
                        item_rating = user_reviews[user_reviews['item_id'] == item['item_id']]['rating'].iloc[0]
                        if brand not in brand_ratings:
                            brand_ratings[brand] = []
                        brand_ratings[brand].append(item_rating)
                    
                    # è®¡ç®—å“ç‰Œå¹³å‡è¯„åˆ†
                    profile['brand_preferences'] = {
                        brand: np.mean(ratings) for brand, ratings in brand_ratings.items()
                    }
                
                # ç±»åˆ«åå¥½
                if 'category' in user_items.columns:
                    categories = []
                    for _, item in user_items.iterrows():
                        if isinstance(item['category'], list):
                            categories.extend(item['category'])
                        elif isinstance(item['category'], str):
                            categories.append(item['category'])
                    profile['preferred_categories'] = list(set(categories))
            
            self.user_profiles[user_id] = profile
        
        print(f"âœ… æˆåŠŸæ„å»º {len(self.user_profiles)} ä¸ªç”¨æˆ·ç”»åƒ")
        return self.user_profiles

    def generate_llm_recommendation(self, user_id: str, model: str = "llama3:latest", 
                                  candidate_items: Optional[List[str]] = None, k: int = 10) -> List[str]:
        """ä¸ºæŒ‡å®šç”¨æˆ·ç”ŸæˆLLMæ¨è"""
        
        if user_id not in self.user_profiles:
            return []
        
        user_profile = self.user_profiles[user_id]
        
        # è·å–å€™é€‰å•†å“
        if candidate_items is None:
            # æ’é™¤ç”¨æˆ·å·²è´­ä¹°çš„å•†å“
            purchased_items = set(user_profile['purchase_history'])
            all_items = set(self.items['item_id'])
            candidate_items = list(all_items - purchased_items)
            
            # é™åˆ¶å€™é€‰å•†å“æ•°é‡
            if len(candidate_items) > 50:
                candidate_items = np.random.choice(candidate_items, 50, replace=False).tolist()
        
        # æ„å»ºå•†å“ä¿¡æ¯
        candidate_info = []
        for item_id in candidate_items[:30]:  # é™åˆ¶æç¤ºé•¿åº¦
            item_data = self.items[self.items['item_id'] == item_id]
            if not item_data.empty:
                item = item_data.iloc[0]
                candidate_info.append({
                    'id': item_id,
                    'title': item['title'][:100],  # é™åˆ¶é•¿åº¦
                    'brand': item['brand'],
                    'category': item['category'][:3] if isinstance(item['category'], list) else []
                })
        
        # æ„å»ºLLMæç¤º
        prompt = self._build_amazon_prompt(user_profile, candidate_info, k)
        
        try:
            # è°ƒç”¨LLM
            response = requests.post(
                self.ollama_endpoint,
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_predict": 500
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                llm_response = response.json().get('response', '')
                recommendations = self._parse_amazon_recommendations(llm_response, [item['id'] for item in candidate_info])
                return recommendations[:k]
            else:
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ LLMæ¨èç”Ÿæˆå¼‚å¸¸: {e}")
            return []

    def _build_amazon_prompt(self, user_profile: Dict, candidate_items: List[Dict], k: int) -> str:
        """æ„å»ºAmazonæ¨èçš„LLMæç¤º"""
        
        # ç”¨æˆ·åå¥½æ€»ç»“
        user_summary = f"""ç”¨æˆ·ç”»åƒ:
- æ€»è¯„è®ºæ•°: {user_profile['total_ratings']}
- å¹³å‡è¯„åˆ†: {user_profile['avg_rating']:.2f}
- åå¥½å“ç‰Œ: {', '.join(list(user_profile['brand_preferences'].keys())[:5])}
- åå¥½ç±»åˆ«: {', '.join(user_profile['preferred_categories'][:5])}
- è¯„åˆ†ä¹ æƒ¯: é«˜è¯„åˆ†({user_profile['rating_distribution'].get(5, 0)}ä¸ª), ä¸­è¯„åˆ†({user_profile['rating_distribution'].get(4, 0)}ä¸ª)
"""
        
        # å€™é€‰å•†å“ä¿¡æ¯
        items_info = "å€™é€‰å•†å“:\n"
        for i, item in enumerate(candidate_items[:20], 1):
            items_info += f"{i}. ID: {item['id']}, æ ‡é¢˜: {item['title']}, å“ç‰Œ: {item['brand']}, ç±»åˆ«: {', '.join(item['category'])}\n"
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Amazonå•†å“æ¨èç³»ç»Ÿã€‚åŸºäºç”¨æˆ·çš„å†å²è¡Œä¸ºå’Œåå¥½ï¼Œä»å€™é€‰å•†å“ä¸­æ¨èæœ€é€‚åˆçš„{k}ä¸ªå•†å“ã€‚

{user_summary}

{items_info}

è¯·æ ¹æ®ç”¨æˆ·çš„åå¥½å’Œè´­ä¹°å†å²ï¼Œæ¨è{k}ä¸ªæœ€åˆé€‚çš„å•†å“ã€‚åªè¿”å›å•†å“IDï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

æ¨èå•†å“ID:"""
        
        return prompt

    def _parse_amazon_recommendations(self, llm_response: str, candidate_items: List[str]) -> List[str]:
        """è§£æLLMè¿”å›çš„Amazonæ¨èç»“æœ"""
        
        recommendations = []
        
        # å°è¯•ä»å“åº”ä¸­æå–å•†å“ID
        lines = llm_response.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line:
                # å¯»æ‰¾é€—å·åˆ†éš”çš„ID
                potential_ids = [item.strip() for item in line.split(',')]
                for item_id in potential_ids:
                    # æ¸…ç†IDæ ¼å¼
                    item_id = item_id.replace('"', '').replace("'", '').strip()
                    if item_id in candidate_items and item_id not in recommendations:
                        recommendations.append(item_id)
        
        return recommendations

def test_amazon_validator():
    """æµ‹è¯•Amazon ReviewséªŒè¯å™¨ - åªä½¿ç”¨çœŸå®æ•°æ®"""
    print("ğŸ§ª æµ‹è¯•Amazon Reviews LLMéªŒè¯å™¨ï¼ˆçœŸå®æ•°æ®ï¼‰")
    print("=" * 60)
    
    validator = AmazonReviewsLLMValidator()
    
    # æµ‹è¯•ä¸åŒçš„æ•°æ®é›†ç±»åˆ« - ä»æœ€å¯èƒ½æˆåŠŸçš„å¼€å§‹
    test_categories = ["Books", "Movies_and_TV", "Electronics", "Amazon_Fashion"]
    
    for category in test_categories:
        print(f"\nğŸ“š æµ‹è¯•ç±»åˆ«: {category}")
        print("-" * 40)
        
        # å°è¯•åŠ è½½çœŸå®æ•°æ® (ä½¿ç”¨è¾ƒå¤§çš„é‡‡æ ·ä»¥è·å¾—è¶³å¤Ÿæ•°æ®)
        if validator.load_amazon_data(category, sample_size=10000):
            print(f"âœ… {category} çœŸå®æ•°æ®åŠ è½½æˆåŠŸ")
            
            # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
            print(f"   ğŸ“Š è¯„è®ºæ•°: {len(validator.reviews):,}")
            print(f"   ğŸ‘¥ ç”¨æˆ·æ•°: {len(validator.users):,}")
            print(f"   ğŸ›ï¸  å•†å“æ•°: {len(validator.items):,}")
            print(f"   â­ è¯„åˆ†åˆ†å¸ƒ: {dict(validator.reviews['rating'].value_counts().sort_index())}")
            
            # æ„å»ºç”¨æˆ·ç”»åƒ
            if validator.build_user_profiles():
                print("âœ… ç”¨æˆ·ç”»åƒæ„å»ºæˆåŠŸ")
                
                # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•ç”¨æˆ·è¿›è¡Œæ¨è
                if validator.user_profiles:
                    test_user = list(validator.user_profiles.keys())[0]
                    print(f"ğŸ¯ æµ‹è¯•ç”¨æˆ·: {test_user}")
                    
                    # æ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯
                    user_profile = validator.user_profiles[test_user]
                    print(f"   ğŸ“ˆ ç”¨æˆ·ç»Ÿè®¡: {user_profile['total_ratings']} ä¸ªè¯„è®º, å¹³å‡è¯„åˆ† {user_profile['avg_rating']:.2f}")
                    
                    # ç”Ÿæˆæ¨è
                    recommendations = validator.generate_llm_recommendation(test_user, k=5)
                    print(f"ğŸ“ LLMæ¨èç»“æœ: {recommendations}")
                    
                    # æˆåŠŸæµ‹è¯•ä¸€ä¸ªç±»åˆ«åé€€å‡º
                    print(f"\nğŸ‰ {category} æµ‹è¯•å®Œæˆï¼")
                    return
                else:
                    print("âŒ æ²¡æœ‰ç”¨æˆ·ç”»åƒ")
            else:
                print("âŒ ç”¨æˆ·ç”»åƒæ„å»ºå¤±è´¥")
        else:
            print(f"âŒ {category} çœŸå®æ•°æ®åŠ è½½å¤±è´¥")
    
    print("\nâŒ æ‰€æœ‰ç±»åˆ«çš„çœŸå®æ•°æ®åŠ è½½éƒ½å¤±è´¥äº†")
    print("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒHugging Faceè®¿é—®")

if __name__ == "__main__":
    test_amazon_validator()


