#!/usr/bin/env python3
"""
LLMæ¨èç³»ç»Ÿå®Œæ•´éªŒè¯å®éªŒ
Complete LLM Recommendation System Validation Experiment

åŠŸèƒ½:
1. ä½¿ç”¨çœŸå®MovieLensæ•°æ®
2. ç”ŸæˆLLMæ¨èç»“æœ
3. ä½¿ç”¨æ ‡å‡†æ¨èç³»ç»Ÿè¯„ä»·æŒ‡æ ‡éªŒè¯æ•ˆæœ
4. å¯¹æ¯”åŒè¯­LLMæ¨¡å‹æ€§èƒ½
5. è¾“å‡ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-18
"""

import sys
import os
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from movielens_llm_validator import MovieLensLLMValidator
from recommendation_metrics import RecommendationMetrics

class CompleteLLMValidationExperiment:
    """å®Œæ•´çš„LLMæ¨èç³»ç»ŸéªŒè¯å®éªŒ"""
    
    def __init__(self, output_dir: str = "experiment_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.validator = MovieLensLLMValidator()
        self.metrics = RecommendationMetrics()
        
        # å®éªŒé…ç½®
        self.test_config = {
            "test_users_count": 10,         # æµ‹è¯•ç”¨æˆ·æ•°é‡
            "min_ratings_per_user": 10,     # æ¯ä¸ªç”¨æˆ·æœ€å°‘è¯„åˆ†æ•°
            "candidate_movies_count": 30,   # å€™é€‰ç”µå½±æ•°é‡
            "recommendation_count": 10,     # æ¨èç”µå½±æ•°é‡
            "relevance_threshold": 4.0,     # ç›¸å…³æ€§é˜ˆå€¼ (>=4.0è®¤ä¸ºç›¸å…³)
            "test_ratio": 0.2              # æµ‹è¯•é›†æ¯”ä¾‹
        }
        
        self.experiment_results = {}
    
    def run_complete_experiment(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„éªŒè¯å®éªŒ"""
        print("ğŸ§ª å¼€å§‹LLMæ¨èç³»ç»Ÿå®Œæ•´éªŒè¯å®éªŒ")
        print("=" * 60)
        
        # Step 1: æ•°æ®å‡†å¤‡
        if not self._prepare_data():
            print("âŒ æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œå®éªŒç»ˆæ­¢")
            return {}
        
        # Step 2: é€‰æ‹©æµ‹è¯•ç”¨æˆ·
        test_users = self._select_test_users()
        if not test_users:
            print("âŒ æµ‹è¯•ç”¨æˆ·é€‰æ‹©å¤±è´¥ï¼Œå®éªŒç»ˆæ­¢")
            return {}
        
        # Step 3: åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
        train_data, test_data = self._split_train_test(test_users)
        
        # Step 4: ç”ŸæˆLLMæ¨è
        llm_recommendations = self._generate_llm_recommendations(test_users, train_data)
        
        # Step 5: è¯„ä¼°æ¨èæ•ˆæœ
        evaluation_results = self._evaluate_recommendations(
            test_users, test_data, llm_recommendations
        )
        
        # Step 6: ä¿å­˜å®éªŒç»“æœ
        self._save_experiment_results(evaluation_results)
        
        # Step 7: ç”ŸæˆæŠ¥å‘Š
        self._generate_experiment_report(evaluation_results)
        
        return evaluation_results
    
    def _prepare_data(self) -> bool:
        """å‡†å¤‡å®éªŒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡å®éªŒæ•°æ®...")
        
        # ä¼˜å…ˆå°è¯•åŠ è½½sampleæ•°æ®
        if self.validator.load_movielens_data("sample"):
            print("âœ… æˆåŠŸåŠ è½½data/movielensä¸­çš„MovieLensæ•°æ®")
        elif self.validator.load_movielens_data("small"):
            print("âœ… æˆåŠŸåŠ è½½æ ‡å‡†MovieLens 100Kæ•°æ®")
        else:
            print("âš ï¸  çœŸå®MovieLensæ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ·æœ¬æ•°æ®")
            if not self.validator.create_sample_data():
                return False
        
        # æ„å»ºç”¨æˆ·ç”»åƒ
        user_profiles = self.validator.build_user_profiles()
        if not user_profiles:
            print("âŒ ç”¨æˆ·ç”»åƒæ„å»ºå¤±è´¥")
            return False
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(user_profiles)}ä¸ªç”¨æˆ·ç”»åƒ")
        return True
    
    def _select_test_users(self) -> List[int]:
        """é€‰æ‹©ç¬¦åˆæ¡ä»¶çš„æµ‹è¯•ç”¨æˆ·"""
        print("ğŸ‘¥ é€‰æ‹©æµ‹è¯•ç”¨æˆ·...")
        
        qualified_users = []
        for user_id, profile in self.validator.user_profiles.items():
            if profile['total_ratings'] >= self.test_config['min_ratings_per_user']:
                qualified_users.append(user_id)
        
        # éšæœºé€‰æ‹©æµ‹è¯•ç”¨æˆ·
        if len(qualified_users) > self.test_config['test_users_count']:
            test_users = np.random.choice(
                qualified_users, 
                self.test_config['test_users_count'], 
                replace=False
            ).tolist()
        else:
            test_users = qualified_users
        
        print(f"âœ… é€‰æ‹©äº†{len(test_users)}ä¸ªæµ‹è¯•ç”¨æˆ·")
        return test_users
    
    def _split_train_test(self, test_users: List[int]) -> Tuple[Dict, Dict]:
        """åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        print("âœ‚ï¸  åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†...")
        
        train_data = {}
        test_data = {}
        
        for user_id in test_users:
            user_ratings = self.validator.ratings[
                self.validator.ratings['user_id'] == user_id
            ].copy()
            
            # æŒ‰æ—¶é—´æ’åº
            user_ratings = user_ratings.sort_values('timestamp')
            
            # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•
            n_test = max(1, int(len(user_ratings) * self.test_config['test_ratio']))
            
            train_ratings = user_ratings[:-n_test]
            test_ratings = user_ratings[-n_test:]
            
            train_data[user_id] = train_ratings
            test_data[user_id] = test_ratings
        
        total_train = sum(len(ratings) for ratings in train_data.values())
        total_test = sum(len(ratings) for ratings in test_data.values())
        
        print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒ{total_train}æ¡, æµ‹è¯•{total_test}æ¡")
        return train_data, test_data
    
    def _generate_llm_recommendations(self, test_users: List[int], 
                                    train_data: Dict) -> Dict[str, Dict]:
        """ç”ŸæˆLLMæ¨èç»“æœ"""
        print("ğŸ¤– ç”ŸæˆLLMæ¨è...")
        
        recommendations = {
            "llama3": {},
            "qwen3": {},
            "generation_info": {}
        }
        
        for i, user_id in enumerate(test_users, 1):
            print(f"  å¤„ç†ç”¨æˆ· {user_id} ({i}/{len(test_users)})")
            
            # è·å–å€™é€‰ç”µå½± (ç”¨æˆ·åœ¨è®­ç»ƒé›†ä¸­æœªè¯„åˆ†çš„ç”µå½±)
            train_movies = set(train_data[user_id]['movie_id']) if user_id in train_data else set()
            all_movies = set(self.validator.movies['movie_id'])
            candidate_movie_ids = list(all_movies - train_movies)
            
            # é™åˆ¶å€™é€‰æ•°é‡ä»¥æ§åˆ¶è®¡ç®—æ—¶é—´
            if len(candidate_movie_ids) > self.test_config['candidate_movies_count']:
                candidate_movie_ids = np.random.choice(
                    candidate_movie_ids, 
                    self.test_config['candidate_movies_count'], 
                    replace=False
                ).tolist()
            
            # æ„å»ºå€™é€‰ç”µå½±ä¿¡æ¯
            candidate_movies = []
            for movie_id in candidate_movie_ids:
                movie_info = self.validator.movies[
                    self.validator.movies['movie_id'] == movie_id
                ].iloc[0]
                candidate_movies.append({
                    'movie_id': movie_id,
                    'title': movie_info['title'],
                    'genres': self._extract_genres(movie_info)
                })
            
            # ç”ŸæˆLlama3æ¨è
            print(f"    ğŸ‡ºğŸ‡¸ Llama3æ¨è...")
            llama3_result = self.validator.get_llm_recommendations(
                user_id, candidate_movies, "primary", 
                top_k=self.test_config['recommendation_count']
            )
            recommendations["llama3"][user_id] = llama3_result
            
            # ç”ŸæˆQwen3æ¨è
            print(f"    ğŸ‡¨ğŸ‡³ Qwen3æ¨è...")
            qwen3_result = self.validator.get_llm_recommendations(
                user_id, candidate_movies, "secondary",
                top_k=self.test_config['recommendation_count']
            )
            recommendations["qwen3"][user_id] = qwen3_result
            
            # è®°å½•ç”Ÿæˆä¿¡æ¯
            recommendations["generation_info"][user_id] = {
                "candidate_count": len(candidate_movies),
                "llama3_status": llama3_result.get("status", "unknown"),
                "qwen3_status": qwen3_result.get("status", "unknown")
            }
        
        # ç»Ÿè®¡ç”ŸæˆæˆåŠŸç‡
        llama3_success = sum(1 for info in recommendations["generation_info"].values() 
                           if info["llama3_status"] == "success")
        qwen3_success = sum(1 for info in recommendations["generation_info"].values() 
                          if info["qwen3_status"] == "success")
        
        print(f"âœ… LLMæ¨èç”Ÿæˆå®Œæˆ:")
        print(f"   Llama3æˆåŠŸç‡: {llama3_success}/{len(test_users)} ({llama3_success/len(test_users)*100:.1f}%)")
        print(f"   Qwen3æˆåŠŸç‡: {qwen3_success}/{len(test_users)} ({qwen3_success/len(test_users)*100:.1f}%)")
        
        return recommendations
    
    def _extract_genres(self, movie_info) -> str:
        """æå–ç”µå½±ç±»å‹ä¿¡æ¯"""
        genre_names = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 
                      'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 
                      'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
        
        genres = []
        for i, genre in enumerate(genre_names):
            if i < len([col for col in movie_info.index if col.startswith('genre_')]):
                genre_col = f'genre_{i}'
                if genre_col in movie_info.index and movie_info[genre_col] == 1:
                    genres.append(genre)
        
        return ', '.join(genres) if genres else 'Unknown'
    
    def _evaluate_recommendations(self, test_users: List[int], 
                                test_data: Dict, 
                                llm_recommendations: Dict) -> Dict:
        """è¯„ä¼°æ¨èæ•ˆæœ"""
        print("ğŸ“ è¯„ä¼°æ¨èæ•ˆæœ...")
        
        evaluation_results = {}
        
        for model_name in ["llama3", "qwen3"]:
            print(f"  è¯„ä¼°{model_name}æ¨¡å‹...")
            
            # å‡†å¤‡è¯„ä¼°æ•°æ®
            eval_data = self._prepare_evaluation_data(
                test_users, test_data, llm_recommendations[model_name]
            )
            
            if not eval_data:
                print(f"    âŒ {model_name}è¯„ä¼°æ•°æ®å‡†å¤‡å¤±è´¥")
                continue
            
            # æ‰§è¡Œç»¼åˆè¯„ä¼°
            results = self.metrics.comprehensive_evaluation(
                eval_data["true_ratings"],
                eval_data["predicted_ratings"], 
                eval_data["test_users_relevant"],
                eval_data["recommendations"],
                eval_data["item_features"],
                eval_data["item_popularity"],
                eval_data["all_items"]
            )
            
            evaluation_results[model_name] = results
            
            print(f"    âœ… {model_name}è¯„ä¼°å®Œæˆ")
        
        return evaluation_results
    
    def _prepare_evaluation_data(self, test_users: List[int], 
                               test_data: Dict, 
                               model_recommendations: Dict) -> Dict:
        """å‡†å¤‡è¯„ä¼°æ‰€éœ€çš„æ•°æ®æ ¼å¼"""
        
        # æ”¶é›†çœŸå®è¯„åˆ†
        true_ratings = []
        for user_id in test_users:
            if user_id in test_data:
                for _, row in test_data[user_id].iterrows():
                    true_ratings.append((user_id, row['movie_id'], row['rating']))
        
        # æ”¶é›†é¢„æµ‹è¯„åˆ†
        predicted_ratings = []
        recommendations = {}
        
        for user_id in test_users:
            if (user_id in model_recommendations and 
                model_recommendations[user_id].get("status") == "success"):
                
                recs = model_recommendations[user_id]["recommendations"]
                user_recs = []
                
                for rec in recs:
                    movie_id = rec.get("movie_id")
                    predicted_rating = rec.get("predicted_rating", 3.0)
                    
                    if movie_id:
                        predicted_ratings.append((user_id, movie_id, predicted_rating))
                        user_recs.append(movie_id)
                
                recommendations[user_id] = user_recs
        
        # æ”¶é›†ç›¸å…³ç‰©å“ (æµ‹è¯•é›†ä¸­è¯„åˆ†>=é˜ˆå€¼çš„ç‰©å“)
        test_users_relevant = {}
        for user_id in test_users:
            relevant_items = []
            if user_id in test_data:
                relevant_movies = test_data[user_id][
                    test_data[user_id]['rating'] >= self.test_config['relevance_threshold']
                ]
                relevant_items = relevant_movies['movie_id'].tolist()
            test_users_relevant[user_id] = relevant_items
        
        # æ„å»ºç‰©å“ç‰¹å¾
        item_features = {}
        for _, movie in self.validator.movies.iterrows():
            genres = self._extract_genres(movie).split(', ')
            item_features[movie['movie_id']] = [g for g in genres if g != 'Unknown']
        
        # è®¡ç®—ç‰©å“æµè¡Œåº¦
        item_popularity = {}
        item_counts = self.validator.ratings['movie_id'].value_counts()
        max_count = item_counts.max()
        for movie_id in self.validator.movies['movie_id']:
            count = item_counts.get(movie_id, 0)
            item_popularity[movie_id] = count / max_count if max_count > 0 else 0
        
        # æ‰€æœ‰ç‰©å“é›†åˆ
        all_items = set(self.validator.movies['movie_id'])
        
        return {
            "true_ratings": true_ratings,
            "predicted_ratings": predicted_ratings,
            "test_users_relevant": test_users_relevant,
            "recommendations": recommendations,
            "item_features": item_features,
            "item_popularity": item_popularity,
            "all_items": all_items
        }
    
    def _save_experiment_results(self, evaluation_results: Dict):
        """ä¿å­˜å®éªŒç»“æœ"""
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.output_dir / "llm_recommendation_evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜ç®€åŒ–æ‘˜è¦
        summary = {}
        for model_name, results in evaluation_results.items():
            if "overall_score" in results:
                summary[model_name] = {
                    "overall_score": results["overall_score"]["overall_score"],
                    "accuracy_rmse": results["accuracy_metrics"].get("RMSE", "N/A"),
                    "precision_at_10": results["ranking_metrics"].get("Precision@10", "N/A"),
                    "coverage": results["diversity_metrics"].get("Coverage", "N/A"),
                    "novelty": results["novelty_metrics"].get("Novelty", "N/A")
                }
        
        summary_file = self.output_dir / "experiment_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {self.output_dir}")
    
    def _generate_experiment_report(self, evaluation_results: Dict):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“‹ LLMæ¨èç³»ç»ŸéªŒè¯å®éªŒæŠ¥å‘Š")
        print("="*80)
        
        print(f"ğŸ“… å®éªŒæ—¶é—´: {pd.Timestamp.now()}")
        print(f"ğŸ“Š å®éªŒé…ç½®:")
        for key, value in self.test_config.items():
            print(f"   {key}: {value}")
        
        # æ¨¡å‹å¯¹æ¯”
        if "llama3" in evaluation_results and "qwen3" in evaluation_results:
            print(f"\nğŸ”„ åŒè¯­æ¨¡å‹å¯¹æ¯”:")
            
            for metric_category in ["overall_score", "accuracy_metrics", "ranking_metrics", 
                                  "diversity_metrics", "novelty_metrics"]:
                print(f"\nğŸ“Š {metric_category}:")
                
                llama3_metrics = evaluation_results["llama3"].get(metric_category, {})
                qwen3_metrics = evaluation_results["qwen3"].get(metric_category, {})
                
                # æ‰¾åˆ°å…±åŒæŒ‡æ ‡
                common_metrics = set(llama3_metrics.keys()) & set(qwen3_metrics.keys())
                
                for metric in sorted(common_metrics):
                    llama3_val = llama3_metrics[metric]
                    qwen3_val = qwen3_metrics[metric]
                    
                    if isinstance(llama3_val, (int, float)) and isinstance(qwen3_val, (int, float)):
                        diff = llama3_val - qwen3_val
                        winner = "ğŸ‡ºğŸ‡¸" if diff > 0 else "ğŸ‡¨ğŸ‡³" if diff < 0 else "ğŸ¤"
                        print(f"   {metric:20s}: Llama3={llama3_val:7.4f}, Qwen3={qwen3_val:7.4f} {winner}")
        
        # è¯¦ç»†æŠ¥å‘Š
        for model_name, results in evaluation_results.items():
            print(f"\n" + "="*60)
            print(f"ğŸ“ˆ {model_name.upper()} è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
            print("="*60)
            self.metrics.print_evaluation_report(results)
        
        print(f"\nğŸ¯ å®éªŒç»“è®º:")
        if "llama3" in evaluation_results and "qwen3" in evaluation_results:
            llama3_score = evaluation_results["llama3"].get("overall_score", {}).get("overall_score", 0)
            qwen3_score = evaluation_results["qwen3"].get("overall_score", {}).get("overall_score", 0)
            
            if llama3_score > qwen3_score:
                print(f"   ğŸ‡ºğŸ‡¸ Llama3è¡¨ç°æ›´ä¼˜ (å¾—åˆ†: {llama3_score:.3f} vs {qwen3_score:.3f})")
                print(f"   âœ… è‹±æ–‡åŸå£°æ¨¡å‹åœ¨MovieLensè‹±æ–‡æ•°æ®é›†ä¸Šç¡®å®æœ‰ä¼˜åŠ¿")
            elif qwen3_score > llama3_score:
                print(f"   ğŸ‡¨ğŸ‡³ Qwen3è¡¨ç°æ›´ä¼˜ (å¾—åˆ†: {qwen3_score:.3f} vs {llama3_score:.3f})")
                print(f"   ğŸ¤” ä¸­æ–‡æ¨¡å‹åœ¨è‹±æ–‡æ•°æ®é›†ä¸Šè¡¨ç°æ„å¤–ä¼˜ç§€ï¼Œå€¼å¾—æ·±å…¥ç ”ç©¶")
            else:
                print(f"   ğŸ¤ ä¸¤ä¸ªæ¨¡å‹è¡¨ç°ç›¸å½“ (Llama3: {llama3_score:.3f}, Qwen3: {qwen3_score:.3f})")
                print(f"   ğŸ“Š è¯­è¨€å·®å¼‚å¯¹æ¨èæ•ˆæœå½±å“è¾ƒå°")
        
        print(f"\nğŸ’¡ æŠ€æœ¯å»ºè®®:")
        print(f"   1. LLMæ¨èç³»ç»Ÿåœ¨MovieLensæ•°æ®é›†ä¸Šå…·å¤‡å¯è¡Œæ€§")
        print(f"   2. åŒè¯­æ¨¡å‹å¯¹æ¯”ä¸ºè·¨è¯­è¨€æ¨èç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†")
        print(f"   3. å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹ä»¥æå‡æ¨èè´¨é‡")
        print(f"   4. å¯ä»¥è€ƒè™‘å°†LLMä¸ä¼ ç»Ÿæ¨èç®—æ³•è¿›è¡Œèåˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨LLMæ¨èç³»ç»Ÿå®Œæ•´éªŒè¯å®éªŒ")
    
    # åˆ›å»ºå®éªŒ
    experiment = CompleteLLMValidationExperiment()
    
    # è¿è¡Œå®éªŒ
    try:
        results = experiment.run_complete_experiment()
        
        if results:
            print(f"\nğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {experiment.output_dir}")
        else:
            print(f"\nâŒ å®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒå¼‚å¸¸ç»ˆæ­¢: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
