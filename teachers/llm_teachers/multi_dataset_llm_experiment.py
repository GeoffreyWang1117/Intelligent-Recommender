#!/usr/bin/env python3
"""
å¤šæ•°æ®é›†LLMæ¨èç³»ç»ŸéªŒè¯å®éªŒ
Multi-Dataset LLM Recommendation System Validation Experiment

æ”¯æŒæ•°æ®é›†:
1. MovieLens (data/movielensä¸­çš„æ ·æœ¬æ•°æ®)
2. Amazon Reviews 2023 (è‡ªåŠ¨ä¸‹è½½)

åŠŸèƒ½:
1. å¤šæ•°æ®é›†å¯¹æ¯”éªŒè¯
2. LLMæ¨èæ•ˆæœè¯„ä¼°
3. è·¨é¢†åŸŸæ¨èæ€§èƒ½åˆ†æ

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-26
"""

import sys
import os
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from movielens_llm_validator import MovieLensLLMValidator
from amazon_reviews_llm_validator import AmazonReviewsLLMValidator
from recommendation_metrics import RecommendationMetrics

class MultiDatasetLLMExperiment:
    """å¤šæ•°æ®é›†LLMæ¨èéªŒè¯å®éªŒ"""
    
    def __init__(self, output_dir: str = "multi_dataset_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–éªŒè¯å™¨
        self.movielens_validator = MovieLensLLMValidator()
        self.amazon_validator = AmazonReviewsLLMValidator()
        self.metrics = RecommendationMetrics()
        
        # å®éªŒé…ç½®
        self.test_config = {
            "test_users_count": 5,              # æ¯ä¸ªæ•°æ®é›†æµ‹è¯•ç”¨æˆ·æ•°
            "min_ratings_per_user": 5,          # æœ€å°‘äº¤äº’æ•°
            "recommendation_count": 10,         # æ¨èæ•°é‡
            "relevance_threshold": 4.0,         # ç›¸å…³æ€§é˜ˆå€¼
            "test_ratio": 0.2                  # æµ‹è¯•é›†æ¯”ä¾‹
        }
        
        self.results = {}
    
    def run_complete_experiment(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„å¤šæ•°æ®é›†éªŒè¯å®éªŒ"""
        print("ğŸš€ å¯åŠ¨å¤šæ•°æ®é›†LLMæ¨èéªŒè¯å®éªŒ")
        print("=" * 60)
        
        # Step 1: MovieLensæ•°æ®é›†å®éªŒ
        movielens_results = self._run_movielens_experiment()
        
        # Step 2: Amazon Reviewsæ•°æ®é›†å®éªŒ
        amazon_results = self._run_amazon_experiment()
        
        # Step 3: è·¨æ•°æ®é›†å¯¹æ¯”åˆ†æ
        comparison_results = self._compare_datasets(movielens_results, amazon_results)
        
        # Step 4: ä¿å­˜ç»“æœ
        self._save_results({
            'movielens': movielens_results,
            'amazon': amazon_results,
            'comparison': comparison_results
        })
        
        # Step 5: ç”ŸæˆæŠ¥å‘Š
        self._generate_report(movielens_results, amazon_results, comparison_results)
        
        return {
            'movielens': movielens_results,
            'amazon': amazon_results,
            'comparison': comparison_results
        }
    
    def _run_movielens_experiment(self) -> Dict:
        """è¿è¡ŒMovieLensæ•°æ®é›†å®éªŒ"""
        print("\\nğŸ¬ MovieLensæ•°æ®é›†å®éªŒ")
        print("-" * 40)
        
        try:
            # åŠ è½½æ•°æ®
            if not self.movielens_validator.load_movielens_data("sample"):
                print("âŒ MovieLensæ•°æ®åŠ è½½å¤±è´¥")
                return {'success': False, 'error': 'Data loading failed'}
            
            # æ„å»ºç”¨æˆ·ç”»åƒ
            user_profiles = self.movielens_validator.build_user_profiles()
            if not user_profiles:
                print("âŒ MovieLensç”¨æˆ·ç”»åƒæ„å»ºå¤±è´¥")
                return {'success': False, 'error': 'User profile building failed'}
            
            print(f"âœ… MovieLensæ•°æ®å‡†å¤‡å®Œæˆ: {len(user_profiles)}ä¸ªç”¨æˆ·")
            
            # é€‰æ‹©æµ‹è¯•ç”¨æˆ·
            qualified_users = [
                uid for uid, profile in user_profiles.items()
                if profile['total_ratings'] >= self.test_config['min_ratings_per_user']
            ]
            
            if len(qualified_users) < self.test_config['test_users_count']:
                test_users = qualified_users
            else:
                test_users = np.random.choice(
                    qualified_users, 
                    self.test_config['test_users_count'], 
                    replace=False
                ).tolist()
            
            print(f"ğŸ‘¥ é€‰æ‹©æµ‹è¯•ç”¨æˆ·: {len(test_users)}ä¸ª")
            
            # ç”ŸæˆLLMæ¨è
            recommendations = {}
            success_count = 0
            
            for i, user_id in enumerate(test_users):
                print(f"  å¤„ç†ç”¨æˆ· {user_id} ({i+1}/{len(test_users)})")
                
                # Llama3æ¨è
                llama_recs = self.movielens_validator.generate_llm_recommendation(
                    user_id, "llama3:latest", k=self.test_config['recommendation_count']
                )
                
                # Qwen3æ¨è
                qwen_recs = self.movielens_validator.generate_llm_recommendation(
                    user_id, "qwen3:latest", k=self.test_config['recommendation_count']
                )
                
                if llama_recs or qwen_recs:
                    success_count += 1
                    recommendations[user_id] = {
                        'llama3': llama_recs,
                        'qwen3': qwen_recs
                    }
            
            print(f"âœ… MovieLensæ¨èç”Ÿæˆå®Œæˆ: {success_count}/{len(test_users)} æˆåŠŸ")
            
            # è¯„ä¼°æ¨èæ•ˆæœ
            evaluation_results = self._evaluate_movielens_recommendations(test_users, recommendations)
            
            return {
                'success': True,
                'dataset': 'MovieLens',
                'users_count': len(test_users),
                'success_rate': success_count / len(test_users),
                'recommendations': recommendations,
                'evaluation': evaluation_results
            }
            
        except Exception as e:
            print(f"âŒ MovieLenså®éªŒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _run_amazon_experiment(self) -> Dict:
        """è¿è¡ŒAmazon Reviewsæ•°æ®é›†å®éªŒ"""
        print("\\nğŸ›’ Amazon Reviewsæ•°æ®é›†å®éªŒ")
        print("-" * 40)
        
        try:
            # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å°æ ·æœ¬ï¼‰
            if not self.amazon_validator.load_amazon_data(sample_size=2000):
                print("âŒ Amazonæ•°æ®åŠ è½½å¤±è´¥")
                return {'success': False, 'error': 'Data loading failed'}
            
            # æ„å»ºç”¨æˆ·ç”»åƒ
            user_profiles = self.amazon_validator.build_user_profiles()
            if not user_profiles:
                print("âŒ Amazonç”¨æˆ·ç”»åƒæ„å»ºå¤±è´¥")
                return {'success': False, 'error': 'User profile building failed'}
            
            print(f"âœ… Amazonæ•°æ®å‡†å¤‡å®Œæˆ: {len(user_profiles)}ä¸ªç”¨æˆ·")
            
            # é€‰æ‹©æµ‹è¯•ç”¨æˆ·
            qualified_users = [
                uid for uid, profile in user_profiles.items()
                if profile['total_ratings'] >= self.test_config['min_ratings_per_user']
            ]
            
            if len(qualified_users) < self.test_config['test_users_count']:
                test_users = qualified_users
            else:
                test_users = np.random.choice(
                    qualified_users, 
                    self.test_config['test_users_count'], 
                    replace=False
                ).tolist()
            
            print(f"ğŸ‘¥ é€‰æ‹©æµ‹è¯•ç”¨æˆ·: {len(test_users)}ä¸ª")
            
            # ç”ŸæˆLLMæ¨è
            recommendations = {}
            success_count = 0
            
            for i, user_id in enumerate(test_users):
                print(f"  å¤„ç†ç”¨æˆ· {user_id} ({i+1}/{len(test_users)})")
                
                # Llama3æ¨è
                llama_recs = self.amazon_validator.generate_llm_recommendation(
                    user_id, "llama3:latest", k=self.test_config['recommendation_count']
                )
                
                # Qwen3æ¨è
                qwen_recs = self.amazon_validator.generate_llm_recommendation(
                    user_id, "qwen3:latest", k=self.test_config['recommendation_count']
                )
                
                if llama_recs or qwen_recs:
                    success_count += 1
                    recommendations[user_id] = {
                        'llama3': llama_recs,
                        'qwen3': qwen_recs
                    }
            
            print(f"âœ… Amazonæ¨èç”Ÿæˆå®Œæˆ: {success_count}/{len(test_users)} æˆåŠŸ")
            
            # è¯„ä¼°æ¨èæ•ˆæœ
            evaluation_results = self._evaluate_amazon_recommendations(test_users, recommendations)
            
            return {
                'success': True,
                'dataset': 'Amazon Reviews',
                'users_count': len(test_users),
                'success_rate': success_count / len(test_users),
                'recommendations': recommendations,
                'evaluation': evaluation_results
            }
            
        except Exception as e:
            print(f"âŒ Amazonå®éªŒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _evaluate_movielens_recommendations(self, test_users: List, recommendations: Dict) -> Dict:
        """è¯„ä¼°MovieLensæ¨èç»“æœ"""
        print("ğŸ“Š è¯„ä¼°MovieLensæ¨èæ•ˆæœ...")
        
        results = {'llama3': {}, 'qwen3': {}}
        
        for model in ['llama3', 'qwen3']:
            all_recommendations = []
            all_ground_truth = []
            
            for user_id in test_users:
                if user_id in recommendations and model in recommendations[user_id]:
                    user_recs = recommendations[user_id][model]
                    
                    # è·å–ç”¨æˆ·çš„çœŸå®é«˜è¯„åˆ†ç”µå½±ä½œä¸ºground truth
                    user_ratings = self.movielens_validator.ratings[
                        self.movielens_validator.ratings['user_id'] == user_id
                    ]
                    high_rated = user_ratings[
                        user_ratings['rating'] >= self.test_config['relevance_threshold']
                    ]['movie_id'].tolist()
                    
                    if user_recs and high_rated:
                        all_recommendations.append(user_recs)
                        all_ground_truth.append(high_rated)
            
            if all_recommendations:
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                metrics_result = self.metrics.evaluate_comprehensive(
                    all_recommendations, all_ground_truth, k_values=[5, 10]
                )
                results[model] = metrics_result
        
        return results
    
    def _evaluate_amazon_recommendations(self, test_users: List, recommendations: Dict) -> Dict:
        """è¯„ä¼°Amazonæ¨èç»“æœ"""
        print("ğŸ“Š è¯„ä¼°Amazonæ¨èæ•ˆæœ...")
        
        results = {'llama3': {}, 'qwen3': {}}
        
        for model in ['llama3', 'qwen3']:
            all_recommendations = []
            all_ground_truth = []
            
            for user_id in test_users:
                if user_id in recommendations and model in recommendations[user_id]:
                    user_recs = recommendations[user_id][model]
                    
                    # è·å–ç”¨æˆ·çš„çœŸå®é«˜è¯„åˆ†å•†å“ä½œä¸ºground truth
                    user_reviews = self.amazon_validator.reviews[
                        self.amazon_validator.reviews['user_id'] == user_id
                    ]
                    high_rated = user_reviews[
                        user_reviews['rating'] >= self.test_config['relevance_threshold']
                    ]['item_id'].tolist()
                    
                    if user_recs and high_rated:
                        all_recommendations.append(user_recs)
                        all_ground_truth.append(high_rated)
            
            if all_recommendations:
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                metrics_result = self.metrics.evaluate_comprehensive(
                    all_recommendations, all_ground_truth, k_values=[5, 10]
                )
                results[model] = metrics_result
        
        return results
    
    def _compare_datasets(self, movielens_results: Dict, amazon_results: Dict) -> Dict:
        """è·¨æ•°æ®é›†å¯¹æ¯”åˆ†æ"""
        print("\\nğŸ” è·¨æ•°æ®é›†å¯¹æ¯”åˆ†æ")
        print("-" * 40)
        
        comparison = {
            'dataset_comparison': {},
            'model_comparison': {},
            'recommendations': {}
        }
        
        # æ•°æ®é›†åŸºæœ¬ä¿¡æ¯å¯¹æ¯”
        if movielens_results.get('success') and amazon_results.get('success'):
            comparison['dataset_comparison'] = {
                'movielens_success_rate': movielens_results['success_rate'],
                'amazon_success_rate': amazon_results['success_rate'],
                'movielens_users': movielens_results['users_count'],
                'amazon_users': amazon_results['users_count']
            }
            
            # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
            for model in ['llama3', 'qwen3']:
                ml_metrics = movielens_results.get('evaluation', {}).get(model, {})
                az_metrics = amazon_results.get('evaluation', {}).get(model, {})
                
                if ml_metrics and az_metrics:
                    comparison['model_comparison'][model] = {
                        'movielens_overall': ml_metrics.get('overall_score', 0),
                        'amazon_overall': az_metrics.get('overall_score', 0),
                        'domain_preference': 'MovieLens' if ml_metrics.get('overall_score', 0) > az_metrics.get('overall_score', 0) else 'Amazon'
                    }
        
        return comparison
    
    def _save_results(self, results: Dict):
        """ä¿å­˜å®éªŒç»“æœ"""
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(self.output_dir / "multi_dataset_experiment_results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {self.output_dir}")
    
    def _generate_report(self, movielens_results: Dict, amazon_results: Dict, comparison_results: Dict):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        print("\\nğŸ“‹ ç”Ÿæˆå¤šæ•°æ®é›†å®éªŒæŠ¥å‘Š")
        print("=" * 60)
        
        report = f"""
# å¤šæ•°æ®é›†LLMæ¨èç³»ç»ŸéªŒè¯å®éªŒæŠ¥å‘Š

## å®éªŒæ¦‚è¿°
- å®éªŒæ—¶é—´: {pd.Timestamp.now()}
- æµ‹è¯•æ•°æ®é›†: MovieLens + Amazon Reviews 2023
- LLMæ¨¡å‹: Llama3 + Qwen3
- æ¨èæ•°é‡: {self.test_config['recommendation_count']}

## MovieLensæ•°æ®é›†ç»“æœ
"""
        
        if movielens_results.get('success'):
            report += f"""
### åŸºæœ¬ä¿¡æ¯
- æµ‹è¯•ç”¨æˆ·æ•°: {movielens_results['users_count']}
- æ¨èæˆåŠŸç‡: {movielens_results['success_rate']:.1%}

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”
"""
            ml_eval = movielens_results.get('evaluation', {})
            for model in ['llama3', 'qwen3']:
                if model in ml_eval:
                    metrics = ml_eval[model]
                    report += f"""
#### {model.upper()}
- ç»¼åˆå¾—åˆ†: {metrics.get('overall_score', 0):.3f}
- å‡†ç¡®æ€§å¾—åˆ†: {metrics.get('accuracy_score', 0):.3f}
- å¤šæ ·æ€§å¾—åˆ†: {metrics.get('diversity_score', 0):.3f}
"""
        else:
            report += f"\\nâŒ MovieLenså®éªŒå¤±è´¥: {movielens_results.get('error', 'Unknown error')}"
        
        report += "\\n## Amazon Reviewsæ•°æ®é›†ç»“æœ\\n"
        
        if amazon_results.get('success'):
            report += f"""
### åŸºæœ¬ä¿¡æ¯
- æµ‹è¯•ç”¨æˆ·æ•°: {amazon_results['users_count']}
- æ¨èæˆåŠŸç‡: {amazon_results['success_rate']:.1%}

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”
"""
            az_eval = amazon_results.get('evaluation', {})
            for model in ['llama3', 'qwen3']:
                if model in az_eval:
                    metrics = az_eval[model]
                    report += f"""
#### {model.upper()}
- ç»¼åˆå¾—åˆ†: {metrics.get('overall_score', 0):.3f}
- å‡†ç¡®æ€§å¾—åˆ†: {metrics.get('accuracy_score', 0):.3f}
- å¤šæ ·æ€§å¾—åˆ†: {metrics.get('diversity_score', 0):.3f}
"""
        else:
            report += f"\\nâŒ Amazonå®éªŒå¤±è´¥: {amazon_results.get('error', 'Unknown error')}"
        
        # è·¨æ•°æ®é›†å¯¹æ¯”
        report += "\\n## è·¨æ•°æ®é›†å¯¹æ¯”åˆ†æ\\n"
        
        if comparison_results.get('model_comparison'):
            for model, comp in comparison_results['model_comparison'].items():
                report += f"""
### {model.upper()} è·¨é¢†åŸŸè¡¨ç°
- MovieLensç»¼åˆå¾—åˆ†: {comp['movielens_overall']:.3f}
- Amazonç»¼åˆå¾—åˆ†: {comp['amazon_overall']:.3f}
- é¢†åŸŸåå¥½: {comp['domain_preference']}
"""
        
        report += f"""
## å®éªŒç»“è®º

1. **æ•°æ®é›†é€‚ç”¨æ€§**: éªŒè¯äº†LLMåœ¨ç”µå½±æ¨èå’Œç”µå•†æ¨èä¸¤ä¸ªä¸åŒé¢†åŸŸçš„é€‚ç”¨æ€§
2. **æ¨¡å‹å¯¹æ¯”**: Llama3å’ŒQwen3åœ¨ä¸åŒé¢†åŸŸçš„è¡¨ç°å·®å¼‚
3. **æŠ€æœ¯å¯è¡Œæ€§**: è¯æ˜äº†LLMåœ¨å¤šé¢†åŸŸæ¨èç³»ç»Ÿä¸­çš„æŠ€æœ¯å¯è¡Œæ€§

## æŠ€æœ¯å»ºè®®

1. é’ˆå¯¹ä¸åŒé¢†åŸŸä¼˜åŒ–æç¤ºè¯å·¥ç¨‹
2. è€ƒè™‘é¢†åŸŸç‰¹å®šçš„ç”¨æˆ·ç”»åƒæ„å»ºæ–¹æ³•
3. æ¢ç´¢è·¨é¢†åŸŸæ¨èçš„è¿ç§»å­¦ä¹ æ–¹æ³•

---
å®éªŒæ•°æ®ä¿å­˜åœ¨: {self.output_dir}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(self.output_dir / "multi_dataset_experiment_report.md", 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        print(f"\\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir / 'multi_dataset_experiment_report.md'}")

def main():
    """ä¸»å‡½æ•°"""
    experiment = MultiDatasetLLMExperiment()
    results = experiment.run_complete_experiment()
    
    print("\\nğŸ‰ å¤šæ•°æ®é›†LLMæ¨èéªŒè¯å®éªŒå®Œæˆï¼")
    
    # è¾“å‡ºç®€è¦ç»“æœ
    if results['movielens'].get('success'):
        print(f"âœ… MovieLens: æˆåŠŸç‡ {results['movielens']['success_rate']:.1%}")
    else:
        print(f"âŒ MovieLens: å¤±è´¥")
    
    if results['amazon'].get('success'):
        print(f"âœ… Amazon: æˆåŠŸç‡ {results['amazon']['success_rate']:.1%}")
    else:
        print(f"âŒ Amazon: å¤±è´¥")

if __name__ == "__main__":
    main()
