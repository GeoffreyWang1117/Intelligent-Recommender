#!/usr/bin/env python3
"""
åŸºäºçœŸå®MovieLensæ•°æ®çš„Fisher Informationæµ‹è¯•
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/home/coder-gw/7Projects_in_7Days/online-inference-system')

from fisher_calculator import FisherInformationCalculator

class RecommenderModel(nn.Module):
    """æ›´ç°å®çš„æ¨èæ¨¡å‹"""
    
    def __init__(self, num_users=610, num_items=9724, embedding_dim=64):
        super().__init__()
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        
        # ç”¨æˆ·å’Œç‰©å“åµŒå…¥
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # å¤šå±‚æ„ŸçŸ¥å™¨
        self.mlp = nn.Sequential(
            nn.Linear(embedding_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.normal_(self.user_embedding.weight, std=0.1)
        nn.init.normal_(self.item_embedding.weight, std=0.1)
        
        for layer in self.mlp:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.1)
    
    def forward(self, batch_data):
        if isinstance(batch_data, (tuple, list)):
            user_ids, item_ids = batch_data[0], batch_data[1]
        else:
            user_ids = batch_data[:, 0].long()
            item_ids = batch_data[:, 1].long()
            
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # ç‰¹å¾èåˆ
        features = torch.cat([user_emb, item_emb], dim=-1)
        output = self.mlp(features)
        
        return torch.sigmoid(output.squeeze())

def load_movielens_data():
    """åŠ è½½MovieLensæ•°æ®"""
    data_path = '/home/coder-gw/7Projects_in_7Days/online-inference-system/data/movielens'
    
    # è¯»å–è¯„åˆ†æ•°æ®
    ratings_file = os.path.join(data_path, 'ratings.csv')
    if not os.path.exists(ratings_file):
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {ratings_file}")
        return None
        
    ratings = pd.read_csv(ratings_file)
    print(f"âœ“ åŠ è½½ {len(ratings)} æ¡è¯„åˆ†æ•°æ®")
    
    # æ•°æ®é¢„å¤„ç†
    # é‡æ–°ç¼–ç ç”¨æˆ·å’Œç‰©å“ID
    user_ids = ratings['user_id'].unique()
    item_ids = ratings['item_id'].unique()
    
    user_map = {uid: idx for idx, uid in enumerate(user_ids)}
    item_map = {iid: idx for idx, iid in enumerate(item_ids)}
    
    ratings['user_idx'] = ratings['user_id'].map(user_map)
    ratings['item_idx'] = ratings['item_id'].map(item_map)
    
    # æ ‡å‡†åŒ–è¯„åˆ†åˆ°0-1
    ratings['rating_norm'] = (ratings['rating'] - 1) / 4.0
    
    print(f"  - ç”¨æˆ·æ•°: {len(user_ids)}")
    print(f"  - ç‰©å“æ•°: {len(item_ids)}")
    print(f"  - è¯„åˆ†èŒƒå›´: {ratings['rating'].min()}-{ratings['rating'].max()}")
    
    return ratings, len(user_ids), len(item_ids)

def create_movielens_dataloader(ratings_df, batch_size=64, sample_size=1000):
    """åˆ›å»ºMovieLensæ•°æ®åŠ è½½å™¨"""
    # é‡‡æ ·æ•°æ®
    if len(ratings_df) > sample_size:
        ratings_sample = ratings_df.sample(n=sample_size, random_state=42)
    else:
        ratings_sample = ratings_df
    
    # åˆ›å»ºå¼ é‡
    user_ids = torch.tensor(ratings_sample['user_idx'].values, dtype=torch.long)
    item_ids = torch.tensor(ratings_sample['item_idx'].values, dtype=torch.long)
    ratings = torch.tensor(ratings_sample['rating_norm'].values, dtype=torch.float32)
    
    # ç»„åˆè¾“å…¥
    inputs = torch.stack([user_ids, item_ids], dim=1).float()
    
    dataset = TensorDataset(inputs, ratings)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    print(f"âœ“ åˆ›å»ºæ•°æ®åŠ è½½å™¨: {len(dataset)} æ ·æœ¬, {len(dataloader)} æ‰¹æ¬¡")
    return dataloader

def test_fisher_on_movielens():
    """åœ¨çœŸå®MovieLensæ•°æ®ä¸Šæµ‹è¯•Fisher Information"""
    print("=== MovieLensæ•°æ®ä¸Šçš„Fisher Informationæµ‹è¯• ===")
    
    # åŠ è½½æ•°æ®
    print("1. åŠ è½½MovieLensæ•°æ®...")
    result = load_movielens_data()
    if result is None:
        return False
        
    ratings_df, num_users, num_items = result
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\n2. åˆ›å»ºæ¨èæ¨¡å‹...")
    model = RecommenderModel(num_users, num_items, embedding_dim=64).to(device)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n3. å‡†å¤‡æ•°æ®...")
    dataloader = create_movielens_dataloader(ratings_df, batch_size=64, sample_size=2000)
    
    # åˆå§‹åŒ–Fisherè®¡ç®—å™¨
    print("\n4. åˆå§‹åŒ–Fisher Informationè®¡ç®—å™¨...")
    fisher_calc = FisherInformationCalculator(model, device=str(device))
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    
    # è®¡ç®—Fisher Information
    print("\n5. è®¡ç®—Fisher Information...")
    try:
        fisher_info = fisher_calc.compute_fisher_information(
            dataloader, criterion, num_batches=10
        )
        
        print("âœ“ Fisher Informationè®¡ç®—æˆåŠŸï¼")
        print(f"  - è¦†ç›–å±‚æ•°: {len(fisher_info)}")
        
        # åˆ†æFisherä¿¡æ¯åˆ†å¸ƒ
        all_values = []
        layer_info = {}
        for name, tensor in fisher_info.items():
            values = tensor.flatten().cpu().numpy()
            all_values.extend(values)
            layer_info[name] = {
                'mean': float(values.mean()),
                'std': float(values.std()),
                'max': float(values.max())
            }
        
        all_values = np.array(all_values)
        print(f"  - å…¨å±€ç»Ÿè®¡: mean={all_values.mean():.6f}, std={all_values.std():.6f}")
        print(f"  - éé›¶å€¼æ¯”ä¾‹: {(all_values > 1e-8).mean():.3f}")
        
        # æ˜¾ç¤ºæœ€é‡è¦çš„å±‚
        sorted_layers = sorted(layer_info.items(), key=lambda x: x[1]['mean'], reverse=True)
        print("  - æœ€é‡è¦çš„5å±‚:")
        for name, stats in sorted_layers[:5]:
            print(f"    * {name}: mean={stats['mean']:.6f}, max={stats['max']:.6f}")
            
    except Exception as e:
        print(f"âœ— Fisher Informationè®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æµ‹è¯•å‰ªææ©ç ç”Ÿæˆ
    print("\n6. æµ‹è¯•å‰ªææ©ç ç”Ÿæˆ...")
    try:
        # ä½¿ç”¨æ›´åˆç†çš„å‰ªææ¯”ä¾‹
        for ratio in [0.1, 0.3, 0.5]:
            masks = fisher_calc.generate_pruning_mask(fisher_info, ratio, 'global')
            
            total_params = sum(mask.numel() for mask in masks.values())
            kept_params = sum(mask.sum().item() for mask in masks.values())
            actual_ratio = 1 - (kept_params / total_params)
            
            print(f"  - ç›®æ ‡å‰ªææ¯”ä¾‹ {ratio:.1%}: å®é™… {actual_ratio:.1%}")
        
        # åˆ†ææ¯å±‚çš„å‰ªææƒ…å†µ
        masks_30 = fisher_calc.generate_pruning_mask(fisher_info, 0.3, 'layer_wise')
        sparsity_stats = fisher_calc.get_sparsity_stats(masks_30)
        
        print("  - é€å±‚å‰ªæç¨€ç–åº¦åˆ†æ:")
        for layer, sparsity in list(sparsity_stats['layer_sparsity'].items())[:5]:
            print(f"    * {layer}: {sparsity:.1%}")
            
    except Exception as e:
        print(f"âœ— å‰ªææ©ç ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\n=== MovieLens Fisher Informationæµ‹è¯•å®Œæˆ ===")
    print("âœ“ æ‰€æœ‰åŠŸèƒ½åœ¨çœŸå®æ•°æ®ä¸Šæ­£å¸¸å·¥ä½œï¼")
    return True

if __name__ == "__main__":
    success = test_fisher_on_movielens()
    if success:
        print("\nğŸ‰ Fisher Informationæ¨¡å—åœ¨çœŸå®æ•°æ®ä¸ŠéªŒè¯æˆåŠŸï¼")
        print("å‡†å¤‡é›†æˆåˆ°pruning-awareè’¸é¦æµç¨‹ä¸­...")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
