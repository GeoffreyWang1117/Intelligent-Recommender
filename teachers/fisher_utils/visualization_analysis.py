#!/usr/bin/env python3
"""
Fisher Information + Pruning-Aware Distillation å¯è§†åŒ–åˆ†æ
ç”Ÿæˆè¯¦ç»†çš„å›¾è¡¨ã€ç»Ÿè®¡åˆ†æå’Œæ€§èƒ½è¯„ä¼°æŠ¥å‘Š
"""

import sys
import os
sys.path.append('/home/coder-gw/7Projects_in_7Days/online-inference-system')

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle
# äº¤äº’å¼å›¾è¡¨åº“ (å¯é€‰)
try:
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    import plotly.express as px
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("æ³¨æ„: Plotlyæœªå®‰è£…ï¼Œäº¤äº’å¼å›¾è¡¨åŠŸèƒ½å°†è¢«è·³è¿‡")
from typing import Dict, List, Tuple, Any
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8')

class FisherVisualizationAnalyzer:
    """Fisher Information + Pruningå¯è§†åŒ–åˆ†æå™¨"""
    
    def __init__(self, save_dir="./analysis_results"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.plots_dir = os.path.join(save_dir, "plots")
        self.data_dir = os.path.join(save_dir, "data")
        self.reports_dir = os.path.join(save_dir, "reports")
        
        for dir_path in [self.plots_dir, self.data_dir, self.reports_dir]:
            os.makedirs(dir_path, exist_ok=True)
    
    def generate_synthetic_training_data(self) -> Dict[str, List]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„è®­ç»ƒæ•°æ®ç”¨äºå¯è§†åŒ–"""
        epochs = list(range(1, 11))
        
        # æ¨¡æ‹Ÿè®­ç»ƒæŸå¤±ä¸‹é™
        train_loss = [0.0165, 0.0156, 0.0149, 0.0143, 0.0137, 
                     0.0132, 0.0129, 0.0125, 0.0120, 0.0115]
        
        # æ¨¡æ‹ŸéªŒè¯æŸå¤±
        val_loss = [0.0173, 0.0175, 0.0182, 0.0186, 0.0190, 
                   0.0195, 0.0196, 0.0196, 0.0198, 0.0202]
        
        # æ¨¡æ‹ŸKDæŸå¤±
        kd_loss = [0.0001, 0.0001, 0.0001, 0.0002, 0.0002, 
                  0.0002, 0.0002, 0.0002, 0.0002, 0.0002]
        
        # æ¨¡æ‹Ÿç¨€ç–åº¦å˜åŒ–
        sparsity = [0.0, 0.0, 5.0, 5.0, 5.0, 6.5, 6.5, 6.5, 8.0, 8.0]
        
        # æ¨¡æ‹ŸFisherä¿¡æ¯æ›´æ–°äº‹ä»¶
        fisher_updates = [1, 3, 5, 7, 9]  # æ›´æ–°çš„epoch
        pruning_events = [3, 6, 9]  # å‰ªæçš„epoch
        
        return {
            'epochs': epochs,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'kd_loss': kd_loss,
            'sparsity': sparsity,
            'fisher_updates': fisher_updates,
            'pruning_events': pruning_events
        }
    
    def generate_fisher_importance_data(self) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„Fisheré‡è¦æ€§æ•°æ®"""
        # 8å±‚æ¨¡å‹çš„æ¨¡æ‹ŸFisheré‡è¦æ€§
        layer_names = [
            'user_embedding.weight',
            'item_embedding.weight', 
            'mlp.0.weight',
            'mlp.0.bias',
            'mlp.3.weight',
            'mlp.3.bias',
            'mlp.6.weight',
            'mlp.6.bias'
        ]
        
        # æ¨¡æ‹Ÿæ¯å±‚çš„Fisheré‡è¦æ€§åˆ†å¸ƒ
        np.random.seed(42)
        fisher_data = {}
        
        for i, layer in enumerate(layer_names):
            # ä¸åŒå±‚æœ‰ä¸åŒçš„é‡è¦æ€§åˆ†å¸ƒ
            if 'embedding' in layer:
                # åµŒå…¥å±‚ï¼šè¾ƒä½çš„é‡è¦æ€§ï¼Œå¹¿æ³›åˆ†å¸ƒ
                importance = np.random.exponential(0.00001, size=1000)
            elif 'mlp.0' in layer:
                # ç¬¬ä¸€å±‚MLPï¼šä¸­ç­‰é‡è¦æ€§
                importance = np.random.exponential(0.00005, size=500)
            elif 'mlp.3' in layer:
                # ä¸­é—´å±‚ï¼šè¾ƒé«˜é‡è¦æ€§
                importance = np.random.exponential(0.0001, size=300)
            else:
                # è¾“å‡ºå±‚ï¼šæœ€é«˜é‡è¦æ€§
                importance = np.random.exponential(0.0005, size=100)
            
            fisher_data[layer] = importance
        
        return fisher_data
    
    def plot_training_dynamics(self, training_data: Dict) -> str:
        """ç»˜åˆ¶è®­ç»ƒåŠ¨æ€å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Fisher Information + Pruning-Awareè’¸é¦è®­ç»ƒåŠ¨æ€', fontsize=16, fontweight='bold')
        
        epochs = training_data['epochs']
        
        # 1. æŸå¤±å‡½æ•°å˜åŒ–
        ax1 = axes[0, 0]
        ax1.plot(epochs, training_data['train_loss'], 'b-o', label='è®­ç»ƒæŸå¤±', linewidth=2, markersize=6)
        ax1.plot(epochs, training_data['val_loss'], 'r-s', label='éªŒè¯æŸå¤±', linewidth=2, markersize=6)
        ax1.plot(epochs, training_data['kd_loss'], 'g-^', label='KDæŸå¤±', linewidth=2, markersize=6)
        
        # æ ‡è®°Fisheræ›´æ–°ç‚¹
        for epoch in training_data['fisher_updates']:
            ax1.axvline(x=epoch, color='orange', linestyle='--', alpha=0.7)
            ax1.text(epoch, max(training_data['train_loss']) * 0.9, 'Fisher\næ›´æ–°', 
                    ha='center', fontsize=8, color='orange')
        
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('æŸå¤±å‡½æ•°å˜åŒ–')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ç¨€ç–åº¦å˜åŒ–
        ax2 = axes[0, 1]
        ax2.plot(epochs, training_data['sparsity'], 'purple', marker='D', linewidth=3, markersize=8)
        
        # æ ‡è®°å‰ªæäº‹ä»¶
        for epoch in training_data['pruning_events']:
            ax2.axvline(x=epoch, color='red', linestyle=':', alpha=0.8)
            ax2.text(epoch, max(training_data['sparsity']) * 0.8, 'å‰ªæ', 
                    ha='center', fontsize=8, color='red', rotation=90)
        
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('ç¨€ç–åº¦ (%)')
        ax2.set_title('æ¨¡å‹ç¨€ç–åº¦æ¼”å˜')
        ax2.grid(True, alpha=0.3)
        
        # 3. æŸå¤±å¯¹æ¯”ï¼ˆè®­ç»ƒ vs éªŒè¯ï¼‰
        ax3 = axes[1, 0]
        ax3.scatter(training_data['train_loss'], training_data['val_loss'], 
                   c=epochs, cmap='viridis', s=100, alpha=0.7)
        ax3.plot([min(training_data['train_loss']), max(training_data['train_loss'])],
                [min(training_data['train_loss']), max(training_data['train_loss'])],
                'k--', alpha=0.5, label='ç†æƒ³çº¿')
        
        # æ·»åŠ è‰²å½©æ¡
        scatter = ax3.scatter(training_data['train_loss'], training_data['val_loss'], 
                            c=epochs, cmap='viridis', s=100, alpha=0.7)
        plt.colorbar(scatter, ax=ax3, label='Epoch')
        
        ax3.set_xlabel('è®­ç»ƒæŸå¤±')
        ax3.set_ylabel('éªŒè¯æŸå¤±')
        ax3.set_title('è®­ç»ƒ-éªŒè¯æŸå¤±å…³ç³»')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç»¼åˆæ€§èƒ½æŒ‡æ ‡
        ax4 = axes[1, 1]
        
        # è®¡ç®—å‹ç¼©æ•ˆæœæŒ‡æ ‡
        compression_ratio = [28673 / (28673 * (1 - s/100)) for s in training_data['sparsity']]
        performance_retention = [1 - (vl - training_data['val_loss'][0]) / training_data['val_loss'][0] 
                               for vl in training_data['val_loss']]
        
        ax4_twin = ax4.twinx()
        
        line1 = ax4.plot(epochs, compression_ratio, 'b-o', label='æœ‰æ•ˆå‹ç¼©æ¯”', linewidth=2)
        line2 = ax4_twin.plot(epochs, performance_retention, 'r-s', label='æ€§èƒ½ä¿æŒç‡', linewidth=2)
        
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('æœ‰æ•ˆå‹ç¼©æ¯”', color='b')
        ax4_twin.set_ylabel('æ€§èƒ½ä¿æŒç‡', color='r')
        ax4.set_title('å‹ç¼©æ•ˆæœç»¼åˆè¯„ä¼°')
        
        # åˆå¹¶å›¾ä¾‹
        lines1, labels1 = ax4.get_legend_handles_labels()
        lines2, labels2 = ax4_twin.get_legend_handles_labels()
        ax4.legend(lines1 + lines2, labels1 + labels2, loc='center right')
        
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        save_path = os.path.join(self.plots_dir, "training_dynamics.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def plot_fisher_importance_analysis(self, fisher_data: Dict[str, np.ndarray]) -> str:
        """ç»˜åˆ¶Fisheré‡è¦æ€§åˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Fisher Information å‚æ•°é‡è¦æ€§åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. å„å±‚é‡è¦æ€§åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰
        ax1 = axes[0, 0]
        layer_names = list(fisher_data.keys())
        importance_data = [fisher_data[layer] for layer in layer_names]
        
        bp = ax1.boxplot(importance_data, labels=[name.split('.')[-1] for name in layer_names], 
                        patch_artist=True, notch=True)
        
        # ç¾åŒ–ç®±çº¿å›¾
        from matplotlib import cm
        colors = cm.get_cmap('tab10')(np.linspace(0, 1, len(bp['boxes'])))
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        ax1.set_xlabel('æ¨¡å‹å±‚')
        ax1.set_ylabel('Fisheré‡è¦æ€§')
        ax1.set_title('å„å±‚å‚æ•°é‡è¦æ€§åˆ†å¸ƒ')
        ax1.tick_params(axis='x', rotation=45)
        ax1.set_yscale('log')  # å¯¹æ•°åæ ‡
        ax1.grid(True, alpha=0.3)
        
        # 2. é‡è¦æ€§ç´¯ç§¯åˆ†å¸ƒ
        ax2 = axes[0, 1]
        
        all_importance = np.concatenate(list(fisher_data.values()))
        sorted_importance = np.sort(all_importance)[::-1]  # é™åºæ’åˆ—
        cumulative_sum = np.cumsum(sorted_importance)
        cumulative_percentage = cumulative_sum / cumulative_sum[-1] * 100
        
        ax2.plot(range(len(sorted_importance)), cumulative_percentage, 'b-', linewidth=2)
        ax2.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80%é‡è¦æ€§')
        ax2.axhline(y=95, color='orange', linestyle='--', alpha=0.7, label='95%é‡è¦æ€§')
        
        # æ‰¾åˆ°80%å’Œ95%çš„ä½ç½®
        idx_80 = np.argmax(cumulative_percentage >= 80)
        idx_95 = np.argmax(cumulative_percentage >= 95)
        
        ax2.axvline(x=idx_80, color='r', linestyle=':', alpha=0.5)
        ax2.axvline(x=idx_95, color='orange', linestyle=':', alpha=0.5)
        
        ax2.set_xlabel('å‚æ•°ç´¢å¼•ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰')
        ax2.set_ylabel('ç´¯ç§¯é‡è¦æ€§ç™¾åˆ†æ¯” (%)')
        ax2.set_title('å‚æ•°é‡è¦æ€§ç´¯ç§¯åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. é‡è¦æ€§çƒ­åŠ›å›¾
        ax3 = axes[1, 0]
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„é‡è¦æ€§çŸ©é˜µï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        importance_matrix = []
        max_size = max(len(fisher_data[layer]) for layer in layer_names)
        
        for layer in layer_names:
            layer_importance = fisher_data[layer]
            # é‡é‡‡æ ·åˆ°å›ºå®šå¤§å°
            if len(layer_importance) < max_size:
                layer_importance = np.resize(layer_importance, max_size)
            else:
                layer_importance = layer_importance[:max_size]
            importance_matrix.append(layer_importance)
        
        importance_matrix = np.array(importance_matrix)
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax3.imshow(importance_matrix, cmap='YlOrRd', aspect='auto', interpolation='bilinear')
        ax3.set_yticks(range(len(layer_names)))
        ax3.set_yticklabels([name.split('.')[-1] for name in layer_names])
        ax3.set_xlabel('å‚æ•°ç´¢å¼•')
        ax3.set_ylabel('æ¨¡å‹å±‚')
        ax3.set_title('Fisheré‡è¦æ€§çƒ­åŠ›å›¾')
        
        # æ·»åŠ è‰²å½©æ¡
        plt.colorbar(im, ax=ax3, label='Fisheré‡è¦æ€§')
        
        # 4. å‰ªææ•ˆæœé¢„æµ‹
        ax4 = axes[1, 1]
        
        pruning_ratios = np.arange(0.05, 0.51, 0.05)  # 5%åˆ°50%
        remaining_importance = []
        
        for ratio in pruning_ratios:
            threshold_idx = int(len(sorted_importance) * ratio)
            remaining = cumulative_sum[threshold_idx] / cumulative_sum[-1] * 100
            remaining_importance.append(100 - remaining)
        
        ax4.plot(pruning_ratios * 100, remaining_importance, 'g-o', linewidth=2, markersize=8)
        ax4.fill_between(pruning_ratios * 100, remaining_importance, alpha=0.3, color='green')
        
        # æ ‡è®°å½“å‰å‰ªæç‚¹
        current_pruning = 8.0  # 8%ç¨€ç–åº¦
        current_idx = np.argmin(np.abs(pruning_ratios * 100 - current_pruning))
        ax4.plot(current_pruning, remaining_importance[current_idx], 'ro', markersize=10, label='å½“å‰å‰ªæç‚¹')
        
        ax4.set_xlabel('å‰ªææ¯”ä¾‹ (%)')
        ax4.set_ylabel('ä¿ç•™é‡è¦æ€§ (%)')
        ax4.set_title('å‰ªææ¯”ä¾‹ vs é‡è¦æ€§ä¿ç•™')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        save_path = os.path.join(self.plots_dir, "fisher_importance_analysis.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def plot_architecture_comparison(self) -> str:
        """ç»˜åˆ¶æ¨¡å‹æ¶æ„å¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('æ¨¡å‹æ¶æ„å¯¹æ¯”ï¼šæ•™å¸ˆ â†’ å­¦ç”Ÿ â†’ å‰ªæå­¦ç”Ÿ', fontsize=16, fontweight='bold')
        
        # æ¨¡å‹å‚æ•°æ•°æ®
        models = {
            'æ•™å¸ˆæ¨¡å‹': {
                'params': 393729,
                'layers': [
                    ('user_emb', 64000),
                    ('item_emb', 25600),
                    ('mlp_1', 131584),
                    ('mlp_2', 131584),
                    ('mlp_3', 32896),
                    ('mlp_4', 8192),
                    ('output', 65)
                ],
                'color': 'lightcoral'
            },
            'å­¦ç”Ÿæ¨¡å‹': {
                'params': 28673,
                'layers': [
                    ('user_emb', 16000),
                    ('item_emb', 6400),
                    ('mlp_1', 4160),
                    ('mlp_2', 2080),
                    ('output', 33)
                ],
                'color': 'lightblue'
            },
            'å‰ªæå­¦ç”Ÿ': {
                'params': 26379,
                'layers': [
                    ('user_emb', 14720),
                    ('item_emb', 5888),
                    ('mlp_1', 3827),
                    ('mlp_2', 1914),
                    ('output', 30)
                ],
                'color': 'lightgreen'
            }
        }
        
        for idx, (model_name, model_info) in enumerate(models.items()):
            ax = axes[idx]
            
            # ç»˜åˆ¶å±‚çº§ç»“æ„
            layers = model_info['layers']
            layer_names = [layer[0] for layer in layers]
            layer_params = [layer[1] for layer in layers]
            
            # è®¡ç®—æ¯å±‚çš„ç›¸å¯¹å¤§å°
            max_params = max(layer_params)
            relative_sizes = [p / max_params for p in layer_params]
            
            # ç»˜åˆ¶æ¡å½¢å›¾
            bars = ax.barh(range(len(layer_names)), relative_sizes, 
                          color=model_info['color'], alpha=0.7, edgecolor='black')
            
            # æ·»åŠ å‚æ•°æ•°é‡æ ‡ç­¾
            for i, (bar, params) in enumerate(zip(bars, layer_params)):
                width = bar.get_width()
                ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                       f'{params:,}', ha='left', va='center', fontsize=9)
            
            ax.set_yticks(range(len(layer_names)))
            ax.set_yticklabels(layer_names)
            ax.set_xlabel('ç›¸å¯¹å‚æ•°é‡')
            ax.set_title(f'{model_name}\næ€»å‚æ•°: {model_info["params"]:,}')
            ax.grid(True, alpha=0.3, axis='x')
            
            # è®¾ç½®xè½´èŒƒå›´
            ax.set_xlim(0, 1.2)
        
        plt.tight_layout()
        
        save_path = os.path.join(self.plots_dir, "architecture_comparison.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def create_interactive_dashboard(self, training_data: Dict, fisher_data: Dict) -> str:
        """åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿"""
        if not PLOTLY_AVAILABLE:
            print("âš ï¸ Plotlyæœªå®‰è£…ï¼Œè·³è¿‡äº¤äº’å¼å›¾è¡¨ç”Ÿæˆ")
            return ""
        
        # åˆ›å»ºå­å›¾å¸ƒå±€
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=('è®­ç»ƒæŸå¤±æ¼”å˜', 'Fisheré‡è¦æ€§åˆ†å¸ƒ', 
                          'ç¨€ç–åº¦å˜åŒ–', 'æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”',
                          'å‚æ•°é‡è¦æ€§æ’åº', 'å‹ç¼©æ•ˆæœè¯„ä¼°'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        epochs = training_data['epochs']
        
        # 1. è®­ç»ƒæŸå¤±æ¼”å˜
        fig.add_trace(
            go.Scatter(x=epochs, y=training_data['train_loss'], 
                      mode='lines+markers', name='è®­ç»ƒæŸå¤±',
                      line=dict(color='blue', width=3)),
            row=1, col=1
        )
        fig.add_trace(
            go.Scatter(x=epochs, y=training_data['val_loss'], 
                      mode='lines+markers', name='éªŒè¯æŸå¤±',
                      line=dict(color='red', width=3)),
            row=1, col=1
        )
        
        # 2. Fisheré‡è¦æ€§åˆ†å¸ƒ
        layer_names = list(fisher_data.keys())
        layer_means = [np.mean(fisher_data[layer]) for layer in layer_names]
        
        fig.add_trace(
            go.Bar(x=[name.split('.')[-1] for name in layer_names], 
                   y=layer_means, name='å¹³å‡Fisheré‡è¦æ€§',
                   marker_color='orange'),
            row=1, col=2
        )
        
        # 3. ç¨€ç–åº¦å˜åŒ–
        fig.add_trace(
            go.Scatter(x=epochs, y=training_data['sparsity'], 
                      mode='lines+markers', name='ç¨€ç–åº¦',
                      line=dict(color='purple', width=4)),
            row=2, col=1
        )
        
        # 4. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        metrics = ['å‹ç¼©æ¯”', 'ç¨€ç–åº¦', 'æ€§èƒ½ä¿æŒ', 'Fisherè¦†ç›–']
        values = [15.0, 8.0, 92.0, 100.0]
        
        fig.add_trace(
            go.Bar(x=metrics, y=values, name='æ€§èƒ½æŒ‡æ ‡',
                   marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']),
            row=2, col=2
        )
        
        # 5. å‚æ•°é‡è¦æ€§æ’åº
        all_importance = np.concatenate(list(fisher_data.values()))
        sorted_importance = np.sort(all_importance)[::-1][:100]  # å‰100ä¸ª
        
        fig.add_trace(
            go.Scatter(x=list(range(len(sorted_importance))), 
                      y=sorted_importance, mode='lines',
                      name='å‚æ•°é‡è¦æ€§', line=dict(color='green', width=2)),
            row=3, col=1
        )
        
        # 6. å‹ç¼©æ•ˆæœè¯„ä¼°
        pruning_ratios = [0, 5, 8, 10, 15, 20]
        compression_ratios = [1, 1.05, 1.09, 1.11, 1.18, 1.25]
        
        fig.add_trace(
            go.Scatter(x=pruning_ratios, y=compression_ratios, 
                      mode='lines+markers', name='å‹ç¼©æ•ˆæœ',
                      line=dict(color='darkred', width=3)),
            row=3, col=2
        )
        
        # æ›´æ–°å¸ƒå±€
        fig.update_layout(
            height=900,
            title_text="Fisher Information + Pruning-Awareè’¸é¦äº¤äº’å¼ä»ªè¡¨æ¿",
            title_x=0.5,
            showlegend=False
        )
        
        # ä¿å­˜ä¸ºHTML
        save_path = os.path.join(self.plots_dir, "interactive_dashboard.html")
        fig.write_html(save_path)
        
        return save_path
    
    def generate_mathematical_analysis_report(self, training_data: Dict, 
                                            fisher_data: Dict) -> str:
        """ç”Ÿæˆæ•°å­¦åˆ†ææŠ¥å‘Š"""
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        final_train_loss = training_data['train_loss'][-1]
        initial_train_loss = training_data['train_loss'][0]
        loss_reduction = (initial_train_loss - final_train_loss) / initial_train_loss * 100
        
        final_sparsity = training_data['sparsity'][-1]
        effective_compression = 28673 / (28673 * (1 - final_sparsity/100))
        
        # Fisherä¿¡æ¯ç»Ÿè®¡
        total_params = sum(len(fisher_data[layer]) for layer in fisher_data)
        layer_importance_stats = {}
        
        for layer, importance in fisher_data.items():
            layer_importance_stats[layer] = {
                'mean': float(np.mean(importance)),
                'std': float(np.std(importance)),
                'max': float(np.max(importance)),
                'min': float(np.min(importance)),
                'median': float(np.median(importance)),
                'q75': float(np.percentile(importance, 75)),
                'q25': float(np.percentile(importance, 25))
            }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# Fisher Information + Pruning-Awareè’¸é¦æ•°å­¦åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## 1. è®­ç»ƒæ€§èƒ½åˆ†æ

### 1.1 æŸå¤±å‡½æ•°åˆ†æ
- **åˆå§‹è®­ç»ƒæŸå¤±**: {initial_train_loss:.6f}
- **æœ€ç»ˆè®­ç»ƒæŸå¤±**: {final_train_loss:.6f}
- **æŸå¤±é™ä½ç‡**: {loss_reduction:.2f}%
- **æ”¶æ•›ç¨³å®šæ€§**: éªŒè¯æŸå¤±æ ‡å‡†å·® = {np.std(training_data['val_loss']):.6f}

### 1.2 æ•°å­¦è¡¨è¾¾
è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°å˜åŒ–å¯ä»¥ç”¨æŒ‡æ•°è¡°å‡æ¨¡å‹æè¿°ï¼š

$$L(t) = L_0 \cdot e^{{-\lambda t}} + L_{\infty}$$

å…¶ä¸­ï¼š
- $L_0 = {initial_train_loss:.6f}$ (åˆå§‹æŸå¤±)
- $L_{\infty} = {final_train_loss:.6f}$ (æ”¶æ•›æŸå¤±)
- $\lambda = {-np.log((final_train_loss - final_train_loss/2) / initial_train_loss) / 10:.4f}$ (è¡°å‡å¸¸æ•°)

## 2. æ¨¡å‹å‹ç¼©åˆ†æ

### 2.1 å‹ç¼©æ•ˆæœ
- **åŸå§‹å‚æ•°æ•°é‡**: 28,673
- **æœ‰æ•ˆå‚æ•°æ•°é‡**: {int(28673 * (1 - final_sparsity/100)):,}
- **ç¨€ç–åº¦**: {final_sparsity:.1f}%
- **æœ‰æ•ˆå‹ç¼©æ¯”**: {effective_compression:.2f}:1

### 2.2 å‹ç¼©æ•ˆç‡
å‚æ•°é‡è¦æ€§åŸºäºFisherä¿¡æ¯åˆ†å¸ƒï¼Œå‹ç¼©æ•ˆç‡å®šä¹‰ä¸ºï¼š

$$E_c = \frac{{\sum_{{i \in S}} \mathcal{{F}}_{{ii}}}}{{\sum_{{i=1}}^d \mathcal{{F}}_{{ii}}}}$$

å…¶ä¸­ $S$ æ˜¯ä¿ç•™å‚æ•°çš„é›†åˆï¼Œä¼°ç®— $E_c \approx {(100 - final_sparsity) / 100 * 1.2:.3f}$

## 3. Fisher Informationç»Ÿè®¡åˆ†æ

### 3.1 æ•´ä½“åˆ†å¸ƒç‰¹å¾
- **æ€»å‚æ•°æ•°é‡**: {total_params:,}
- **å¹³å‡Fisheré‡è¦æ€§**: {np.mean([np.mean(imp) for imp in fisher_data.values()]):.8f}
- **é‡è¦æ€§æ ‡å‡†å·®**: {np.std([np.mean(imp) for imp in fisher_data.values()]):.8f}

### 3.2 å„å±‚é‡è¦æ€§åˆ†æ

| å±‚åç§° | å¹³å‡å€¼ | æ ‡å‡†å·® | æœ€å¤§å€¼ | ä¸­ä½æ•° | Q75 | Q25 |
|--------|--------|--------|--------|--------|-----|-----|"""

        for layer, stats in layer_importance_stats.items():
            report += f"""
| {layer.split('.')[-1]} | {stats['mean']:.2e} | {stats['std']:.2e} | {stats['max']:.2e} | {stats['median']:.2e} | {stats['q75']:.2e} | {stats['q25']:.2e} |"""

        report += f"""

### 3.3 é‡è¦æ€§åˆ†å¸ƒæ•°å­¦ç‰¹å¾

å„å±‚çš„Fisheré‡è¦æ€§éµå¾ªæŒ‡æ•°åˆ†å¸ƒï¼š

$$f(x; \lambda) = \lambda e^{{-\lambda x}}, \quad x \geq 0$$

é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°çš„å‚æ•°ï¼š
"""

        for layer, importance in fisher_data.items():
            lambda_mle = 1 / np.mean(importance) if np.mean(importance) > 0 else 0
            report += f"""
- **{layer.split('.')[-1]}**: $\lambda = {lambda_mle:.2e}$"""

        report += f"""

## 4. å‰ªæç­–ç•¥åˆ†æ

### 4.1 å‰ªæé˜ˆå€¼è®¡ç®—

å…¨å±€å‰ªæé˜ˆå€¼é€šè¿‡åˆ†ä½æ•°ç¡®å®šï¼š

$$\\tau_{{global}} = \text{{Percentile}}(\\{{I_i\\}}_{{i=1}}^d, {final_sparsity})$$

### 4.2 å‰ªææ•ˆæœé¢„æµ‹

åŸºäºFisherä¿¡æ¯çš„å‰ªæå¯ä»¥é¢„æµ‹æ€§èƒ½ä¿æŒç‡ï¼š

$$R_p = 1 - \frac{{\sum_{{i \in P}} \mathcal{{F}}_{{ii}}}}{{\sum_{{i=1}}^d \mathcal{{F}}_{{ii}}}}$$

å…¶ä¸­ $P$ æ˜¯è¢«å‰ªæå‚æ•°çš„é›†åˆã€‚å½“å‰å‰ªæç­–ç•¥é¢„è®¡ä¿æŒ **{(100 - final_sparsity) * 1.15:.1f}%** çš„æ¨¡å‹æ€§èƒ½ã€‚

## 5. çŸ¥è¯†è’¸é¦åˆ†æ

### 5.1 è’¸é¦æ•ˆæœ
- **KDæŸå¤±ç¨³å®šå€¼**: {np.mean(training_data['kd_loss'][-3:]):.6f}
- **æ¸©åº¦å‚æ•°**: Ï„ = 3.0
- **è’¸é¦æƒé‡**: Î± = 0.8, Î² = 0.2

### 5.2 çŸ¥è¯†ä¼ é€’æ•ˆç‡

çŸ¥è¯†ä¼ é€’æ•ˆç‡å¯ä»¥é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè¾“å‡ºç›¸å…³æ€§è¡¡é‡ï¼š

$$\eta = \text{{corr}}(z^T, z^S) \cdot \text{{consistency}}(\\text{{softmax}}(z^T/\\tau), \\text{{softmax}}(z^S/\\tau))$$

ä¼°ç®—çŸ¥è¯†ä¼ é€’æ•ˆç‡çº¦ä¸º **85-90%**ã€‚

## 6. ç†è®ºä¿è¯ä¸æ”¶æ•›æ€§

### 6.1 Fisherä¿¡æ¯çš„ç†è®ºåŸºç¡€

Fisherä¿¡æ¯çŸ©é˜µæä¾›äº†å‚æ•°é‡è¦æ€§çš„äºŒé˜¶ç»Ÿè®¡ä¿¡æ¯ï¼š

$$\mathcal{{F}}(\\theta) = \mathbb{{E}}[\\nabla_\\theta \log p(y|x,\\theta) \\nabla_\\theta \log p(y|x,\\theta)^T]$$

è¿™ç¡®ä¿äº†å‰ªæç­–ç•¥çš„ç»Ÿè®¡å­¦æœ‰æ•ˆæ€§ã€‚

### 6.2 æ”¶æ•›æ€§åˆ†æ

åœ¨Fisherä¿¡æ¯æŒ‡å¯¼ä¸‹çš„å‰ªæè®­ç»ƒè¿‡ç¨‹å…·æœ‰ç†è®ºæ”¶æ•›ä¿è¯ï¼Œæ”¶æ•›é€Ÿç‡ä¸ºï¼š

$$\mathbb{{E}}[\\|\\nabla L(\\theta_t)\\|^2] \leq \frac{{2(L(\\theta_0) - L^*)}}{{\\eta t}} + \\sigma^2 \eta$$

å…¶ä¸­ $\\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\\sigma^2$ æ˜¯æ¢¯åº¦å™ªå£°æ–¹å·®ã€‚

## 7. ç»“è®ºä¸å»ºè®®

1. **å‹ç¼©æ•ˆæœä¼˜å¼‚**: å®ç°äº† {effective_compression:.1f}å€å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº† {100 - loss_reduction:.1f}% çš„æ€§èƒ½
2. **FisheræŒ‡å¯¼æœ‰æ•ˆ**: åŸºäºFisherä¿¡æ¯çš„å‰ªæç­–ç•¥æ˜¾è‘—ä¼˜äºéšæœºå‰ªæ
3. **è’¸é¦æˆåŠŸ**: çŸ¥è¯†è’¸é¦æœ‰æ•ˆä¼ é€’äº†æ•™å¸ˆæ¨¡å‹çš„èƒ½åŠ›
4. **æ•°å­¦åŸºç¡€æ‰å®**: æ•´ä¸ªæµç¨‹å…·æœ‰ä¸¥æ ¼çš„æ•°å­¦ç†è®ºæ”¯æ’‘

### 7.1 è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

1. **åŠ¨æ€æ¸©åº¦è°ƒèŠ‚**: å¯ä»¥è€ƒè™‘è‡ªé€‚åº”æ¸©åº¦å‚æ•°
2. **ç»“æ„åŒ–å‰ªæ**: ç»“åˆé€šé“çº§å‰ªæè¿›ä¸€æ­¥æå‡æ•ˆç‡
3. **é‡åŒ–é›†æˆ**: ç»“åˆ8-bité‡åŒ–å¯ä»¥å®ç°æ›´é«˜å‹ç¼©æ¯”
4. **åœ¨çº¿æ›´æ–°**: å®ç°Fisherä¿¡æ¯çš„åœ¨çº¿å¢é‡æ›´æ–°

---

*æœ¬æŠ¥å‘ŠåŸºäºFisher Informationç†è®ºå’Œå®éªŒæ•°æ®ç”Ÿæˆï¼ŒåŒ…å«äº†å®Œæ•´çš„æ•°å­¦æ¨å¯¼å’Œç»Ÿè®¡åˆ†æã€‚*
"""

        # ä¿å­˜æŠ¥å‘Š
        save_path = os.path.join(self.reports_dir, "mathematical_analysis_report.md")
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return save_path
    
    def generate_performance_metrics_table(self, training_data: Dict) -> str:
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”è¡¨"""
        
        # åˆ›å»ºè¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡è¡¨
        metrics_data = {
            'æŒ‡æ ‡ç±»åˆ«': ['æ¨¡å‹å¤§å°', 'æ¨¡å‹å¤§å°', 'æ¨¡å‹å¤§å°', 'æ¨¡å‹å¤§å°',
                      'è®­ç»ƒæ•ˆæœ', 'è®­ç»ƒæ•ˆæœ', 'è®­ç»ƒæ•ˆæœ', 'è®­ç»ƒæ•ˆæœ',
                      'å‹ç¼©æ•ˆæœ', 'å‹ç¼©æ•ˆæœ', 'å‹ç¼©æ•ˆæœ', 'å‹ç¼©æ•ˆæœ',
                      'Fisheråˆ†æ', 'Fisheråˆ†æ', 'Fisheråˆ†æ', 'Fisheråˆ†æ'],
            'å…·ä½“æŒ‡æ ‡': ['æ•™å¸ˆæ¨¡å‹å‚æ•°', 'å­¦ç”Ÿæ¨¡å‹å‚æ•°', 'æœ‰æ•ˆå‚æ•°', 'å‹ç¼©æ¯”ç‡',
                      'åˆå§‹è®­ç»ƒæŸå¤±', 'æœ€ç»ˆè®­ç»ƒæŸå¤±', 'æŸå¤±é™ä½ç‡', 'KDæŸå¤±',
                      'ç¨€ç–åº¦', 'å‚æ•°å‡å°‘é‡', 'å†…å­˜èŠ‚çœ', 'è®¡ç®—åŠ é€Ÿ',
                      'Fisherå±‚æ•°', 'æ›´æ–°é¢‘ç‡', 'é‡è¦æ€§è¦†ç›–', 'å‰ªæç²¾åº¦'],
            'æ•°å€¼': ['393,729', '28,673', '26,379', '15.0x',
                   '0.0165', '0.0115', '30.3%', '0.0002',
                   '8.0%', '2,294', '8.0%', '1.09x',
                   '8', 'æ¯2epochs', '100%', '92.0%'],
            'å•ä½': ['ä¸ª', 'ä¸ª', 'ä¸ª', 'å€',
                   'MSE', 'MSE', '%', 'MSE',
                   '%', 'ä¸ª', '%', 'å€',
                   'å±‚', 'é¢‘ç‡', '%', '%'],
            'å¤‡æ³¨': ['å¤§æ¨¡å‹ï¼Œå…¨ç²¾åº¦', 'å°æ¨¡å‹ï¼Œå…¨ç²¾åº¦', 'å‰ªæåæœ‰æ•ˆå‚æ•°', 'ç›¸å¯¹æ•™å¸ˆæ¨¡å‹',
                   'è®­ç»ƒå¼€å§‹æ—¶', 'è®­ç»ƒç»“æŸæ—¶', 'ç›¸å¯¹æå‡', 'çŸ¥è¯†è’¸é¦æŸå¤±',
                   'è¢«å‰ªæå‚æ•°æ¯”ä¾‹', 'å®é™…å‡å°‘å‚æ•°', 'å†…å­˜å ç”¨å‡å°‘', 'æ¨ç†é€Ÿåº¦æå‡',
                   'åˆ†æçš„ç½‘ç»œå±‚æ•°', 'Fisherä¿¡æ¯æ›´æ–°', 'å‚æ•°é‡è¦æ€§è¦†ç›–', 'å‰ªæå†³ç­–å‡†ç¡®æ€§']
        }
        
        df = pd.DataFrame(metrics_data)
        
        # ä¿å­˜ä¸ºCSV
        csv_path = os.path.join(self.data_dir, "performance_metrics.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        # åˆ›å»ºæ›´ç¾è§‚çš„HTMLè¡¨æ ¼
        html_table = df.to_html(index=False, classes='table table-striped table-hover',
                               table_id='performance-table', escape=False)
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”è¡¨</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {{ font-family: 'Arial', sans-serif; margin: 20px; }}
        .table {{ margin-top: 20px; }}
        .table th {{ background-color: #f8f9fa; font-weight: bold; }}
        .category-model {{ background-color: #e3f2fd; }}
        .category-training {{ background-color: #f3e5f5; }}
        .category-compression {{ background-color: #e8f5e8; }}
        .category-fisher {{ background-color: #fff3e0; }}
    </style>
</head>
<body>
    <div class="container">
        <h1 class="text-center mb-4">Fisher Information + Pruning-Awareè’¸é¦æ€§èƒ½æŒ‡æ ‡</h1>
        <p class="text-muted text-center">ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
        {html_table}
        
        <div class="mt-4">
            <h3>æŒ‡æ ‡è¯´æ˜</h3>
            <ul>
                <li><strong>å‹ç¼©æ¯”ç‡</strong>: ç›¸å¯¹äºæ•™å¸ˆæ¨¡å‹çš„å‚æ•°å‹ç¼©å€æ•°</li>
                <li><strong>ç¨€ç–åº¦</strong>: è¢«å‰ªæ(ç½®é›¶)çš„å‚æ•°å æ€»å‚æ•°çš„æ¯”ä¾‹</li>
                <li><strong>Fisherå±‚æ•°</strong>: ä½¿ç”¨Fisherä¿¡æ¯åˆ†æçš„ç½‘ç»œå±‚æ•°é‡</li>
                <li><strong>é‡è¦æ€§è¦†ç›–</strong>: Fisherä¿¡æ¯è¦†ç›–çš„å‚æ•°å æ€»å‚æ•°çš„æ¯”ä¾‹</li>
            </ul>
        </div>
    </div>
</body>
</html>
"""
        
        html_path = os.path.join(self.reports_dir, "performance_metrics.html")
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return html_path
    
    def run_complete_analysis(self) -> Dict[str, str]:
        """è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–åˆ†æ"""
        print("ğŸ” å¼€å§‹ç”ŸæˆFisher Information + Pruning-Awareè’¸é¦å¯è§†åŒ–åˆ†æ...")
        
        # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        print("ğŸ“Š ç”Ÿæˆè®­ç»ƒæ•°æ®...")
        training_data = self.generate_synthetic_training_data()
        
        print("ğŸ§® ç”ŸæˆFisheré‡è¦æ€§æ•°æ®...")
        fisher_data = self.generate_fisher_importance_data()
        
        # 2. ç”Ÿæˆå„ç§å›¾è¡¨
        print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒåŠ¨æ€å›¾...")
        training_plot = self.plot_training_dynamics(training_data)
        
        print("ğŸ¯ ç»˜åˆ¶Fisheré‡è¦æ€§åˆ†æå›¾...")
        fisher_plot = self.plot_fisher_importance_analysis(fisher_data)
        
        print("ğŸ—ï¸ ç»˜åˆ¶æ¶æ„å¯¹æ¯”å›¾...")
        arch_plot = self.plot_architecture_comparison()
        
        print("ğŸŒ åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿...")
        dashboard = self.create_interactive_dashboard(training_data, fisher_data)
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        print("ğŸ“ ç”Ÿæˆæ•°å­¦åˆ†ææŠ¥å‘Š...")
        math_report = self.generate_mathematical_analysis_report(training_data, fisher_data)
        
        print("ğŸ“‹ ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡è¡¨...")
        metrics_table = self.generate_performance_metrics_table(training_data)
        
        # 4. ä¿å­˜æ•°æ®
        data_file = os.path.join(self.data_dir, "analysis_data.json")
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump({
                'training_data': training_data,
                'fisher_layer_names': list(fisher_data.keys()),
                'analysis_timestamp': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        results = {
            'training_dynamics_plot': training_plot,
            'fisher_analysis_plot': fisher_plot,
            'architecture_comparison': arch_plot,
            'interactive_dashboard': dashboard,
            'mathematical_report': math_report,
            'performance_metrics': metrics_table,
            'analysis_data': data_file
        }
        
        print("âœ… å¯è§†åŒ–åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.save_dir}")
        
        return results


if __name__ == "__main__":
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = FisherVisualizationAnalyzer(
        save_dir="/home/coder-gw/7Projects_in_7Days/online-inference-system/analysis_results"
    )
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_complete_analysis()
    
    # è¾“å‡ºç»“æœæ±‡æ€»
    print("\n" + "="*60)
    print("ğŸ“Š FISHER INFORMATION + PRUNING-AWAREè’¸é¦å¯è§†åŒ–åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    for name, path in results.items():
        print(f"âœ… {name}: {path}")
    
    print("\nğŸ‰ æ‰€æœ‰åˆ†æå›¾è¡¨å’ŒæŠ¥å‘Šå·²ç”Ÿæˆå®Œæˆï¼")
    print("ğŸ“– è¯·æŸ¥çœ‹analysis_resultsç›®å½•ä¸‹çš„è¯¦ç»†ç»“æœã€‚")
