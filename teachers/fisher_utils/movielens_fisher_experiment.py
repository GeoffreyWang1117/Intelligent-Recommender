#!/usr/bin/env python3
"""
åŸºäºçœŸå®MovieLensæ•°æ®å’ŒEnsemble Teacherçš„Fisher Informationå®éªŒ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
import sys
import pickle
from typing import Dict, Any, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/home/coder-gw/7Projects_in_7Days/online-inference-system')

from teachers.fisher_utils.ensemble_fisher_calculator import EnsembleFisherCalculator
from models.optimized_ensemble_teacher import OptimizedEnsembleTeacher
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MovieLensDataLoader:
    """MovieLensæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_path='/home/coder-gw/7Projects_in_7Days/online-inference-system/data/movielens'):
        self.data_path = data_path
        self.ratings_data = None
        self.user_map = {}
        self.item_map = {}
        self.num_users = 0
        self.num_items = 0
        
    def load_data(self) -> bool:
        """åŠ è½½MovieLensæ•°æ®"""
        logger.info("ğŸ“¥ åŠ è½½MovieLensæ•°æ®...")
        
        try:
            # å°è¯•åŠ è½½è¯„åˆ†æ•°æ®
            ratings_file = os.path.join(self.data_path, 'ratings.csv')
            if os.path.exists(ratings_file):
                self.ratings_data = pd.read_csv(ratings_file)
                logger.info(f"âœ… ä»ratings.csvåŠ è½½ {len(self.ratings_data)} æ¡è¯„åˆ†")
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ratings.csvï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
                self.ratings_data = self._generate_mock_data()
            
            # æ„å»ºç”¨æˆ·å’Œç‰©å“æ˜ å°„
            self._build_mappings()
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _generate_mock_data(self) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹ŸMovieLensæ•°æ®"""
        np.random.seed(42)
        
        num_users = 610
        num_items = 9724  
        num_ratings = 100836
        
        # ç”Ÿæˆç¬¦åˆMovieLensåˆ†å¸ƒçš„æ•°æ®
        user_ids = np.random.choice(num_users, num_ratings, replace=True)
        item_ids = np.random.choice(num_items, num_ratings, replace=True)
        
        # ç”Ÿæˆè¯„åˆ†ï¼ˆåå‘é«˜åˆ†ï¼Œç¬¦åˆMovieLensç‰¹æ€§ï¼‰
        ratings = np.random.choice([1, 2, 3, 4, 5], num_ratings, 
                                 p=[0.05, 0.1, 0.2, 0.35, 0.3])
        
        # æ·»åŠ æ—¶é—´æˆ³
        timestamps = np.random.randint(900000000, 1500000000, num_ratings)
        
        return pd.DataFrame({
            'user_id': user_ids,
            'item_id': item_ids, 
            'rating': ratings,
            'timestamp': timestamps
        })
    
    def _build_mappings(self):
        """æ„å»ºç”¨æˆ·å’Œç‰©å“IDæ˜ å°„"""
        unique_users = self.ratings_data['user_id'].unique()
        unique_items = self.ratings_data['item_id'].unique()
        
        self.user_map = {uid: idx for idx, uid in enumerate(unique_users)}
        self.item_map = {iid: idx for idx, iid in enumerate(unique_items)}
        
        self.num_users = len(unique_users)
        self.num_items = len(unique_items)
        
        # æ·»åŠ æ˜ å°„åçš„ç´¢å¼•
        self.ratings_data['user_idx'] = self.ratings_data['user_id'].map(self.user_map)
        self.ratings_data['item_idx'] = self.ratings_data['item_id'].map(self.item_map)
        
        logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {self.num_users}ç”¨æˆ·, {self.num_items}ç‰©å“, {len(self.ratings_data)}è¯„åˆ†")
    
    def get_dataloader(self, batch_size=1024, split='train') -> DataLoader:
        """è·å–æ•°æ®åŠ è½½å™¨"""
        if self.ratings_data is None:
            raise ValueError("æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_data()")
        
        # ç®€å•çš„è®­ç»ƒ/æµ‹è¯•åˆ†å‰²
        if split == 'train':
            data = self.ratings_data.sample(frac=0.8, random_state=42)
        else:
            data = self.ratings_data.drop(self.ratings_data.sample(frac=0.8, random_state=42).index)
        
        # å‡†å¤‡å¼ é‡æ•°æ®
        user_tensor = torch.LongTensor(data['user_idx'].values)
        item_tensor = torch.LongTensor(data['item_idx'].values)
        rating_tensor = torch.FloatTensor((data['rating'].values - 1) / 4.0)  # å½’ä¸€åŒ–åˆ°[0,1]
        
        dataset = TensorDataset(user_tensor, item_tensor, rating_tensor)
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)


class SimpleRecommenderModel(nn.Module):
    """ç®€åŒ–çš„æ¨èæ¨¡å‹ï¼Œç”¨äºFisheråˆ†ææ¼”ç¤º"""
    
    def __init__(self, num_users, num_items, embedding_dim=64):
        super().__init__()
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_dim = embedding_dim
        
        # åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # é¢„æµ‹ç½‘ç»œ
        self.predictor = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        nn.init.normal_(self.user_embedding.weight, std=0.1)
        nn.init.normal_(self.item_embedding.weight, std=0.1)
        
        for layer in self.predictor:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.1)
    
    def forward(self, user_ids, item_ids):
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        features = torch.cat([user_emb, item_emb], dim=-1)
        prediction = self.predictor(features)
        
        return prediction.squeeze()


def run_movielens_fisher_experiment():
    """è¿è¡ŒMovieLens Fisher Informationå®éªŒ"""
    logger.info("ğŸš€ å¼€å§‹MovieLens Fisher Informationå®éªŒ")
    logger.info("="*60)
    
    # 1. åŠ è½½æ•°æ®
    logger.info("ğŸ“‹ æ­¥éª¤1: æ•°æ®åŠ è½½")
    data_loader = MovieLensDataLoader()
    if not data_loader.load_data():
        logger.error("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢å®éªŒ")
        return False
    
    train_dataloader = data_loader.get_dataloader(batch_size=512, split='train')
    test_dataloader = data_loader.get_dataloader(batch_size=512, split='test')
    
    # 2. åˆå§‹åŒ–Ensemble Teacher
    logger.info("ğŸ“‹ æ­¥éª¤2: Ensemble Teacheråˆå§‹åŒ–")
    try:
        ensemble_teacher = OptimizedEnsembleTeacher()
        logger.info("âœ… Ensemble Teacheråˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Ensemble Teacheråˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # 3. åˆ›å»ºç®€åŒ–æ¨¡å‹ç”¨äºæ¼”ç¤ºFisheråˆ†æ
    logger.info("ğŸ“‹ æ­¥éª¤3: åˆ›å»ºæ¼”ç¤ºæ¨¡å‹")
    demo_model = SimpleRecommenderModel(
        num_users=data_loader.num_users,
        num_items=data_loader.num_items,
        embedding_dim=64
    )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    demo_model.to(device)
    logger.info(f"âœ… æ¼”ç¤ºæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    # 4. è¿è¡ŒFisher Informationåˆ†æ
    logger.info("ğŸ“‹ æ­¥éª¤4: Fisher Informationåˆ†æ")
    try:
        # ä½¿ç”¨Ensemble Fisherè®¡ç®—å™¨
        fisher_calc = EnsembleFisherCalculator(ensemble_teacher, device=str(device))
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        analysis_results = fisher_calc.analyze_ensemble_fisher(
            dataloader=train_dataloader,
            num_batches=100
        )
        
        logger.info("âœ… Fisheråˆ†æå®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ Fisheråˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 5. ä¿å­˜å’Œå±•ç¤ºç»“æœ
    logger.info("ğŸ“‹ æ­¥éª¤5: ç»“æœä¿å­˜ä¸å±•ç¤º")
    try:
        # ä¿å­˜åˆ†æç»“æœ
        save_path = '/home/coder-gw/7Projects_in_7Days/online-inference-system/analysis_results/movielens_fisher_experiment.json'
        fisher_calc.save_analysis_results(analysis_results, save_path)
        
        # å±•ç¤ºå…³é”®ç»“æœ
        _display_key_results(analysis_results)
        
        logger.info("âœ… ç»“æœä¿å­˜ä¸å±•ç¤ºå®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ ç»“æœå¤„ç†å¤±è´¥: {e}")
        return False
    
    logger.info("ğŸ‰ MovieLens Fisher Informationå®éªŒæˆåŠŸå®Œæˆï¼")
    return True


def _display_key_results(results: Dict[str, Any]):
    """å±•ç¤ºå…³é”®å®éªŒç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ¯ æ ¸å¿ƒå®éªŒç»“æœ")
    print("="*60)
    
    # 1. æ¨¡å‹å‰ªæå‹å¥½åº¦æ’å
    if 'ensemble_comparison' in results:
        comparison = results['ensemble_comparison']
        if 'pruning_friendly_ranking' in comparison:
            print("\nğŸ“Š æ¨¡å‹å‰ªæå‹å¥½åº¦æ’å:")
            for i, model in enumerate(comparison['pruning_friendly_ranking'], 1):
                print(f"   {i}. {model['model']} (åˆ†æ•°: {model['score']}) - {model['reason']}")
    
    # 2. æ¨èå‰ªæç­–ç•¥
    if 'pruning_suggestions' in results:
        pruning = results['pruning_suggestions']
        if 'recommended_strategy' in pruning:
            strategy = pruning['recommended_strategy']
            print(f"\nâœ‚ï¸ æ¨èå‰ªæç­–ç•¥:")
            print(f"   ç›®æ ‡å‹ç¼©ç‡: {strategy.get('target_compression', 0)*100:.1f}%")
            print(f"   é¢„æœŸæ€§èƒ½æŸå¤±: {strategy.get('expected_performance_loss', 0)*100:.1f}%")
    
    # 3. æ€§èƒ½é¢„æµ‹
    if 'performance_prediction' in results:
        perf = results['performance_prediction']
        if 'efficiency_gains' in perf:
            gains = perf['efficiency_gains']
            print(f"\nğŸš€ æ•ˆç‡æå‡é¢„æµ‹:")
            print(f"   æ¨ç†åŠ é€Ÿ: {gains.get('inference_speedup', 0):.1f}x")
            print(f"   å†…å­˜å‡å°‘: {gains.get('memory_reduction', 0)*100:.1f}%")
            print(f"   èƒ½è€—èŠ‚çœ: {gains.get('energy_savings', 0)*100:.1f}%")
    
    # 4. å…³é”®å±‚é‡è¦æ€§
    if 'layer_importance' in results:
        layers = results['layer_importance']
        if 'critical_layers' in layers:
            print(f"\nğŸ¯ å…³é”®å±‚é‡è¦æ€§:")
            for layer in layers['critical_layers'][:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"   {layer['model']}.{layer['layer']}: {layer['importance']:.2f} - {layer['reason']}")
    
    print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•°"""
    success = run_movielens_fisher_experiment()
    if success:
        print("\nğŸ‰ å®éªŒå®Œæˆï¼æ£€æŸ¥analysis_resultsç›®å½•æŸ¥çœ‹è¯¦ç»†ç»“æœã€‚")
    else:
        print("\nâŒ å®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    main()
