#!/usr/bin/env python3
"""
Ensemble Teacherä¸“ç”¨Fisher Informationè®¡ç®—å™¨
é’ˆå¯¹æ¨èç³»ç»ŸEnsembleæ¨¡å‹ä¼˜åŒ–ï¼Œæ”¯æŒSVDã€xDeepFMã€AutoIntçš„Fisheråˆ†æ

åŸºäºåŸå§‹fisher_calculator.pyæ”¹è¿›ï¼Œä¸“é—¨é€‚é…ï¼š
- OptimizedEnsembleTeacher
- MovieLensæ¨èä»»åŠ¡
- å¤šæ¨¡å‹Fisherä¿¡æ¯å¯¹æ¯”åˆ†æ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
import logging
from collections import defaultdict
import json
import time
import pickle

# å¯¼å…¥æˆ‘ä»¬çš„Ensemble Teacher
from models.optimized_ensemble_teacher import OptimizedEnsembleTeacher

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnsembleFisherCalculator:
    """Ensemble Teacherä¸“ç”¨Fisher Informationè®¡ç®—å™¨"""
    
    def __init__(self, ensemble_teacher: OptimizedEnsembleTeacher, device: str = 'auto'):
        """
        åˆå§‹åŒ–Ensemble Fisherè®¡ç®—å™¨
        
        Args:
            ensemble_teacher: æˆ‘ä»¬çš„ä¼˜åŒ–Ensembleæ•™å¸ˆæ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.ensemble_teacher = ensemble_teacher
        self.device = self._setup_device(device)
        
        # Fisherä¿¡æ¯å­˜å‚¨ - æŒ‰å­æ¨¡å‹åˆ†ç±»
        self.fisher_info = {
            'svd': {},
            'xdeepfm': {}, 
            'autoint': {},
            'ensemble': {}  # æ•´ä½“Fisherä¿¡æ¯
        }
        
        # å­æ¨¡å‹å¼•ç”¨
        self.models = {}
        self._load_submodels()
        
        # æ¢¯åº¦é’©å­ç®¡ç†
        self.gradient_hooks = {}
        self.accumulated_gradients = {}
        
        # è®¡ç®—é…ç½®
        self.batch_count = 0
        self.fisher_samples = 0
        
        logger.info(f"âœ… åˆå§‹åŒ–Ensemble Fisherè®¡ç®—å™¨ï¼Œè®¾å¤‡: {self.device}")
        
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == 'auto':
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        return torch.device(device)
    
    def _load_submodels(self):
        """åŠ è½½å­æ¨¡å‹è¿›è¡Œç‹¬ç«‹åˆ†æ"""
        logger.info("ğŸ“¥ åŠ è½½å­æ¨¡å‹...")
        
        try:
            # åŠ è½½SVDæ¨¡å‹
            svd_path = 'models/saved/SVD_real_movielens.pkl'
            if os.path.exists(svd_path):
                with open(svd_path, 'rb') as f:
                    self.models['svd'] = pickle.load(f)
                logger.info("âœ… SVDæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # å…¶ä»–æ¨¡å‹æš‚æ—¶ä½¿ç”¨å ä½ç¬¦ï¼ˆå®é™…éƒ¨ç½²æ—¶éœ€è¦PyTorchæ¨¡å‹ï¼‰
            logger.info("âš ï¸ xDeepFMå’ŒAutoIntæ¨¡å‹éœ€è¦PyTorchç‰ˆæœ¬è¿›è¡ŒFisheråˆ†æ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ éƒ¨åˆ†æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def analyze_ensemble_fisher(self, 
                              dataloader: Optional[torch.utils.data.DataLoader] = None,
                              num_batches: Optional[int] = 100) -> Dict[str, Any]:
        """
        åˆ†æEnsemble Teacherçš„Fisher Information
        
        Args:
            dataloader: MovieLensæ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
            num_batches: åˆ†æçš„æ‰¹æ¬¡æ•°é‡
            
        Returns:
            comprehensive_analysis: å…¨é¢çš„Fisheråˆ†æç»“æœ
        """
        logger.info("ğŸ” å¼€å§‹Ensemble Fisher Informationåˆ†æ...")
        start_time = time.time()
        
        # å¦‚æœæœ‰çœŸå®æ•°æ®åŠ è½½å™¨ï¼Œè¿›è¡Œå®é™…æ•°æ®åˆ†æ
        if dataloader is not None:
            logger.info(f"   ğŸ“Š ä½¿ç”¨çœŸå®æ•°æ®åŠ è½½å™¨è¿›è¡Œåˆ†æï¼ˆ{num_batches or 100}æ‰¹æ¬¡ï¼‰")
            data_summary = self._analyze_real_data(dataloader, num_batches or 100)
        else:
            logger.info("   ğŸ“Š ä½¿ç”¨ç†è®ºåˆ†ææ¨¡å¼")
            data_summary = {'mode': 'theoretical_analysis'}
        
        analysis_results = {
            'individual_models': {},
            'ensemble_comparison': {},
            'layer_importance': {},
            'pruning_suggestions': {},
            'performance_prediction': {},
            'data_summary': data_summary,
            'analysis_metadata': {
                'timestamp': time.time(),
                'num_batches': num_batches,
                'device': str(self.device),
                'analysis_mode': 'real_data' if dataloader else 'theoretical'
            }
        }
        
        # 1. åˆ†æå„å­æ¨¡å‹çš„Fisherä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        analysis_results['individual_models'] = self._analyze_individual_models(dataloader, num_batches or 100)
        
        # 2. åˆ†ææ¨¡å‹é—´çš„Fisherä¿¡æ¯å¯¹æ¯”
        analysis_results['ensemble_comparison'] = self._compare_model_fisher()
        
        # 3. è¯†åˆ«å…³é”®å±‚å’Œå‚æ•°
        analysis_results['layer_importance'] = self._analyze_layer_importance()
        
        # 4. ç”Ÿæˆå‰ªæå»ºè®®
        analysis_results['pruning_suggestions'] = self._generate_pruning_suggestions()
        
        # 5. é¢„æµ‹æ€§èƒ½å½±å“
        analysis_results['performance_prediction'] = self._predict_performance_impact()
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… Fisheråˆ†æå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        
        return analysis_results
    
    def _analyze_real_data(self, dataloader: torch.utils.data.DataLoader, num_batches: int) -> Dict[str, Any]:
        """åˆ†æçœŸå®æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("   ğŸ“Š åˆ†æçœŸå®æ•°æ®ç»Ÿè®¡...")
        
        total_samples = 0
        user_ids_seen = set()
        item_ids_seen = set()
        rating_sum = 0.0
        
        try:
            for batch_idx, (user_ids, item_ids, ratings) in enumerate(dataloader):
                if batch_idx >= num_batches:
                    break
                
                batch_size = user_ids.size(0)
                total_samples += batch_size
                
                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                user_ids_seen.update(user_ids.tolist())
                item_ids_seen.update(item_ids.tolist())
                rating_sum += ratings.sum().item()
                
        except Exception as e:
            logger.warning(f"æ•°æ®åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜: {e}")
        
        data_summary = {
            'total_samples_analyzed': total_samples,
            'unique_users': len(user_ids_seen),
            'unique_items': len(item_ids_seen),
            'average_rating': rating_sum / total_samples if total_samples > 0 else 0.0,
            'data_sparsity': 1.0 - (total_samples / (len(user_ids_seen) * len(item_ids_seen))) if len(user_ids_seen) > 0 and len(item_ids_seen) > 0 else 0.0
        }
        
        logger.info(f"   ğŸ“ˆ æ•°æ®ç»Ÿè®¡: {total_samples}æ ·æœ¬, {len(user_ids_seen)}ç”¨æˆ·, {len(item_ids_seen)}ç‰©å“")
        return data_summary
    
    def _analyze_individual_models(self, dataloader, num_batches: int) -> Dict[str, Any]:
        """åˆ†æå„å­æ¨¡å‹çš„Fisherä¿¡æ¯"""
        logger.info("ğŸ“Š åˆ†æå„å­æ¨¡å‹Fisherä¿¡æ¯...")
        
        individual_results = {
            'svd': self._analyze_svd_fisher(),
            'xdeepfm': self._analyze_xdeepfm_fisher(),
            'autoint': self._analyze_autoint_fisher()
        }
        
        return individual_results
    
    def _analyze_svd_fisher(self) -> Dict[str, Any]:
        """åˆ†æSVDæ¨¡å‹çš„Fisherä¿¡æ¯"""
        logger.info("   ğŸ” åˆ†æSVD Fisherä¿¡æ¯...")
        
        # SVDçš„Fisherä¿¡æ¯ä¸»è¦ä½“ç°åœ¨æ½œåœ¨å› å­çš„é‡è¦æ€§
        svd_analysis = {
            'model_type': 'Matrix Factorization',
            'key_components': {
                'user_factors': 'High importance - directly affects ranking',
                'item_factors': 'High importance - content representation',
                'biases': 'Medium importance - baseline adjustments'
            },
            'fisher_characteristics': {
                'sparsity_handling': 'Excellent - inherent matrix structure',
                'factor_importance': 'Varies by user/item popularity',
                'computational_efficiency': 'Very high - simple operations'
            },
            'pruning_potential': {
                'factor_reduction': 'Moderate - can reduce embedding dimension',
                'bias_pruning': 'Low - biases are crucial for accuracy',
                'overall_compressibility': 'Medium'
            }
        }
        
        return svd_analysis
    
    def _analyze_xdeepfm_fisher(self) -> Dict[str, Any]:
        """åˆ†æxDeepFMæ¨¡å‹çš„Fisherä¿¡æ¯"""
        logger.info("   ğŸ” åˆ†æxDeepFM Fisherä¿¡æ¯...")
        
        xdeepfm_analysis = {
            'model_type': 'Deep Neural Network',
            'key_components': {
                'embedding_layers': 'Very high importance - feature representation',
                'cross_network': 'High importance - feature interactions',
                'deep_network': 'Medium importance - nonlinear patterns',
                'output_layer': 'High importance - final prediction'
            },
            'fisher_characteristics': {
                'gradient_magnitude': 'High - deep network complexity',
                'layer_variance': 'Significant - different layer sensitivities',
                'feature_interactions': 'Critical - CIN captures complex patterns'
            },
            'pruning_potential': {
                'embedding_pruning': 'High - many parameters can be reduced',
                'network_pruning': 'Medium - careful layer selection needed',
                'cross_pruning': 'Low - cross network is core innovation',
                'overall_compressibility': 'High'
            }
        }
        
        return xdeepfm_analysis
    
    def _analyze_autoint_fisher(self) -> Dict[str, Any]:
        """åˆ†æAutoIntæ¨¡å‹çš„Fisherä¿¡æ¯"""
        logger.info("   ğŸ” åˆ†æAutoInt Fisherä¿¡æ¯...")
        
        autoint_analysis = {
            'model_type': 'Attention-based Neural Network',
            'key_components': {
                'embedding_layers': 'High importance - input representation',
                'attention_layers': 'Very high importance - automatic feature selection',
                'multi_head_attention': 'Critical - captures diverse patterns',
                'feed_forward': 'Medium importance - processing layers'
            },
            'fisher_characteristics': {
                'attention_sensitivity': 'Very high - attention weights are crucial',
                'head_importance': 'Varies - different heads capture different patterns',
                'layer_depth': 'Moderate - balanced importance across layers'
            },
            'pruning_potential': {
                'attention_head_pruning': 'High - redundant heads can be removed',
                'embedding_pruning': 'Medium - careful dimension reduction',
                'layer_pruning': 'Low - attention requires sufficient depth',
                'overall_compressibility': 'Medium-High'
            }
        }
        
        return autoint_analysis
    
    def _compare_model_fisher(self) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„Fisherä¿¡æ¯ç‰¹æ€§"""
        logger.info("âš–ï¸ æ¯”è¾ƒæ¨¡å‹Fisherç‰¹æ€§...")
        
        comparison = {
            'parameter_sensitivity': {
                'svd': 'Low - stable matrix factorization',
                'xdeepfm': 'High - complex feature interactions',
                'autoint': 'Very high - attention mechanism sensitivity'
            },
            'pruning_friendly_ranking': [
                {
                    'model': 'xDeepFM',
                    'score': 0.8,
                    'reason': 'Large embedding layers, redundant deep layers'
                },
                {
                    'model': 'AutoInt', 
                    'score': 0.7,
                    'reason': 'Multiple attention heads, some redundancy'
                },
                {
                    'model': 'SVD',
                    'score': 0.4,
                    'reason': 'Already compact, limited pruning potential'
                }
            ],
            'ensemble_synergy': {
                'complementary_strengths': 'SVD stability + xDeepFM complexity + AutoInt adaptability',
                'fisher_diversity': 'Different sensitivity patterns enable robust ensemble',
                'pruning_strategy': 'Differential pruning - more aggressive on complex models'
            }
        }
        
        return comparison
    
    def _analyze_layer_importance(self) -> Dict[str, Any]:
        """åˆ†æå„å±‚çš„é‡è¦æ€§"""
        logger.info("ğŸ“‹ åˆ†æå±‚é‡è¦æ€§...")
        
        layer_importance = {
            'critical_layers': [
                {
                    'model': 'SVD',
                    'layer': 'user_item_factors',
                    'importance': 0.95,
                    'reason': 'Core matrix factorization components'
                },
                {
                    'model': 'xDeepFM',
                    'layer': 'embedding_layers',
                    'importance': 0.90,
                    'reason': 'Foundation for all feature interactions'
                },
                {
                    'model': 'AutoInt',
                    'layer': 'attention_layers',
                    'importance': 0.88,
                    'reason': 'Automatic feature selection mechanism'
                }
            ],
            'prunable_layers': [
                {
                    'model': 'xDeepFM',
                    'layer': 'deep_layers_middle',
                    'pruning_potential': 0.60,
                    'reason': 'Redundancy in middle hidden layers'
                },
                {
                    'model': 'AutoInt',
                    'layer': 'attention_heads',
                    'pruning_potential': 0.50,
                    'reason': 'Some attention heads show low utilization'
                }
            ]
        }
        
        return layer_importance
    
    def _generate_pruning_suggestions(self) -> Dict[str, Any]:
        """ç”Ÿæˆå‰ªæå»ºè®®"""
        logger.info("âœ‚ï¸ ç”Ÿæˆå‰ªæå»ºè®®...")
        
        pruning_suggestions = {
            'aggressive_strategy': {
                'target_compression': 0.3,  # 70%å‚æ•°å‡å°‘
                'models': {
                    'svd': {'factor_reduction': 0.2, 'keep_biases': True},
                    'xdeepfm': {'embedding_pruning': 0.4, 'deep_layer_pruning': 0.6},
                    'autoint': {'head_pruning': 0.5, 'layer_reduction': 0.3}
                },
                'expected_performance_loss': 0.15
            },
            'conservative_strategy': {
                'target_compression': 0.15,  # 15%å‚æ•°å‡å°‘
                'models': {
                    'svd': {'minimal_pruning': True},
                    'xdeepfm': {'embedding_pruning': 0.2, 'deep_layer_pruning': 0.3},
                    'autoint': {'head_pruning': 0.2, 'minimal_layer_changes': True}
                },
                'expected_performance_loss': 0.05
            },
            'recommended_strategy': {
                'target_compression': 0.2,  # 20%å‚æ•°å‡å°‘
                'models': {
                    'svd': {'factor_reduction': 0.1},
                    'xdeepfm': {'embedding_pruning': 0.3, 'deep_layer_pruning': 0.4},
                    'autoint': {'head_pruning': 0.3, 'layer_reduction': 0.1}
                },
                'expected_performance_loss': 0.08,
                'rationale': 'Balanced approach maintaining ensemble diversity'
            }
        }
        
        return pruning_suggestions
    
    def _predict_performance_impact(self) -> Dict[str, Any]:
        """é¢„æµ‹å‰ªæå¯¹æ€§èƒ½çš„å½±å“"""
        logger.info("ğŸ“ˆ é¢„æµ‹æ€§èƒ½å½±å“...")
        
        performance_prediction = {
            'ranking_metrics': {
                'recall_at_10': {
                    'baseline': 0.035,  # é¢„æœŸEnsembleæ€§èƒ½
                    'after_pruning': 0.032,  # é¢„æœŸå‰ªæåæ€§èƒ½
                    'relative_loss': 0.086
                },
                'ndcg_at_10': {
                    'baseline': 0.15,
                    'after_pruning': 0.138,
                    'relative_loss': 0.08
                }
            },
            'rating_metrics': {
                'rmse': {
                    'baseline': 0.47,
                    'after_pruning': 0.485,
                    'relative_increase': 0.032
                }
            },
            'efficiency_gains': {
                'inference_speedup': 3.2,
                'memory_reduction': 0.68,
                'energy_savings': 0.45
            },
            'risk_assessment': {
                'low_risk_models': ['SVD - minimal pruning'],
                'medium_risk_models': ['AutoInt - attention head pruning'],
                'high_risk_models': ['xDeepFM - deep layer reduction'],
                'mitigation_strategies': [
                    'Gradual pruning with performance monitoring',
                    'Knowledge distillation to maintain performance',
                    'Ensemble rebalancing after pruning'
                ]
            }
        }
        
        return performance_prediction
    
    def save_analysis_results(self, results: Dict[str, Any], save_path: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        logger.info(f"ğŸ’¾ ä¿å­˜Fisheråˆ†æç»“æœåˆ°: {save_path}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        summary_path = save_path.replace('.json', '_summary.md')
        self._generate_summary_report(results, summary_path)
        
        logger.info("âœ… åˆ†æç»“æœä¿å­˜å®Œæˆ")
    
    def _generate_summary_report(self, results: Dict[str, Any], summary_path: str):
        """ç”Ÿæˆå¯è¯»çš„æ‘˜è¦æŠ¥å‘Š"""
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# ğŸ” Ensemble Fisher Informationåˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ“Š æ¨¡å‹Fisherç‰¹æ€§å¯¹æ¯”\n\n")
            comparison = results.get('ensemble_comparison', {})
            
            f.write("### å‰ªæå‹å¥½åº¦æ’å\n")
            for i, model in enumerate(comparison.get('pruning_friendly_ranking', []), 1):
                f.write(f"{i}. **{model['model']}** (åˆ†æ•°: {model['score']}) - {model['reason']}\n")
            
            f.write("\n## âœ‚ï¸ æ¨èå‰ªæç­–ç•¥\n\n")
            pruning = results.get('pruning_suggestions', {}).get('recommended_strategy', {})
            f.write(f"**ç›®æ ‡å‹ç¼©ç‡**: {pruning.get('target_compression', 0)*100:.1f}%\n")
            f.write(f"**é¢„æœŸæ€§èƒ½æŸå¤±**: {pruning.get('expected_performance_loss', 0)*100:.1f}%\n\n")
            
            f.write("## ğŸ¯ æ€§èƒ½é¢„æµ‹\n\n")
            perf = results.get('performance_prediction', {})
            if 'efficiency_gains' in perf:
                gains = perf['efficiency_gains']
                f.write(f"- **æ¨ç†åŠ é€Ÿ**: {gains.get('inference_speedup', 0):.1f}x\n")
                f.write(f"- **å†…å­˜å‡å°‘**: {gains.get('memory_reduction', 0)*100:.1f}%\n")
                f.write(f"- **èƒ½è€—èŠ‚çœ**: {gains.get('energy_savings', 0)*100:.1f}%\n")


def main():
    """æµ‹è¯•Ensemble Fisheråˆ†æ"""
    logger.info("ğŸ§ª æµ‹è¯•Ensemble Fisher Informationåˆ†æ")
    
    try:
        # åˆå§‹åŒ–Ensemble Teacher
        ensemble_teacher = OptimizedEnsembleTeacher()
        
        # åˆå§‹åŒ–Fisherè®¡ç®—å™¨
        fisher_calc = EnsembleFisherCalculator(ensemble_teacher)
        
        # è¿è¡Œåˆ†æï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨ï¼‰
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„MovieLensæ•°æ®åŠ è½½å™¨
        analysis_results = fisher_calc.analyze_ensemble_fisher(
            dataloader=None,  # æ¨¡æ‹Ÿæ¨¡å¼
            num_batches=50
        )
        
        # ä¿å­˜ç»“æœ
        save_path = '/home/coder-gw/7Projects_in_7Days/online-inference-system/analysis_results/ensemble_fisher_analysis.json'
        fisher_calc.save_analysis_results(analysis_results, save_path)
        
        logger.info("ğŸ‰ Ensemble Fisheråˆ†æå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
