#!/usr/bin/env python3
"""
Pruning-Aware Knowledge Distillation ä¸»æµç¨‹
é›†æˆFisher InformationæŒ‡å¯¼çš„å‰ªæä¸çŸ¥è¯†è’¸é¦

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-27
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
import logging
import json
import time
from collections import defaultdict

from fisher_calculator import FisherInformationCalculator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PruningAwareDistillation:
    """Pruning-Aware Knowledge Distillationä¸»ç±»"""
    
    def __init__(self, 
                 teacher_model: nn.Module,
                 student_model: nn.Module,
                 device: str = 'auto',
                 distillation_config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–Pruning-Awareè’¸é¦
        
        Args:
            teacher_model: æ•™å¸ˆæ¨¡å‹
            student_model: å­¦ç”Ÿæ¨¡å‹  
            device: è®¡ç®—è®¾å¤‡
            distillation_config: è’¸é¦é…ç½®
        """
        self.teacher = teacher_model
        self.student = student_model
        self.device = self._setup_device(device)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.teacher.to(self.device)
        self.student.to(self.device)
        
        # é…ç½®
        self.config = self._setup_config(distillation_config)
        
        # Fisher Informationè®¡ç®—å™¨
        self.fisher_calc = FisherInformationCalculator(self.student, device=str(self.device))
        
        # çŠ¶æ€è·Ÿè¸ª
        self.current_epoch = 0
        self.pruning_masks = {}
        self.fisher_info = {}
        self.distillation_losses = []
        
        logger.info(f"åˆå§‹åŒ–Pruning-Awareè’¸é¦ï¼Œè®¾å¤‡: {self.device}")
        logger.info(f"æ•™å¸ˆæ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.teacher.parameters()):,}")
        logger.info(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.student.parameters()):,}")
        
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == 'auto':
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        return torch.device(device)
    
    def _setup_config(self, config: Optional[Dict]) -> Dict:
        """è®¾ç½®é»˜è®¤é…ç½®"""
        default_config = {
            'temperature': 4.0,
            'alpha': 0.7,  # KDæŸå¤±æƒé‡
            'beta': 0.3,   # ä»»åŠ¡æŸå¤±æƒé‡
            'pruning_schedule': {
                'start_epoch': 5,
                'frequency': 10,
                'initial_ratio': 0.1,
                'final_ratio': 0.5,
                'strategy': 'gradual'  # 'gradual' or 'aggressive'
            },
            'fisher_config': {
                'update_frequency': 5,
                'num_batches': 10,
                'accumulate': True
            }
        }
        
        if config:
            default_config.update(config)
        
        return default_config
    
    def compute_distillation_loss(self, 
                                student_outputs: torch.Tensor,
                                teacher_outputs: torch.Tensor,
                                true_labels: torch.Tensor,
                                task_criterion: nn.Module) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—è’¸é¦æŸå¤±
        
        Args:
            student_outputs: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_outputs: æ•™å¸ˆæ¨¡å‹è¾“å‡º  
            true_labels: çœŸå®æ ‡ç­¾
            task_criterion: ä»»åŠ¡æŸå¤±å‡½æ•°
            
        Returns:
            å„ç§æŸå¤±ç»„ä»¶
        """
        temperature = self.config['temperature']
        alpha = self.config['alpha']
        beta = self.config['beta']
        
        # ä»»åŠ¡æŸå¤±
        task_loss = task_criterion(student_outputs, true_labels)
        
        # çŸ¥è¯†è’¸é¦æŸå¤±
        if student_outputs.dim() > 1:
            student_soft = F.softmax(student_outputs / temperature, dim=-1)
            teacher_soft = F.softmax(teacher_outputs / temperature, dim=-1)
            kd_loss = F.kl_div(F.log_softmax(student_outputs / temperature, dim=-1), 
                              teacher_soft, reduction='batchmean') * (temperature ** 2)
        else:
            student_soft = torch.sigmoid(student_outputs / temperature)
            teacher_soft = torch.sigmoid(teacher_outputs / temperature)
            kd_loss = F.mse_loss(student_soft, teacher_soft)
        
        # æ€»æŸå¤±
        total_loss = alpha * kd_loss + beta * task_loss
        
        return {
            'total_loss': total_loss,
            'kd_loss': kd_loss,
            'task_loss': task_loss
        }\n    \n    def should_update_fisher(self, epoch: int) -> bool:\n        \"\"\"åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°Fisher Information\"\"\"\n        frequency = self.config['fisher_config']['update_frequency']\n        return epoch % frequency == 0\n    \n    def should_prune(self, epoch: int) -> bool:\n        \"\"\"åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œå‰ªæ\"\"\"\n        schedule = self.config['pruning_schedule']\n        start_epoch = schedule['start_epoch']\n        frequency = schedule['frequency']\n        \n        return epoch >= start_epoch and (epoch - start_epoch) % frequency == 0\n    \n    def compute_pruning_ratio(self, epoch: int) -> float:\n        \"\"\"è®¡ç®—å½“å‰epochçš„å‰ªææ¯”ä¾‹\"\"\"\n        schedule = self.config['pruning_schedule']\n        start_epoch = schedule['start_epoch']\n        initial_ratio = schedule['initial_ratio']\n        final_ratio = schedule['final_ratio']\n        strategy = schedule['strategy']\n        \n        if epoch < start_epoch:\n            return 0.0\n        \n        if strategy == 'gradual':\n            # æ¸è¿›å¼å‰ªæ\n            progress = min(1.0, (epoch - start_epoch) / 50)  # 50ä¸ªepochè¾¾åˆ°æœ€ç»ˆæ¯”ä¾‹\n            return initial_ratio + (final_ratio - initial_ratio) * progress\n        else:\n            # æ¿€è¿›å¼å‰ªæ\n            return final_ratio\n    \n    def apply_pruning_masks(self, masks: Dict[str, torch.Tensor]):\n        \"\"\"åº”ç”¨å‰ªææ©ç åˆ°å­¦ç”Ÿæ¨¡å‹\"\"\"\n        masked_params = 0\n        total_params = 0\n        \n        with torch.no_grad():\n            for name, param in self.student.named_parameters():\n                if name in masks:\n                    mask = masks[name]\n                    param.data *= mask\n                    \n                    masked_params += (mask == 0).sum().item()\n                    total_params += mask.numel()\n        \n        sparsity = masked_params / total_params if total_params > 0 else 0\n        logger.info(f\"åº”ç”¨å‰ªææ©ç : {masked_params:,}/{total_params:,} å‚æ•°è¢«å‰ªæ (ç¨€ç–åº¦: {sparsity:.1%})\")\n        \n        return sparsity\n    \n    def train_epoch(self, \n                   train_dataloader: DataLoader,\n                   optimizer: torch.optim.Optimizer,\n                   task_criterion: nn.Module,\n                   epoch: int) -> Dict[str, float]:\n        \"\"\"\n        è®­ç»ƒä¸€ä¸ªepoch\n        \n        Args:\n            train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n            optimizer: ä¼˜åŒ–å™¨\n            task_criterion: ä»»åŠ¡æŸå¤±å‡½æ•°\n            epoch: å½“å‰epoch\n            \n        Returns:\n            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯\n        \"\"\"\n        self.current_epoch = epoch\n        \n        # è®¾ç½®æ¨¡å‹æ¨¡å¼\n        self.teacher.eval()  # æ•™å¸ˆæ¨¡å‹å§‹ç»ˆè¯„ä¼°æ¨¡å¼\n        self.student.train()\n        \n        # ç»Ÿè®¡ä¿¡æ¯\n        epoch_stats = {\n            'total_loss': 0.0,\n            'kd_loss': 0.0,\n            'task_loss': 0.0,\n            'batch_count': 0\n        }\n        \n        # æ˜¯å¦éœ€è¦æ›´æ–°Fisher Information\n        update_fisher = self.should_update_fisher(epoch)\n        if update_fisher:\n            logger.info(f\"ğŸ“Š Epoch {epoch}: æ›´æ–°Fisher Information...\")\n            \n        for batch_idx, batch_data in enumerate(train_dataloader):\n            # æ•°æ®é¢„å¤„ç†\n            if isinstance(batch_data, (tuple, list)):\n                inputs, targets = batch_data\n            else:\n                inputs = batch_data\n                targets = None\n            \n            inputs = inputs.to(self.device)\n            if targets is not None:\n                targets = targets.to(self.device)\n            \n            # å‰å‘ä¼ æ’­\n            optimizer.zero_grad()\n            \n            # å­¦ç”Ÿæ¨¡å‹è¾“å‡º\n            student_outputs = self.student(inputs)\n            \n            # æ•™å¸ˆæ¨¡å‹è¾“å‡º\n            with torch.no_grad():\n                teacher_outputs = self.teacher(inputs)\n            \n            # è®¡ç®—æŸå¤±\n            if targets is not None:\n                loss_dict = self.compute_distillation_loss(\n                    student_outputs, teacher_outputs, targets, task_criterion\n                )\n            else:\n                # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œåªä½¿ç”¨KDæŸå¤±\n                kd_loss = F.mse_loss(student_outputs, teacher_outputs)\n                loss_dict = {\n                    'total_loss': kd_loss,\n                    'kd_loss': kd_loss,\n                    'task_loss': torch.tensor(0.0)\n                }\n            \n            # åå‘ä¼ æ’­\n            loss_dict['total_loss'].backward()\n            optimizer.step()\n            \n            # åº”ç”¨å‰ªææ©ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n            if self.pruning_masks:\n                self.apply_pruning_masks(self.pruning_masks)\n            \n            # æ›´æ–°ç»Ÿè®¡\n            epoch_stats['total_loss'] += loss_dict['total_loss'].item()\n            epoch_stats['kd_loss'] += loss_dict['kd_loss'].item()\n            epoch_stats['task_loss'] += loss_dict['task_loss'].item()\n            epoch_stats['batch_count'] += 1\n            \n            # æ‰“å°è¿›åº¦\n            if batch_idx % 10 == 0:\n                logger.debug(f\"Batch {batch_idx}: Loss={loss_dict['total_loss'].item():.4f}\")\n        \n        # è®¡ç®—å¹³å‡æŸå¤±\n        for key in ['total_loss', 'kd_loss', 'task_loss']:\n            epoch_stats[key] /= epoch_stats['batch_count']\n        \n        # æ›´æ–°Fisher Information\n        if update_fisher:\n            try:\n                self.fisher_info = self.fisher_calc.compute_fisher_information(\n                    train_dataloader, \n                    task_criterion,\n                    num_batches=self.config['fisher_config']['num_batches']\n                )\n                logger.info(f\"âœ… Fisher Informationæ›´æ–°å®Œæˆï¼Œè¦†ç›– {len(self.fisher_info)} å±‚\")\n            except Exception as e:\n                logger.warning(f\"Fisher Informationæ›´æ–°å¤±è´¥: {e}\")\n        \n        # æ‰§è¡Œå‰ªæ\n        if self.should_prune(epoch) and self.fisher_info:\n            pruning_ratio = self.compute_pruning_ratio(epoch)\n            logger.info(f\"âœ‚ï¸ Epoch {epoch}: æ‰§è¡Œå‰ªæï¼Œæ¯”ä¾‹ {pruning_ratio:.1%}\")\n            \n            try:\n                self.pruning_masks = self.fisher_calc.generate_pruning_mask(\n                    self.fisher_info, pruning_ratio, 'global'\n                )\n                sparsity = self.apply_pruning_masks(self.pruning_masks)\n                epoch_stats['sparsity'] = sparsity\n                logger.info(f\"âœ… å‰ªæå®Œæˆï¼Œå½“å‰ç¨€ç–åº¦: {sparsity:.1%}\")\n            except Exception as e:\n                logger.warning(f\"å‰ªææ‰§è¡Œå¤±è´¥: {e}\")\n        \n        return epoch_stats\n    \n    def evaluate(self, \n                val_dataloader: DataLoader,\n                task_criterion: nn.Module) -> Dict[str, float]:\n        \"\"\"\n        è¯„ä¼°æ¨¡å‹æ€§èƒ½\n        \n        Args:\n            val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨\n            task_criterion: ä»»åŠ¡æŸå¤±å‡½æ•°\n            \n        Returns:\n            è¯„ä¼°ç»“æœ\n        \"\"\"\n        self.student.eval()\n        self.teacher.eval()\n        \n        total_loss = 0.0\n        kd_loss = 0.0\n        task_loss = 0.0\n        batch_count = 0\n        \n        with torch.no_grad():\n            for batch_data in val_dataloader:\n                if isinstance(batch_data, (tuple, list)):\n                    inputs, targets = batch_data\n                else:\n                    inputs = batch_data\n                    targets = None\n                \n                inputs = inputs.to(self.device)\n                if targets is not None:\n                    targets = targets.to(self.device)\n                \n                # å‰å‘ä¼ æ’­\n                student_outputs = self.student(inputs)\n                teacher_outputs = self.teacher(inputs)\n                \n                # è®¡ç®—æŸå¤±\n                if targets is not None:\n                    loss_dict = self.compute_distillation_loss(\n                        student_outputs, teacher_outputs, targets, task_criterion\n                    )\n                else:\n                    kd_loss_val = F.mse_loss(student_outputs, teacher_outputs)\n                    loss_dict = {\n                        'total_loss': kd_loss_val,\n                        'kd_loss': kd_loss_val,\n                        'task_loss': torch.tensor(0.0)\n                    }\n                \n                total_loss += loss_dict['total_loss'].item()\n                kd_loss += loss_dict['kd_loss'].item()\n                task_loss += loss_dict['task_loss'].item()\n                batch_count += 1\n        \n        # è®¡ç®—ç¨€ç–åº¦\n        sparsity = 0.0\n        if self.pruning_masks:\n            total_params = sum(mask.numel() for mask in self.pruning_masks.values())\n            masked_params = sum((mask == 0).sum().item() for mask in self.pruning_masks.values())\n            sparsity = masked_params / total_params if total_params > 0 else 0.0\n        \n        return {\n            'val_total_loss': total_loss / batch_count,\n            'val_kd_loss': kd_loss / batch_count,\n            'val_task_loss': task_loss / batch_count,\n            'sparsity': sparsity\n        }\n    \n    def get_model_statistics(self) -> Dict[str, Any]:\n        \"\"\"è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯\"\"\"\n        student_params = sum(p.numel() for p in self.student.parameters())\n        teacher_params = sum(p.numel() for p in self.teacher.parameters())\n        \n        # æœ‰æ•ˆå‚æ•°ï¼ˆæœªè¢«å‰ªæï¼‰\n        effective_params = student_params\n        if self.pruning_masks:\n            effective_params = sum(mask.sum().item() for mask in self.pruning_masks.values())\n        \n        compression_ratio = effective_params / teacher_params\n        \n        return {\n            'student_params': student_params,\n            'teacher_params': teacher_params,\n            'effective_params': int(effective_params),\n            'compression_ratio': compression_ratio,\n            'sparsity': 1 - (effective_params / student_params),\n            'fisher_layers': len(self.fisher_info),\n            'pruning_masks': len(self.pruning_masks)\n        }\n    \n    def save_checkpoint(self, save_path: str, epoch: int, optimizer_state: Dict = None):\n        \"\"\"ä¿å­˜æ£€æŸ¥ç‚¹\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'student_state_dict': self.student.state_dict(),\n            'pruning_masks': self.pruning_masks,\n            'fisher_info': {k: v.cpu() for k, v in self.fisher_info.items()},\n            'config': self.config,\n            'model_stats': self.get_model_statistics()\n        }\n        \n        if optimizer_state:\n            checkpoint['optimizer_state_dict'] = optimizer_state\n        \n        torch.save(checkpoint, save_path)\n        logger.info(f\"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {save_path}\")\n    \n    def load_checkpoint(self, load_path: str) -> Dict:\n        \"\"\"åŠ è½½æ£€æŸ¥ç‚¹\"\"\"\n        checkpoint = torch.load(load_path, map_location=self.device)\n        \n        self.student.load_state_dict(checkpoint['student_state_dict'])\n        self.pruning_masks = checkpoint.get('pruning_masks', {})\n        self.fisher_info = {k: v.to(self.device) for k, v in checkpoint.get('fisher_info', {}).items()}\n        self.config.update(checkpoint.get('config', {}))\n        \n        logger.info(f\"ğŸ“‚ ä» {load_path} åŠ è½½æ£€æŸ¥ç‚¹\")\n        return checkpoint
