#!/usr/bin/env python3
"""
æµ‹è¯•Fisher Information per-layerè®¡ç®—æ¨¡å—
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/home/coder-gw/7Projects_in_7Days/online-inference-system')

from fisher_calculator import FisherInformationCalculator

class MockStudentModel(nn.Module):
    """ç®€å•çš„mockå­¦ç”Ÿæ¨¡å‹"""
    
    def __init__(self, embedding_dim=32, num_users=1000, num_items=1000):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.num_users = num_users
        self.num_items = num_items
        
        # ç”¨æˆ·å’Œç‰©å“åµŒå…¥
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # ç®€å•çš„é¢„æµ‹å¤´
        self.predictor = nn.Sequential(
            nn.Linear(embedding_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
    def forward(self, batch_data):
        """å¤„ç†batchæ•°æ®"""
        if isinstance(batch_data, (tuple, list)):
            user_ids, item_ids = batch_data[0], batch_data[1]
        else:
            # å‡è®¾è¾“å…¥æ˜¯[batch_size, 2]æ ¼å¼
            user_ids = batch_data[:, 0].long()
            item_ids = batch_data[:, 1].long()
            
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        combined = torch.cat([user_emb, item_emb], dim=-1)
        return self.predictor(combined).squeeze()

def create_mock_dataloader(batch_size=32, num_batches=10, num_users=1000, num_items=1000):
    """åˆ›å»ºmock DataLoader"""
    total_samples = batch_size * num_batches
    
    # åˆ›å»ºéšæœºç”¨æˆ·IDå’Œç‰©å“ID
    user_ids = torch.randint(0, num_users, (total_samples,))
    item_ids = torch.randint(0, num_items, (total_samples,))
    ratings = torch.rand(total_samples)
    
    # åˆ›å»ºè¾“å…¥å¼ é‡ [user_id, item_id]
    inputs = torch.stack([user_ids, item_ids], dim=1).float()
    
    dataset = TensorDataset(inputs, ratings)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    return dataloader

def test_fisher_calculator():
    """æµ‹è¯•Fisher Informationè®¡ç®—å™¨"""
    print("=== æµ‹è¯•Fisher Information per-layerè®¡ç®—å™¨ ===")
    
    # æ£€æŸ¥CUDA
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    student_model = MockStudentModel().to(device)
    print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in student_model.parameters())}")
    
    # åˆ›å»ºFisherè®¡ç®—å™¨
    fisher_calc = FisherInformationCalculator(student_model, device=str(device))
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = create_mock_dataloader(batch_size=32, num_batches=5)
    print(f"å‡†å¤‡äº†æ•°æ®åŠ è½½å™¨ï¼Œbatchæ•°é‡: 5")
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    
    # è®¡ç®—Fisher Information
    print("\n1. è®¡ç®—Fisher Information...")
    try:
        fisher_info = fisher_calc.compute_fisher_information(
            dataloader, criterion, num_batches=5
        )
        print(f"âœ“ Fisher Informationè®¡ç®—æˆåŠŸï¼")
        print(f"  - è¦†ç›–å±‚æ•°: {len(fisher_info)}")
        print(f"  - å±‚åç§°: {list(fisher_info.keys())[:3]}...")  # æ˜¾ç¤ºå‰3ä¸ª
        
        # æ£€æŸ¥æ¯å±‚çš„Fisherä¿¡æ¯
        for name, fisher_tensor in list(fisher_info.items())[:3]:
            print(f"  - {name}: shape={fisher_tensor.shape}, mean={fisher_tensor.mean():.6f}")
            
    except Exception as e:
        print(f"âœ— Fisher Informationè®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # åˆ†æé‡è¦æ€§
    print("\n2. åˆ†æå‚æ•°é‡è¦æ€§...")
    try:
        importance = fisher_calc.analyze_importance(fisher_info)
        print(f"âœ“ é‡è¦æ€§åˆ†ææˆåŠŸï¼")
        print(f"  - æ€»ä½“ç»Ÿè®¡: mean={importance['global_stats']['mean']:.6f}")
        print(f"  - å±‚çº§ç»Ÿè®¡æ•°é‡: {len(importance['layer_stats'])}")
        
        # æ˜¾ç¤ºæœ€é‡è¦çš„å‡ å±‚
        layer_means = {name: stats['mean'] for name, stats in importance['layer_stats'].items()}
        top_layers = sorted(layer_means.items(), key=lambda x: x[1], reverse=True)[:3]
        print(f"  - æœ€é‡è¦çš„3å±‚:")
        for name, mean_val in top_layers:
            print(f"    * {name}: {mean_val:.6f}")
            
    except Exception as e:
        print(f"âœ— é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # ç”Ÿæˆå‰ªææ©ç 
    print("\n3. ç”Ÿæˆå‰ªææ©ç ...")
    try:
        # æµ‹è¯•å…¨å±€ç­–ç•¥
        global_masks = fisher_calc.generate_pruning_mask(
            fisher_info, 0.2, 'global'
        )
        print("âœ“ å…¨å±€å‰ªææ©ç ç”ŸæˆæˆåŠŸï¼")
        print(f"  - æ©ç å±‚æ•°: {len(global_masks)}")
        
        # ç»Ÿè®¡å‰ªææ¯”ä¾‹
        total_params = 0
        pruned_params = 0
        for name, mask in global_masks.items():
            total_params += mask.numel()
            pruned_params += (mask == 0).sum().item()
        
        actual_ratio = pruned_params / total_params
        print(f"  - å®é™…å‰ªææ¯”ä¾‹: {actual_ratio:.3f} (ç›®æ ‡: 0.2)")
        
        # æµ‹è¯•é€å±‚ç­–ç•¥
        layer_masks = fisher_calc.generate_pruning_mask(
            fisher_info, 0.3, 'layer_wise'
        )
        print("âœ“ é€å±‚å‰ªææ©ç ç”ŸæˆæˆåŠŸï¼")
        print(f"  - æ©ç å±‚æ•°: {len(layer_masks)}")
        
    except Exception as e:
        print(f"âœ— å‰ªææ©ç ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æµ‹è¯•ç¨€ç–æ€§æ£€æŸ¥
    print("\n4. æµ‹è¯•ç¨€ç–æ€§æ£€æŸ¥...")
    try:
        sparsity_stats = fisher_calc.get_sparsity_stats(global_masks)
        print(f"âœ“ ç¨€ç–æ€§ç»Ÿè®¡æˆåŠŸï¼")
        print(f"  - æ€»ä½“ç¨€ç–åº¦: {sparsity_stats['overall_sparsity']:.3f}")
        print(f"  - éé›¶å‚æ•°æ¯”ä¾‹: {sparsity_stats['non_zero_ratio']:.3f}")
        print(f"  - å±‚çº§ç¨€ç–åº¦èŒƒå›´: {sparsity_stats['layer_sparsity_range']}")
        
    except Exception as e:
        print(f"âœ— ç¨€ç–æ€§æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\n=== Fisher Informationæ¨¡å—æµ‹è¯•å®Œæˆ ===")
    print("âœ“ æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼")
    return True

if __name__ == "__main__":
    success = test_fisher_calculator()
    if success:
        print("\nğŸ‰ Fisher Information per-layeræ¨¡å—å·²å°±ç»ªï¼")
        print("å¯ä»¥ç»§ç»­é›†æˆåˆ°è’¸é¦æµç¨‹ä¸­ã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜ã€‚")
