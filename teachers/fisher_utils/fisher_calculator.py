#!/usr/bin/env python3
"""
Fisher Information Per-Layerè®¡ç®—æ¨¡å—
ç”¨äºåˆ†æç¥ç»ç½‘ç»œå„å±‚å‚æ•°é‡è¦æ€§ï¼ŒæŒ‡å¯¼pruning-awareè’¸é¦

Fisher Information è¡¡é‡æ¨¡å‹å‚æ•°å¯¹è¾“å‡ºçš„æ•æ„Ÿåº¦:
F_ij = E[âˆ‚log p(y|x,Î¸)/âˆ‚Î¸_i * âˆ‚log p(y|x,Î¸)/âˆ‚Î¸_j]

ä½œè€…: GitHub Copilot  
æ—¥æœŸ: 2025-08-27
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
import logging
from collections import defaultdict
import json
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FisherInformationCalculator:
    """Fisher Information per-layerè®¡ç®—å™¨"""
    
    def __init__(self, model: nn.Module, device: str = 'auto'):
        """
        åˆå§‹åŒ–Fisherè®¡ç®—å™¨
        
        Args:
            model: è¦åˆ†æçš„PyTorchæ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.device = self._setup_device(device)
        self.model.to(self.device)
        
        # Fisherä¿¡æ¯å­˜å‚¨
        self.fisher_info = {}
        self.layer_names = []
        self.gradient_hooks = {}
        self.accumulated_gradients = {}
        
        # è®¡ç®—é…ç½®
        self.batch_count = 0
        self.fisher_samples = 0
        
        logger.info(f"åˆå§‹åŒ–Fisher Informationè®¡ç®—å™¨ï¼Œè®¾å¤‡: {self.device}")
        
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == 'auto':
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        return torch.device(device)
    
    def register_gradient_hooks(self):
        """æ³¨å†Œæ¢¯åº¦é’©å­å‡½æ•°"""
        logger.info("ğŸ”— æ³¨å†Œæ¢¯åº¦é’©å­...")
        
        def create_hook(name: str):
            def hook_fn(grad):
                if name not in self.accumulated_gradients:
                    self.accumulated_gradients[name] = []
                # å­˜å‚¨æ¢¯åº¦çš„å¹³æ–¹ï¼ˆFisher Informationçš„æ ¸å¿ƒï¼‰
                self.accumulated_gradients[name].append(grad.detach().clone() ** 2)
                return grad
            return hook_fn
        
        # ä¸ºæ‰€æœ‰å¯è®­ç»ƒå‚æ•°æ³¨å†Œé’©å­
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.layer_names.append(name)
                hook = param.register_hook(create_hook(name))
                self.gradient_hooks[name] = hook
                logger.debug(f"   æ³¨å†Œé’©å­: {name}, å½¢çŠ¶: {param.shape}")
        
        logger.info(f"âœ… æˆåŠŸæ³¨å†Œ {len(self.layer_names)} ä¸ªå‚æ•°çš„æ¢¯åº¦é’©å­")
    
    def compute_fisher_information(self, 
                                 dataloader: torch.utils.data.DataLoader,
                                 criterion: nn.Module,
                                 num_batches: Optional[int] = None,
                                 accumulate: bool = True) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—Fisher Information
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            criterion: æŸå¤±å‡½æ•°
            num_batches: é™åˆ¶æ‰¹æ¬¡æ•°é‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
            accumulate: æ˜¯å¦ç´¯ç§¯ä¹‹å‰çš„Fisherä¿¡æ¯
            
        Returns:
            æ¯å±‚çš„Fisher Informationå­—å…¸
        """
        logger.info("ğŸ§® å¼€å§‹è®¡ç®—Fisher Information...")
        
        if not accumulate:
            self.accumulated_gradients.clear()
            self.batch_count = 0
            self.fisher_samples = 0
        
        # ç¡®ä¿é’©å­å·²æ³¨å†Œ
        if not self.gradient_hooks:
            self.register_gradient_hooks()
        
        self.model.train()  # è®­ç»ƒæ¨¡å¼ä»¥è®¡ç®—æ¢¯åº¦
        
        batch_processed = 0
        total_samples = 0
        
        with torch.enable_grad():
            for batch_idx, batch_data in enumerate(dataloader):
                if num_batches and batch_idx >= num_batches:
                    break
                
                try:
                    # æ•°æ®é¢„å¤„ç†
                    if isinstance(batch_data, (tuple, list)):
                        if len(batch_data) == 2:
                            inputs, targets = batch_data
                        else:
                            inputs = batch_data[0]
                            targets = batch_data[1] if len(batch_data) > 1 else None
                    else:
                        inputs = batch_data
                        targets = None
                    
                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    inputs = self._move_to_device(inputs)
                    if targets is not None:
                        targets = self._move_to_device(targets)
                    
                    # å‰å‘ä¼ æ’­
                    self.model.zero_grad()
                    outputs = self.model(inputs)
                    
                    # è®¡ç®—æŸå¤±
                    if targets is not None:
                        loss = criterion(outputs, targets)
                    else:
                        # å¦‚æœæ²¡æœ‰ç›®æ ‡ï¼Œä½¿ç”¨è¾“å‡ºçš„å‡å€¼ä½œä¸ºæŸå¤±ï¼ˆæ— ç›‘ç£åœºæ™¯ï¼‰
                        loss = outputs.mean()
                    
                    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
                    loss.backward()
                    
                    batch_processed += 1
                    # å®‰å…¨åœ°è®¡ç®—æ ·æœ¬æ•°é‡
                    if torch.is_tensor(inputs):
                        total_samples += inputs.shape[0]
                    elif isinstance(inputs, (tuple, list)) and len(inputs) > 0:
                        first_tensor = inputs[0]
                        if torch.is_tensor(first_tensor):
                            total_samples += first_tensor.shape[0]
                        else:
                            total_samples += 1
                    else:
                        total_samples += 1
                    
                    if batch_idx % 50 == 0:
                        logger.debug(f"   å¤„ç†æ‰¹æ¬¡ {batch_idx}, æŸå¤±: {loss.item():.4f}")
                
                except Exception as e:
                    logger.warning(f"   æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue
        
        self.batch_count += batch_processed
        self.fisher_samples += total_samples
        
        logger.info(f"âœ… å®Œæˆ {batch_processed} ä¸ªæ‰¹æ¬¡ï¼Œ{total_samples} ä¸ªæ ·æœ¬çš„Fisherè®¡ç®—")
        
        # è®¡ç®—å¹³å‡Fisher Information
        return self._compute_average_fisher()
    
    def _move_to_device(self, data):
        """å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        if torch.is_tensor(data):
            return data.to(self.device)
        elif isinstance(data, dict):
            return {k: self._move_to_device(v) for k, v in data.items()}
        elif isinstance(data, (list, tuple)):
            return type(data)(self._move_to_device(item) for item in data)
        else:
            return data
    
    def _compute_average_fisher(self) -> Dict[str, torch.Tensor]:
        """è®¡ç®—å¹³å‡Fisher Information"""
        logger.info("ğŸ“Š è®¡ç®—å¹³å‡Fisher Information...")
        
        fisher_info = {}
        
        for name in self.layer_names:
            if name in self.accumulated_gradients and self.accumulated_gradients[name]:
                # è®¡ç®—æ¢¯åº¦å¹³æ–¹çš„å¹³å‡å€¼
                grad_squares = torch.stack(self.accumulated_gradients[name])
                fisher_info[name] = grad_squares.mean(dim=0)
                
                logger.debug(f"   {name}: Fisherå½¢çŠ¶ {fisher_info[name].shape}, "
                           f"å¹³å‡å€¼ {fisher_info[name].mean().item():.6f}")
            else:
                logger.warning(f"   {name}: æ²¡æœ‰æ”¶é›†åˆ°æ¢¯åº¦ä¿¡æ¯")
        
        self.fisher_info = fisher_info
        logger.info(f"âœ… è®¡ç®—å®Œæˆï¼Œè·å¾— {len(fisher_info)} å±‚çš„Fisherä¿¡æ¯")
        
        return fisher_info
    
    def analyze_importance(self, fisher_info: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        åˆ†æFisher Informationé‡è¦æ€§ç»Ÿè®¡
        
        Args:
            fisher_info: Fisher Informationå­—å…¸
            
        Returns:
            é‡è¦æ€§ç»Ÿè®¡ä¿¡æ¯
        """
        # å…¨å±€ç»Ÿè®¡
        all_values = []
        layer_stats = {}
        
        for name, tensor in fisher_info.items():
            values = tensor.flatten().cpu().numpy()
            all_values.extend(values)
            
            layer_stats[name] = {
                'mean': float(values.mean()),
                'std': float(values.std()),
                'min': float(values.min()),
                'max': float(values.max()),
                'median': float(np.median(values))
            }
        
        all_values = np.array(all_values)
        global_stats = {
            'mean': float(all_values.mean()),
            'std': float(all_values.std()),
            'min': float(all_values.min()),
            'max': float(all_values.max()),
            'median': float(np.median(all_values))
        }
        
        return {
            'global_stats': global_stats,
            'layer_stats': layer_stats
        }
    
    def get_sparsity_stats(self, masks: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        è·å–ç¨€ç–æ€§ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            masks: å‰ªææ©ç å­—å…¸
            
        Returns:
            ç¨€ç–æ€§ç»Ÿè®¡ä¿¡æ¯
        """
        total_params = 0
        zero_params = 0
        layer_sparsity = {}
        
        for name, mask in masks.items():
            layer_total = mask.numel()
            layer_zeros = (mask == 0).sum().item()
            layer_sparsity[name] = layer_zeros / layer_total
            
            total_params += layer_total
            zero_params += layer_zeros
        
        overall_sparsity = zero_params / total_params
        non_zero_ratio = 1.0 - overall_sparsity
        
        sparsity_values = list(layer_sparsity.values())
        sparsity_range = (min(sparsity_values), max(sparsity_values))
        
        return {
            'overall_sparsity': overall_sparsity,
            'non_zero_ratio': non_zero_ratio,
            'layer_sparsity': layer_sparsity,
            'layer_sparsity_range': sparsity_range,
            'total_params': total_params,
            'zero_params': zero_params
        }
    
    def analyze_layer_importance(self, 
                                normalize: bool = True, 
                                top_k: int = 10) -> Dict[str, Any]:
        """
        åˆ†æå„å±‚é‡è¦æ€§
        
        Args:
            normalize: æ˜¯å¦å½’ä¸€åŒ–é‡è¦æ€§åˆ†æ•°
            top_k: è¿”å›å‰Kä¸ªé‡è¦å±‚
            
        Returns:
            å±‚é‡è¦æ€§åˆ†æç»“æœ
        """
        logger.info("ğŸ“ˆ åˆ†æå±‚é‡è¦æ€§...")
        
        if not self.fisher_info:
            raise ValueError("è¯·å…ˆè®¡ç®—Fisher Information")
        
        # è®¡ç®—æ¯å±‚çš„é‡è¦æ€§åˆ†æ•°
        layer_importance = {}
        
        for name, fisher_tensor in self.fisher_info.items():
            # ä½¿ç”¨Fisherä¿¡æ¯çš„æ€»å’Œä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
            importance = fisher_tensor.sum().item()
            layer_importance[name] = importance
        
        # å½’ä¸€åŒ–
        if normalize and layer_importance:
            total_importance = sum(layer_importance.values())
            layer_importance = {k: v/total_importance for k, v in layer_importance.items()}
        
        # æ’åº
        sorted_layers = sorted(layer_importance.items(), key=lambda x: x[1], reverse=True)
        
        # åˆ†æç»“æœ
        analysis = {
            'layer_importance': dict(sorted_layers),
            'top_layers': sorted_layers[:top_k],
            'bottom_layers': sorted_layers[-top_k:],
            'importance_distribution': self._analyze_importance_distribution(layer_importance),
            'pruning_candidates': self._identify_pruning_candidates(layer_importance)
        }
        
        logger.info("ğŸ“‹ é‡è¦æ€§åˆ†æå®Œæˆ:")
        logger.info(f"   æœ€é‡è¦å±‚: {sorted_layers[0][0]} (é‡è¦æ€§: {sorted_layers[0][1]:.6f})")
        logger.info(f"   æœ€ä¸é‡è¦å±‚: {sorted_layers[-1][0]} (é‡è¦æ€§: {sorted_layers[-1][1]:.6f})")
        
        return analysis
    
    def _analyze_importance_distribution(self, layer_importance: Dict[str, float]) -> Dict[str, float]:
        """åˆ†æé‡è¦æ€§åˆ†å¸ƒ"""
        values = list(layer_importance.values())
        
        return {
            'mean': float(np.mean(values)),
            'std': float(np.std(values)),
            'min': float(np.min(values)),
            'max': float(np.max(values)),
            'median': float(np.median(values)),
            'q25': float(np.percentile(values, 25)),
            'q75': float(np.percentile(values, 75))
        }
    
    def _identify_pruning_candidates(self, 
                                   layer_importance: Dict[str, float],
                                   threshold_percentile: float = 20) -> List[str]:
        """è¯†åˆ«å‰ªæå€™é€‰å±‚"""
        values = list(layer_importance.values())
        threshold = np.percentile(values, threshold_percentile)
        
        candidates = [name for name, importance in layer_importance.items() 
                     if importance <= threshold]
        
        return candidates
    
    def compute_parameter_sensitivity(self, 
                                    sensitivity_type: str = 'absolute') -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å‚æ•°æ•æ„Ÿåº¦
        
        Args:
            sensitivity_type: 'absolute', 'relative', 'normalized'
            
        Returns:
            å‚æ•°æ•æ„Ÿåº¦å¼ é‡
        """
        logger.info(f"ğŸ¯ è®¡ç®—å‚æ•°æ•æ„Ÿåº¦ ({sensitivity_type})...")
        
        if not self.fisher_info:
            raise ValueError("è¯·å…ˆè®¡ç®—Fisher Information")
        
        sensitivity = {}
        
        for name, fisher_tensor in self.fisher_info.items():
            param = dict(self.model.named_parameters())[name]
            
            if sensitivity_type == 'absolute':
                # ç»å¯¹æ•æ„Ÿåº¦ï¼šç›´æ¥ä½¿ç”¨Fisherä¿¡æ¯
                sensitivity[name] = fisher_tensor
                
            elif sensitivity_type == 'relative':
                # ç›¸å¯¹æ•æ„Ÿåº¦ï¼šFisherä¿¡æ¯é™¤ä»¥å‚æ•°å€¼çš„å¹³æ–¹
                param_squared = param.detach() ** 2
                sensitivity[name] = fisher_tensor / (param_squared + 1e-8)
                
            elif sensitivity_type == 'normalized':
                # å½’ä¸€åŒ–æ•æ„Ÿåº¦ï¼šæŒ‰å±‚å†…æœ€å¤§å€¼å½’ä¸€åŒ–
                max_fisher = fisher_tensor.max()
                sensitivity[name] = fisher_tensor / (max_fisher + 1e-8)
                
            else:
                raise ValueError(f"æœªçŸ¥çš„æ•æ„Ÿåº¦ç±»å‹: {sensitivity_type}")
        
        logger.info(f"âœ… å®Œæˆ {len(sensitivity)} å±‚çš„æ•æ„Ÿåº¦è®¡ç®—")
        return sensitivity
    
    def generate_pruning_mask(self, 
                            fisher_info: Dict[str, torch.Tensor],
                            pruning_ratio: float = 0.2,
                            strategy: str = 'global') -> Dict[str, torch.Tensor]:
        """
        ç”Ÿæˆå‰ªææ©ç 
        
        Args:
            pruning_ratio: å‰ªææ¯”ä¾‹ (0-1)
            strategy: 'global' (å…¨å±€é˜ˆå€¼) æˆ– 'layer_wise' (é€å±‚é˜ˆå€¼)
            
        Returns:
            å‰ªææ©ç å­—å…¸
        """
        logger.info(f"âœ‚ï¸ ç”Ÿæˆå‰ªææ©ç  (æ¯”ä¾‹: {pruning_ratio:.1%}, ç­–ç•¥: {strategy})...")
        
        sensitivity = fisher_info  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„Fisherä¿¡æ¯
        masks = {}
        
        if strategy == 'global':
            # å…¨å±€é˜ˆå€¼ï¼šæ”¶é›†æ‰€æœ‰å‚æ•°çš„Fisherä¿¡æ¯
            all_fisher_values = []
            for fisher_tensor in sensitivity.values():
                all_fisher_values.extend(fisher_tensor.flatten().tolist())
            
            threshold = np.percentile(all_fisher_values, pruning_ratio * 100)
            
            for name, fisher_tensor in sensitivity.items():
                # ç¡®ä¿æ˜¯torch tensor
                if not torch.is_tensor(fisher_tensor):
                    fisher_tensor = torch.tensor(fisher_tensor, device=self.device)
                mask = fisher_tensor > threshold
                if torch.is_tensor(mask):
                    masks[name] = mask.to(torch.float32)
                else:
                    masks[name] = torch.tensor(mask, dtype=torch.float32, device=self.device)
                
        elif strategy == 'layer_wise':
            # é€å±‚é˜ˆå€¼ï¼šæ¯å±‚å•ç‹¬è®¡ç®—é˜ˆå€¼
            for name, fisher_tensor in sensitivity.items():
                # ç¡®ä¿æ˜¯torch tensor
                if not torch.is_tensor(fisher_tensor):
                    fisher_tensor = torch.tensor(fisher_tensor, device=self.device)
                layer_threshold = torch.quantile(fisher_tensor.flatten(), pruning_ratio)
                mask = fisher_tensor > layer_threshold
                if torch.is_tensor(mask):
                    masks[name] = mask.to(torch.float32)
                else:
                    masks[name] = torch.tensor(mask, dtype=torch.float32, device=self.device)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„å‰ªæç­–ç•¥: {strategy}")
        
        # ç»Ÿè®¡å‰ªæä¿¡æ¯
        total_params = sum(mask.numel() for mask in masks.values())
        kept_params = sum(mask.sum().item() for mask in masks.values())
        actual_pruning_ratio = 1 - (kept_params / total_params)
        
        logger.info(f"âœ… å‰ªææ©ç ç”Ÿæˆå®Œæˆ:")
        logger.info(f"   æ€»å‚æ•°: {total_params}")
        logger.info(f"   ä¿ç•™å‚æ•°: {int(kept_params)}")
        logger.info(f"   å®é™…å‰ªææ¯”ä¾‹: {actual_pruning_ratio:.1%}")
        
        return masks
    
    def save_fisher_info(self, save_path: str):
        """ä¿å­˜Fisher Information"""
        logger.info(f"ğŸ’¾ ä¿å­˜Fisher Informationåˆ°: {save_path}")
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'fisher_info': {name: tensor.cpu().numpy() for name, tensor in self.fisher_info.items()},
            'layer_names': self.layer_names,
            'batch_count': self.batch_count,
            'fisher_samples': self.fisher_samples,
            'model_info': {
                'model_class': self.model.__class__.__name__,
                'device': str(self.device)
            }
        }
        
        # ä¿å­˜ä¸ºnumpyæ ¼å¼ï¼ˆæ›´é«˜æ•ˆï¼‰
        np.savez_compressed(save_path, **save_data)
        logger.info("âœ… Fisher Informationä¿å­˜å®Œæˆ")
    
    def load_fisher_info(self, load_path: str):
        """åŠ è½½Fisher Information"""
        logger.info(f"ğŸ“‚ ä» {load_path} åŠ è½½Fisher Information...")
        
        data = np.load(load_path, allow_pickle=True)
        
        # æ¢å¤Fisherä¿¡æ¯
        self.fisher_info = {}
        fisher_data = data['fisher_info'].item()
        for name, array in fisher_data.items():
            self.fisher_info[name] = torch.from_numpy(array).to(self.device)
        
        self.layer_names = data['layer_names'].tolist()
        self.batch_count = int(data['batch_count'])
        self.fisher_samples = int(data['fisher_samples'])
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.fisher_info)} å±‚çš„Fisherä¿¡æ¯")
        logger.info(f"   åŸºäº {self.batch_count} æ‰¹æ¬¡ï¼Œ{self.fisher_samples} æ ·æœ¬")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†Fisherè®¡ç®—å™¨èµ„æº...")
        
        # ç§»é™¤æ¢¯åº¦é’©å­
        for hook in self.gradient_hooks.values():
            hook.remove()
        
        # æ¸…ç†ç´¯ç§¯çš„æ¢¯åº¦
        self.accumulated_gradients.clear()
        self.gradient_hooks.clear()
        
        logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")


def create_simple_model_for_testing():
    """åˆ›å»ºç®€å•æ¨¡å‹ç”¨äºæµ‹è¯•"""
    class SimpleRecommenderNet(nn.Module):
        def __init__(self, num_users=500, num_items=200, embedding_dim=32):
            super().__init__()
            self.user_embedding = nn.Embedding(num_users, embedding_dim)
            self.item_embedding = nn.Embedding(num_items, embedding_dim)
            
            self.fc_layers = nn.Sequential(
                nn.Linear(embedding_dim * 2, 128),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(64, 1)
            )
            
        def forward(self, user_ids, item_ids):
            user_emb = self.user_embedding(user_ids)
            item_emb = self.item_embedding(item_ids)
            
            combined = torch.cat([user_emb, item_emb], dim=-1)
            output = self.fc_layers(combined)
            
            return output.squeeze()
    
    return SimpleRecommenderNet()


def demo_fisher_information():
    """æ¼”ç¤ºFisher Informationè®¡ç®—"""
    print("ğŸ§® Fisher Information per-layer è®¡ç®—æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # 1. åˆ›å»ºç®€å•æ¨¡å‹
        print("ğŸ”§ åˆ›å»ºæµ‹è¯•æ¨¡å‹...")
        model = create_simple_model_for_testing()
        print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # 2. åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        print("\\nğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
        batch_size = 64
        num_batches = 20
        
        # æ¨¡æ‹Ÿç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®
        dataset = []
        for _ in range(num_batches):
            user_ids = torch.randint(0, 500, (batch_size,))
            item_ids = torch.randint(0, 200, (batch_size,))
            # æ¨¡æ‹Ÿè¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰
            ratings = torch.randint(1, 6, (batch_size,)).float()
            dataset.append(((user_ids, item_ids), ratings))
        
        dataloader = dataset  # ç®€åŒ–çš„æ•°æ®åŠ è½½å™¨
        
        # 3. åˆ›å»ºFisherè®¡ç®—å™¨
        print("\\nğŸ¯ åˆå§‹åŒ–Fisher Informationè®¡ç®—å™¨...")
        fisher_calc = FisherInformationCalculator(model)
        
        # 4. è®¡ç®—Fisher Information
        print("\\nğŸ§® è®¡ç®—Fisher Information...")
        criterion = nn.MSELoss()
        
        fisher_calc.register_gradient_hooks()
        
        # æ‰‹åŠ¨æ¨¡æ‹Ÿæ•°æ®åŠ è½½è¿‡ç¨‹
        model.train()
        for batch_idx, (inputs, targets) in enumerate(dataset):
            user_ids, item_ids = inputs
            
            model.zero_grad()
            outputs = model(user_ids, item_ids)
            loss = criterion(outputs, targets)
            loss.backward()
            
            if batch_idx % 5 == 0:
                print(f"   æ‰¹æ¬¡ {batch_idx}, æŸå¤±: {loss.item():.4f}")
        
        # æ‰‹åŠ¨è§¦å‘Fisherè®¡ç®—
        fisher_info = fisher_calc._compute_average_fisher()
        
        # 5. åˆ†æå±‚é‡è¦æ€§
        print("\\nğŸ“ˆ åˆ†æå±‚é‡è¦æ€§...")
        importance_analysis = fisher_calc.analyze_layer_importance()
        
        print("\\nğŸ† Top-5 é‡è¦å±‚:")
        for i, (layer_name, importance) in enumerate(importance_analysis['top_layers'][:5]):
            print(f"   {i+1}. {layer_name}: {importance:.6f}")
        
        print("\\nâš ï¸ Bottom-5 é‡è¦å±‚:")
        for i, (layer_name, importance) in enumerate(importance_analysis['bottom_layers'][:5]):
            print(f"   {i+1}. {layer_name}: {importance:.6f}")
        
        # 6. ç”Ÿæˆå‰ªææ©ç 
        print("\\nâœ‚ï¸ ç”Ÿæˆå‰ªææ©ç ...")
        pruning_masks = fisher_calc.generate_pruning_mask(fisher_info, pruning_ratio=0.3)
        
        print(f"   ç”Ÿæˆäº† {len(pruning_masks)} å±‚çš„å‰ªææ©ç ")
        
        # 7. å‚æ•°æ•æ„Ÿåº¦åˆ†æ
        print("\\nğŸ¯ å‚æ•°æ•æ„Ÿåº¦åˆ†æ...")
        sensitivity = fisher_calc.compute_parameter_sensitivity('relative')
        
        for name in list(sensitivity.keys())[:3]:  # æ˜¾ç¤ºå‰3å±‚
            sens_tensor = sensitivity[name]
            print(f"   {name}: æ•æ„Ÿåº¦èŒƒå›´ [{sens_tensor.min():.6f}, {sens_tensor.max():.6f}]")
        
        # 8. ä¿å­˜ç»“æœ
        print("\\nğŸ’¾ ä¿å­˜Fisher Information...")
        fisher_calc.save_fisher_info("fisher_info_demo.npz")
        
        print("\\nğŸ‰ Fisher Informationè®¡ç®—æ¼”ç¤ºå®Œæˆï¼")
        print("\\nğŸ“‹ å…³é”®å‘ç°:")
        dist = importance_analysis['importance_distribution']
        print(f"   é‡è¦æ€§åˆ†å¸ƒ: å‡å€¼={dist['mean']:.6f}, æ ‡å‡†å·®={dist['std']:.6f}")
        print(f"   å‰ªæå€™é€‰å±‚æ•°: {len(importance_analysis['pruning_candidates'])}")
        print("\\nğŸš€ å‡†å¤‡è¿›è¡Œpruning-awareè’¸é¦!")
        
        # æ¸…ç†èµ„æº
        fisher_calc.cleanup()
        
        return fisher_info, importance_analysis
        
    except Exception as e:
        print(f"\\nâŒ æ¼”ç¤ºå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None


if __name__ == "__main__":
    demo_fisher_information()
