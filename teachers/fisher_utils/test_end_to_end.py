#!/usr/bin/env python3
"""
Pruning-Aware Knowledge Distillation ç«¯åˆ°ç«¯æµ‹è¯•
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/home/coder-gw/7Projects_in_7Days/online-inference-system')

from pruning_distillation import PruningAwareDistillation

class TeacherModel(nn.Module):
    """æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰"""
    
    def __init__(self, num_users=500, num_items=200, embedding_dim=128):
        super().__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        self.mlp = nn.Sequential(
            nn.Linear(embedding_dim * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self._init_weights()
        
    def _init_weights(self):
        nn.init.normal_(self.user_embedding.weight, std=0.1)
        nn.init.normal_(self.item_embedding.weight, std=0.1)
        
        for layer in self.mlp:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.1)
    
    def forward(self, batch_data):
        if isinstance(batch_data, (tuple, list)):
            user_ids, item_ids = batch_data[0], batch_data[1]
        else:
            user_ids = batch_data[:, 0].long()
            item_ids = batch_data[:, 1].long()
            
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        features = torch.cat([user_emb, item_emb], dim=-1)
        output = self.mlp(features)
        
        return torch.sigmoid(output.squeeze())

class StudentModel(nn.Module):
    """å­¦ç”Ÿæ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼‰"""
    
    def __init__(self, num_users=500, num_items=200, embedding_dim=32):
        super().__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        self.mlp = nn.Sequential(
            nn.Linear(embedding_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
        self._init_weights()
        
    def _init_weights(self):
        nn.init.normal_(self.user_embedding.weight, std=0.1)
        nn.init.normal_(self.item_embedding.weight, std=0.1)
        
        for layer in self.mlp:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.1)
    
    def forward(self, batch_data):
        if isinstance(batch_data, (tuple, list)):
            user_ids, item_ids = batch_data[0], batch_data[1]
        else:
            user_ids = batch_data[:, 0].long()
            item_ids = batch_data[:, 1].long()
            
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        features = torch.cat([user_emb, item_emb], dim=-1)
        output = self.mlp(features)
        
        return torch.sigmoid(output.squeeze())

def load_movielens_data():
    """åŠ è½½MovieLensæ•°æ®"""
    data_path = '/home/coder-gw/7Projects_in_7Days/online-inference-system/data/movielens'
    ratings_file = os.path.join(data_path, 'ratings.csv')
    
    if not os.path.exists(ratings_file):
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {ratings_file}")
        return None
        
    ratings = pd.read_csv(ratings_file)
    print(f"âœ“ åŠ è½½ {len(ratings)} æ¡è¯„åˆ†æ•°æ®")
    
    # æ•°æ®é¢„å¤„ç†
    user_ids = ratings['user_id'].unique()
    item_ids = ratings['item_id'].unique()
    
    user_map = {uid: idx for idx, uid in enumerate(user_ids)}
    item_map = {iid: idx for idx, iid in enumerate(item_ids)}
    
    ratings['user_idx'] = ratings['user_id'].map(user_map)
    ratings['item_idx'] = ratings['item_id'].map(item_map)
    ratings['rating_norm'] = (ratings['rating'] - 1) / 4.0
    
    print(f"  - ç”¨æˆ·æ•°: {len(user_ids)}")
    print(f"  - ç‰©å“æ•°: {len(item_ids)}")
    
    return ratings, len(user_ids), len(item_ids)

def create_dataloaders(ratings_df, num_users, num_items, train_ratio=0.8, batch_size=64):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_size = int(len(ratings_df) * train_ratio)
    train_df = ratings_df.iloc[:train_size]
    val_df = ratings_df.iloc[train_size:]
    
    def create_dataloader(df, shuffle=True):
        user_ids = torch.tensor(df['user_idx'].values, dtype=torch.long)
        item_ids = torch.tensor(df['item_idx'].values, dtype=torch.long)
        ratings = torch.tensor(df['rating_norm'].values, dtype=torch.float32)
        
        inputs = torch.stack([user_ids, item_ids], dim=1).float()
        dataset = TensorDataset(inputs, ratings)
        
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    
    train_loader = create_dataloader(train_df, shuffle=True)
    val_loader = create_dataloader(val_df, shuffle=False)
    
    print(f"âœ“ åˆ›å»ºæ•°æ®åŠ è½½å™¨:")
    print(f"  - è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬, {len(train_loader)} æ‰¹æ¬¡")
    print(f"  - éªŒè¯é›†: {len(val_df)} æ ·æœ¬, {len(val_loader)} æ‰¹æ¬¡")
    
    return train_loader, val_loader

def test_pruning_aware_distillation():
    """æµ‹è¯•Pruning-Aware Knowledge Distillation"""
    print("=== Pruning-Aware Knowledge Distillation ç«¯åˆ°ç«¯æµ‹è¯• ===")
    
    # 1. åŠ è½½æ•°æ®
    print("\n1. åŠ è½½æ•°æ®...")
    result = load_movielens_data()
    if result is None:
        return False
        
    ratings_df, num_users, num_items = result
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n2. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader = create_dataloaders(ratings_df, num_users, num_items)
    
    # 3. æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\n4. åˆ›å»ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹...")
    teacher = TeacherModel(num_users, num_items, embedding_dim=128)
    student = StudentModel(num_users, num_items, embedding_dim=32)
    
    print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°: {sum(p.numel() for p in teacher.parameters()):,}")
    print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°: {sum(p.numel() for p in student.parameters()):,}")
    
    # 5. é…ç½®è’¸é¦å‚æ•°
    print("\n5. é…ç½®Pruning-Awareè’¸é¦...")
    distillation_config = {
        'temperature': 3.0,
        'alpha': 0.8,  # KDæŸå¤±æƒé‡
        'beta': 0.2,   # ä»»åŠ¡æŸå¤±æƒé‡
        'pruning_schedule': {
            'start_epoch': 2,  # æ—©ç‚¹å¼€å§‹å‰ªæç”¨äºæµ‹è¯•
            'frequency': 3,    # æ¯3ä¸ªepochå‰ªæä¸€æ¬¡
            'initial_ratio': 0.05,
            'final_ratio': 0.3,
            'strategy': 'gradual'
        },
        'fisher_config': {
            'update_frequency': 2,  # æ¯2ä¸ªepochæ›´æ–°Fisher
            'num_batches': 5,       # ä½¿ç”¨5ä¸ªbatchè®¡ç®—Fisher
            'accumulate': True
        }
    }
    
    # 6. åˆå§‹åŒ–è’¸é¦å™¨
    distiller = PruningAwareDistillation(
        teacher_model=teacher,
        student_model=student,
        device=str(device),
        distillation_config=distillation_config
    )
    
    # 7. è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    print("\n7. è®¾ç½®è®­ç»ƒç»„ä»¶...")
    optimizer = optim.Adam(student.parameters(), lr=0.001)
    task_criterion = nn.MSELoss()
    
    # 8. è®­ç»ƒå¾ªç¯
    print("\n8. å¼€å§‹è®­ç»ƒ...")
    num_epochs = 10
    
    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")
        
        # è®­ç»ƒ
        train_stats = distiller.train_epoch(train_loader, optimizer, task_criterion, epoch)
        
        # éªŒè¯
        val_stats = distiller.evaluate(val_loader, task_criterion)
        
        # è·å–æ¨¡å‹ç»Ÿè®¡
        model_stats = distiller.get_model_statistics()
        
        # æ‰“å°ç»“æœ
        print(f"è®­ç»ƒæŸå¤±: {train_stats['total_loss']:.4f} "
              f"(KD: {train_stats['kd_loss']:.4f}, Task: {train_stats['task_loss']:.4f})")
        print(f"éªŒè¯æŸå¤±: {val_stats['val_total_loss']:.4f}")
        print(f"å½“å‰ç¨€ç–åº¦: {model_stats['sparsity']:.1%}")
        print(f"å‹ç¼©æ¯”ç‡: {model_stats['compression_ratio']:.3f}")
        print(f"æœ‰æ•ˆå‚æ•°: {model_stats['effective_params']:,}/{model_stats['student_params']:,}")
    
    # 9. æœ€ç»ˆè¯„ä¼°
    print("\n9. æœ€ç»ˆè¯„ä¼°...")
    final_stats = distiller.get_model_statistics()
    
    print("=== æœ€ç»ˆç»“æœ ===")
    print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°: {final_stats['teacher_params']:,}")
    print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°: {final_stats['student_params']:,}")
    print(f"æœ‰æ•ˆå‚æ•°: {final_stats['effective_params']:,}")
    print(f"ç¨€ç–åº¦: {final_stats['sparsity']:.1%}")
    print(f"å‹ç¼©æ¯”ç‡: {final_stats['compression_ratio']:.3f}")
    print(f"Fisherå±‚æ•°: {final_stats['fisher_layers']}")
    print(f"å‰ªææ©ç æ•°: {final_stats['pruning_masks']}")
    
    # 10. ä¿å­˜æ¨¡å‹
    print("\n10. ä¿å­˜æ¨¡å‹...")
    save_path = "/home/coder-gw/7Projects_in_7Days/online-inference-system/teachers/fisher_utils/pruned_student_model.pt"
    distiller.save_checkpoint(save_path, num_epochs-1, optimizer.state_dict())
    
    print("\n=== Pruning-Aware Knowledge Distillation æµ‹è¯•å®Œæˆ ===")
    print("âœ… æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼")
    return True

if __name__ == "__main__":
    success = test_pruning_aware_distillation()
    if success:
        print("\nğŸ‰ Fisher Information + Pruning-Awareè’¸é¦ç³»ç»ŸéªŒè¯æˆåŠŸï¼")
        print("ç³»ç»Ÿå·²å…·å¤‡å®Œæ•´çš„è’¸é¦å’Œå‰ªæèƒ½åŠ›ã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
