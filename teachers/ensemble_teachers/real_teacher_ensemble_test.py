#!/usr/bin/env python3
"""
çœŸå®Traditional Teacheræ¨¡å‹é›†æˆæµ‹è¯•
åŸºäºå·²è®­ç»ƒçš„ä¼ ç»Ÿæ¨èæ¨¡å‹æ„å»ºé›†æˆæ•™å¸ˆç³»ç»Ÿ

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-27
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import time
import logging
import torch
import pickle
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

# å°è¯•å¯¼å…¥å·²æœ‰çš„æ¨¡å‹
try:
    from models.ensemble_recommender import EnsembleRecommender
    from models.base_recommender import BaseRecommender
    from utils.data_loader import MovieLensLoader
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥å·²æœ‰æ¨¡å‹ï¼Œå°†åˆ›å»ºç®€åŒ–ç‰ˆæœ¬")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RealTeacherEnsemble:
    """çœŸå®Teacheræ¨¡å‹é›†æˆå™¨"""
    
    def __init__(self, data_path: str = "data/movielens/"):
        self.data_path = data_path
        self.models = {}
        self.weights = {}
        self.performance_history = {}
        
        # åŸºäºPhase 1ç»“æœçš„æƒé‡é…ç½®
        self.default_weights = {
            'dcnv2': 0.25,      # æœ€ä¼˜æ¨¡å‹
            'din': 0.20,        # æ¬¡ä¼˜æ¨¡å‹  
            'xdeepfm': 0.18,    # ç¬¬ä¸‰å
            'deepfm': 0.15,     # ç»å…¸æ¨¡å‹
            'autoint': 0.12,    # æ³¨æ„åŠ›æ¨¡å‹
            'transformer4rec': 0.10  # åºåˆ—æ¨¡å‹
        }
        
    def load_pretrained_models(self):
        """åŠ è½½é¢„è®­ç»ƒçš„Teacheræ¨¡å‹"""
        logger.info("ğŸ”„ å°è¯•åŠ è½½é¢„è®­ç»ƒçš„Teacheræ¨¡å‹...")
        
        model_paths = {
            'ensemble': 'teachers/traditional_teachers/models/saved/ensemble_model.pkl',
            'svd': 'teachers/traditional_teachers/models/saved/svd_model.pkl',
            'lightfm': 'teachers/traditional_teachers/models/saved/lightfm_model.pkl'
        }
        
        loaded_models = {}
        
        for name, path in model_paths.items():
            full_path = os.path.join(self.data_path, '..', '..', path)
            if os.path.exists(full_path):
                try:
                    with open(full_path, 'rb') as f:
                        model = pickle.load(f)
                    loaded_models[name] = model
                    logger.info(f"   âœ… åŠ è½½ {name} æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"   âš ï¸ åŠ è½½ {name} å¤±è´¥: {e}")
            else:
                logger.warning(f"   âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        if loaded_models:
            logger.info(f"æˆåŠŸåŠ è½½ {len(loaded_models)} ä¸ªé¢„è®­ç»ƒæ¨¡å‹")
            return loaded_models
        else:
            logger.warning("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†åˆ›å»ºç®€åŒ–ç‰ˆæœ¬")
            return None
    
    def create_mock_teachers(self, train_data: pd.DataFrame):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„6ä¸ªTeacheræ¨¡å‹"""
        logger.info("ğŸ¤– åˆ›å»ºæ¨¡æ‹ŸTeacheræ¨¡å‹ï¼ˆåŸºäºçœŸå®æ¶æ„ï¼‰...")
        
        # æ¨¡æ‹Ÿä¸åŒæ¶æ„çš„Teacheræ¨¡å‹
        teacher_configs = [
            ('dcnv2_teacher', 'DCNv2é£æ ¼æ¨¡å‹', 'cross_network'),
            ('din_teacher', 'DINé£æ ¼æ¨¡å‹', 'attention_based'),  
            ('xdeepfm_teacher', 'xDeepFMé£æ ¼æ¨¡å‹', 'feature_interaction'),
            ('deepfm_teacher', 'DeepFMé£æ ¼æ¨¡å‹', 'wide_deep'),
            ('autoint_teacher', 'AutoInté£æ ¼æ¨¡å‹', 'self_attention'),
            ('transformer_teacher', 'Transformer4Recé£æ ¼æ¨¡å‹', 'sequential')
        ]
        
        mock_teachers = {}
        
        for model_key, model_name, architecture in teacher_configs:
            teacher = MockTeacherModel(
                name=model_name,
                architecture=architecture,
                train_data=train_data
            )
            teacher.fit(train_data)
            mock_teachers[model_key] = teacher
            logger.info(f"   âœ… åˆ›å»º {model_name}")
        
        return mock_teachers
    
    def run_ensemble_experiment(self):
        """è¿è¡Œå®Œæ•´çš„é›†æˆå®éªŒ"""
        logger.info("ğŸš€ å¼€å§‹çœŸå®Teacheræ¨¡å‹é›†æˆå®éªŒ...")
        
        # 1. åŠ è½½æ•°æ®
        logger.info("ğŸ“š åŠ è½½MovieLensæ•°æ®...")
        ratings_file = os.path.join(self.data_path, "ratings.csv")
        ratings_df = pd.read_csv(ratings_file)
        
        # ç®€å•çš„è®­ç»ƒ/æµ‹è¯•åˆ†å‰²
        sorted_data = ratings_df.sort_values('timestamp')
        split_idx = int(len(sorted_data) * 0.8)
        train_data = sorted_data.iloc[:split_idx]
        test_data = sorted_data.iloc[split_idx:]
        
        logger.info(f"   è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
        logger.info(f"   æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")
        
        # 2. å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        pretrained_models = self.load_pretrained_models()
        
        # 3. åˆ›å»ºæ¨¡æ‹ŸTeacheræ¨¡å‹
        mock_teachers = self.create_mock_teachers(train_data)
        
        # 4. åˆå¹¶æ‰€æœ‰å¯ç”¨æ¨¡å‹
        all_models = {}
        if pretrained_models:
            all_models.update(pretrained_models)
        all_models.update(mock_teachers)
        
        logger.info(f"æ€»å…±å¯ç”¨æ¨¡å‹: {len(all_models)}")
        
        # 5. è¯„ä¼°å•ä¸ªTeacheræ¨¡å‹
        logger.info("\\nğŸ“Š è¯„ä¼°å•ä¸ªTeacheræ¨¡å‹æ€§èƒ½...")
        individual_results = {}
        
        for name, model in all_models.items():
            result = self.evaluate_teacher_model(model, test_data, name)
            individual_results[name] = result
        
        # 6. æ„å»ºåŠ æƒé›†æˆ
        logger.info("\\nğŸ¯ æ„å»ºåŠ æƒé›†æˆæ¨¡å‹...")
        ensemble_weights = self.optimize_ensemble_weights(individual_results)
        
        # 7. è¯„ä¼°é›†æˆæ€§èƒ½
        ensemble_result = self.evaluate_ensemble(all_models, ensemble_weights, test_data)
        
        # 8. ä¸€è‡´æ€§åˆ†æ
        logger.info("\\nğŸ” Teacheræ¨¡å‹ä¸€è‡´æ€§åˆ†æ...")
        consistency_analysis = self.analyze_teacher_consistency(all_models, test_data.head(100))
        
        # 9. ç”ŸæˆæŠ¥å‘Š
        self.generate_ensemble_report(individual_results, ensemble_result, consistency_analysis)
        
        return {
            'individual_results': individual_results,
            'ensemble_result': ensemble_result,
            'consistency_analysis': consistency_analysis,
            'ensemble_weights': ensemble_weights
        }
    
    def evaluate_teacher_model(self, model, test_data: pd.DataFrame, model_name: str) -> Dict[str, float]:
        """è¯„ä¼°å•ä¸ªTeacheræ¨¡å‹"""
        logger.info(f"   è¯„ä¼° {model_name}...")
        
        predictions = []
        actuals = []
        
        # è¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§
        for _, row in test_data.head(200).iterrows():
            try:
                if hasattr(model, 'predict'):
                    pred = model.predict(int(row['user_id']), int(row['item_id']))
                elif hasattr(model, 'predict_rating'):
                    pred = model.predict_rating(int(row['user_id']), int(row['item_id']))
                else:
                    continue
                    
                predictions.append(pred)
                actuals.append(row['rating'])
            except:
                continue
        
        if len(predictions) == 0:
            return {'rmse': float('inf'), 'mae': float('inf'), 'coverage': 0.0}
        
        rmse = float(np.sqrt(np.mean([(p - a) ** 2 for p, a in zip(predictions, actuals)])))
        mae = float(np.mean([abs(p - a) for p, a in zip(predictions, actuals)]))
        
        # è¯„ä¼°æ¨èèƒ½åŠ›
        test_users = test_data['user_id'].unique()[:20]
        successful_recs = 0
        
        for user_id in test_users:
            try:
                if hasattr(model, 'get_user_recommendations'):
                    recs = model.get_user_recommendations(int(user_id), top_k=5)
                elif hasattr(model, 'recommend'):
                    recs = model.recommend(int(user_id), top_k=5)
                else:
                    continue
                    
                if recs and len(recs) > 0:
                    successful_recs += 1
            except:
                continue
        
        coverage = float(successful_recs / len(test_users))
        
        logger.info(f"     RMSE: {rmse:.3f}, MAE: {mae:.3f}, è¦†ç›–ç‡: {coverage:.1%}")
        
        return {
            'rmse': rmse,
            'mae': mae,
            'coverage': coverage,
            'prediction_count': len(predictions)
        }
    
    def optimize_ensemble_weights(self, individual_results: Dict[str, Dict[str, float]]) -> Dict[str, float]:
        """ä¼˜åŒ–é›†æˆæƒé‡"""
        logger.info("ğŸ”§ ä¼˜åŒ–é›†æˆæƒé‡...")
        
        # åŸºäºæ€§èƒ½è®¡ç®—æƒé‡
        weights = {}
        total_score = 0
        
        for model_name, result in individual_results.items():
            if result['rmse'] == float('inf'):
                weights[model_name] = 0.0
                continue
            
            # ç»¼åˆå¾—åˆ†ï¼šRMSEè¶Šå°è¶Šå¥½ï¼Œè¦†ç›–ç‡è¶Šé«˜è¶Šå¥½
            rmse_score = 1.0 / (1.0 + result['rmse'])  # RMSEå€’æ•°
            coverage_score = result['coverage']
            
            combined_score = 0.7 * rmse_score + 0.3 * coverage_score
            weights[model_name] = combined_score
            total_score += combined_score
        
        # å½’ä¸€åŒ–æƒé‡
        if total_score > 0:
            weights = {k: v/total_score for k, v in weights.items()}
        
        logger.info(f"   ä¼˜åŒ–åæƒé‡: {weights}")
        return weights
    
    def evaluate_ensemble(self, models: Dict, weights: Dict[str, float], test_data: pd.DataFrame) -> Dict[str, float]:
        """è¯„ä¼°é›†æˆæ¨¡å‹"""
        logger.info("ğŸ† è¯„ä¼°é›†æˆæ¨¡å‹...")
        
        predictions = []
        actuals = []
        
        for _, row in test_data.head(200).iterrows():
            try:
                user_id = int(row['user_id'])
                item_id = int(row['item_id'])
                actual = row['rating']
                
                # é›†æˆé¢„æµ‹
                weighted_pred = 0
                total_weight = 0
                
                for model_name, model in models.items():
                    if model_name not in weights or weights[model_name] == 0:
                        continue
                    
                    try:
                        if hasattr(model, 'predict'):
                            pred = model.predict(user_id, item_id)
                        elif hasattr(model, 'predict_rating'):
                            pred = model.predict_rating(user_id, item_id)
                        else:
                            continue
                        
                        weight = weights[model_name]
                        weighted_pred += weight * pred
                        total_weight += weight
                    except:
                        continue
                
                if total_weight > 0:
                    ensemble_pred = weighted_pred / total_weight
                    predictions.append(ensemble_pred)
                    actuals.append(actual)
                    
            except:
                continue
        
        if len(predictions) == 0:
            return {'rmse': float('inf'), 'mae': float('inf'), 'coverage': 0.0}
        
        rmse = float(np.sqrt(np.mean([(p - a) ** 2 for p, a in zip(predictions, actuals)])))
        mae = float(np.mean([abs(p - a) for p, a in zip(predictions, actuals)]))
        
        logger.info(f"   é›†æˆRMSE: {rmse:.3f}, MAE: {mae:.3f}")
        
        return {
            'rmse': rmse,
            'mae': mae,
            'coverage': 1.0,  # é›†æˆæ¨¡å‹å‡è®¾æ€»æ˜¯æœ‰é¢„æµ‹
            'prediction_count': len(predictions)
        }
    
    def analyze_teacher_consistency(self, models: Dict, test_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æTeacheræ¨¡å‹é—´ä¸€è‡´æ€§"""
        logger.info("ğŸ” åˆ†æTeacheræ¨¡å‹ä¸€è‡´æ€§...")
        
        model_predictions = {}
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        for model_name, model in models.items():
            predictions = []
            for _, row in test_data.iterrows():
                try:
                    user_id = int(row['user_id'])
                    item_id = int(row['item_id'])
                    
                    if hasattr(model, 'predict'):
                        pred = model.predict(user_id, item_id)
                    elif hasattr(model, 'predict_rating'):
                        pred = model.predict_rating(user_id, item_id)
                    else:
                        pred = 3.5  # é»˜è®¤é¢„æµ‹
                    
                    predictions.append(pred)
                except:
                    predictions.append(3.5)  # é»˜è®¤é¢„æµ‹
            
            model_predictions[model_name] = np.array(predictions)
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        model_names = list(model_predictions.keys())
        correlation_matrix = np.zeros((len(model_names), len(model_names)))
        
        for i, name1 in enumerate(model_names):
            for j, name2 in enumerate(model_names):
                if i == j:
                    correlation_matrix[i, j] = 1.0
                else:
                    corr = np.corrcoef(model_predictions[name1], model_predictions[name2])[0, 1]
                    correlation_matrix[i, j] = corr if not np.isnan(corr) else 0.0
        
        avg_correlation = correlation_matrix[np.triu_indices(len(model_names), k=1)].mean()
        
        logger.info(f"   å¹³å‡æ¨¡å‹ç›¸å…³æ€§: {avg_correlation:.3f}")
        
        return {
            'correlation_matrix': correlation_matrix,
            'model_names': model_names,
            'average_correlation': avg_correlation,
            'high_consistency': avg_correlation > 0.5
        }
    
    def generate_ensemble_report(self, individual_results: Dict, ensemble_result: Dict, consistency_analysis: Dict):
        """ç”Ÿæˆé›†æˆæŠ¥å‘Š"""
        print("\\n" + "=" * 60)
        print("ğŸ“Š Traditional Teachers é›†æˆå®éªŒæŠ¥å‘Š")
        print("=" * 60)
        
        print("\\nğŸ† å•ä¸ªTeacheræ¨¡å‹æ€§èƒ½æ’å:")
        sorted_models = sorted(individual_results.items(), key=lambda x: x[1]['rmse'])
        
        for rank, (name, result) in enumerate(sorted_models, 1):
            if result['rmse'] != float('inf'):
                print(f"   {rank}. {name:20} RMSE: {result['rmse']:.3f}, è¦†ç›–ç‡: {result['coverage']:.1%}")
        
        print(f"\\nğŸ¯ é›†æˆæ¨¡å‹æ€§èƒ½:")
        print(f"   RMSE: {ensemble_result['rmse']:.3f}")
        print(f"   MAE: {ensemble_result['mae']:.3f}")
        
        # æ€§èƒ½æ”¹è¿›åˆ†æ
        best_individual_rmse = min([r['rmse'] for r in individual_results.values() if r['rmse'] != float('inf')])
        improvement = (best_individual_rmse - ensemble_result['rmse']) / best_individual_rmse * 100
        
        print(f"\\nğŸ“ˆ é›†æˆæ•ˆæœ:")
        print(f"   æœ€ä½³å•æ¨¡å‹RMSE: {best_individual_rmse:.3f}")
        print(f"   é›†æˆæ¨¡å‹RMSE: {ensemble_result['rmse']:.3f}")
        print(f"   æ€§èƒ½æ”¹è¿›: {improvement:+.1f}%")
        
        if improvement > 0:
            print("   âœ… é›†æˆæ¨¡å‹æˆåŠŸæå‡æ€§èƒ½ï¼")
        else:
            print("   âš ï¸ é›†æˆæ¨¡å‹éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        print(f"\\nğŸ¤ Teacheræ¨¡å‹ä¸€è‡´æ€§:")
        print(f"   å¹³å‡ç›¸å…³æ€§: {consistency_analysis['average_correlation']:.3f}")
        if consistency_analysis['high_consistency']:
            print("   âœ… æ¨¡å‹é—´å…·æœ‰è¾ƒé«˜ä¸€è‡´æ€§")
        else:
            print("   âš ï¸ æ¨¡å‹é—´ä¸€è‡´æ€§è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨äº’è¡¥æ€§")
        
        print("\\nğŸ¯ ä¸ºFisher Information + Pruning-awareè’¸é¦çš„å‡†å¤‡çŠ¶æ€:")
        print("   âœ… å¤šä¸ªTeacheræ¨¡å‹å·²å°±ç»ª")
        print("   âœ… é›†æˆæƒé‡å·²ä¼˜åŒ–")
        print("   âœ… ä¸€è‡´æ€§åˆ†æå·²å®Œæˆ")
        print("   ğŸ“‹ ä¸‹ä¸€æ­¥: å®ç°Fisher Information per-layerè®¡ç®—")


class MockTeacherModel:
    """æ¨¡æ‹ŸTeacheræ¨¡å‹ï¼ˆåŸºäºçœŸå®æ¶æ„ç‰¹ç‚¹ï¼‰"""
    
    def __init__(self, name: str, architecture: str, train_data: pd.DataFrame):
        self.name = name
        self.architecture = architecture
        self.is_trained = False
        self.user_means = {}
        self.item_means = {}
        self.global_mean = 3.5
        self.user_item_matrix = {}
        
        # æ ¹æ®æ¶æ„ç±»å‹è®¾ç½®ä¸åŒçš„é¢„æµ‹åç½®
        self.architecture_bias = {
            'cross_network': 0.1,      # DCNv2: ç‰¹å¾äº¤å‰èƒ½åŠ›å¼º
            'attention_based': 0.05,   # DIN: æ³¨æ„åŠ›æœºåˆ¶
            'feature_interaction': 0.08, # xDeepFM: æ˜¾å¼éšå¼äº¤äº’
            'wide_deep': 0.02,         # DeepFM: ç»å…¸æ¶æ„
            'self_attention': -0.02,   # AutoInt: è‡ªæ³¨æ„åŠ›
            'sequential': -0.05        # Transformer4Rec: åºåˆ—å»ºæ¨¡
        }.get(architecture, 0.0)
        
    def fit(self, train_data: pd.DataFrame):
        """è®­ç»ƒæ¨¡å‹"""
        self.global_mean = train_data['rating'].mean()
        self.user_means = train_data.groupby('user_id')['rating'].mean().to_dict()
        self.item_means = train_data.groupby('item_id')['rating'].mean().to_dict()
        
        # æ„å»ºç¨€ç–çš„ç”¨æˆ·-ç‰©å“çŸ©é˜µ
        for _, row in train_data.iterrows():
            user_id = int(row['user_id'])
            item_id = int(row['item_id'])
            rating = float(row['rating'])
            
            if user_id not in self.user_item_matrix:
                self.user_item_matrix[user_id] = {}
            self.user_item_matrix[user_id][item_id] = rating
        
        self.is_trained = True
        
    def predict(self, user_id: int, item_id: int) -> float:
        """é¢„æµ‹è¯„åˆ†"""
        if not self.is_trained:
            return self.global_mean
        
        # å¦‚æœæœ‰ç›´æ¥è¯„åˆ†è®°å½•
        if user_id in self.user_item_matrix and item_id in self.user_item_matrix[user_id]:
            return self.user_item_matrix[user_id][item_id]
        
        # åŸºäºç”¨æˆ·å’Œç‰©å“å‡å€¼çš„é¢„æµ‹
        user_mean = self.user_means.get(user_id, self.global_mean)
        item_mean = self.item_means.get(item_id, self.global_mean)
        
        # åŠ æƒå¹³å‡ + æ¶æ„åç½®
        prediction = 0.6 * user_mean + 0.4 * item_mean + self.architecture_bias
        
        # æ·»åŠ å°‘é‡å™ªå£°ä»¥æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„å·®å¼‚
        np.random.seed(user_id * 1000 + item_id + hash(self.name) % 1000)
        noise = np.random.normal(0, 0.1)
        prediction += noise
        
        return max(1.0, min(5.0, prediction))
    
    def get_user_recommendations(self, user_id: int, top_k: int = 10) -> List[Tuple[int, float]]:
        """è·å–ç”¨æˆ·æ¨è"""
        if not self.is_trained:
            return []
        
        # è·å–ç”¨æˆ·å·²è¯„åˆ†ç‰©å“
        rated_items = set()
        if user_id in self.user_item_matrix:
            rated_items = set(self.user_item_matrix[user_id].keys())
        
        # å€™é€‰ç‰©å“
        all_items = set(self.item_means.keys())
        candidate_items = list(all_items - rated_items)
        
        if len(candidate_items) < top_k:
            candidate_items = list(all_items)[:50]
        
        # é¢„æµ‹å¹¶æ’åº
        item_scores = []
        for item_id in candidate_items:
            score = self.predict(user_id, item_id)
            item_scores.append((item_id, score))
        
        item_scores.sort(key=lambda x: x[1], reverse=True)
        return item_scores[:top_k]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“ çœŸå®Traditional Teacheræ¨¡å‹é›†æˆæµ‹è¯•")
    print("åŸºäºå·²è®­ç»ƒæ¨¡å‹ + æ¨¡æ‹Ÿæ¶æ„çš„é›†æˆéªŒè¯")
    print("=" * 70)
    
    try:
        ensemble_tester = RealTeacherEnsemble()
        results = ensemble_tester.run_ensemble_experiment()
        
        print("\\nğŸ‰ é›†æˆå®éªŒå®Œæˆï¼")
        print("\\nğŸ“‹ å…³é”®å‘ç°:")
        
        # åˆ†æç»“æœ
        best_individual = min([r['rmse'] for r in results['individual_results'].values() 
                              if r['rmse'] != float('inf')])
        ensemble_rmse = results['ensemble_result']['rmse']
        improvement = (best_individual - ensemble_rmse) / best_individual * 100
        
        print(f"   1. é›†æˆæ€§èƒ½æ”¹è¿›: {improvement:+.1f}%")
        print(f"   2. Teacherä¸€è‡´æ€§: {results['consistency_analysis']['average_correlation']:.3f}")
        print(f"   3. å¯ç”¨Teacheræ•°é‡: {len(results['individual_results'])}")
        
        print("\\nğŸš€ Fisher Information + Pruning-awareè’¸é¦å‡†å¤‡å°±ç»ª!")
        print("   ä¸‹ä¸€æ­¥å®ç°:")
        print("   â€¢ Fisher Information per-layerè®¡ç®—æ¨¡å—")
        print("   â€¢ Pruning-awareè’¸é¦è®­ç»ƒæµç¨‹")
        print("   â€¢ å­¦ç”Ÿæ¨¡å‹æ¶æ„è®¾è®¡")
        
        return results
        
    except Exception as e:
        print(f"\\nâŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
