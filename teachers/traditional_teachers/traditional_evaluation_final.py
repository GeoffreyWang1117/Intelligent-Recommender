#!/usr/bin/env python3
"""
ä¼ ç»Ÿæ¨èç®—æ³•Teacherså®Œæ•´è¯„ä¼° - æœ€ç»ˆä¿®å¤ç‰ˆ
Complete Traditional Teachers Evaluation - Final Fixed Version

åŸºäºçœŸå®MovieLensæ•°æ®ï¼Œä¿®å¤æ‰€æœ‰åˆ—åå’Œæ¥å£é—®é¢˜

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-18
"""

import sys
import os
import pandas as pd
import numpy as np
import json
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime

# æ·»åŠ æ¨¡å‹è·¯å¾„
sys.path.append('../../')
from models.algorithm_factory import create_recommender
from recommendation_metrics import RecommendationMetrics

warnings.filterwarnings('ignore')

class TraditionalTeachersEvaluator:
    """ä¼ ç»Ÿæ¨èç®—æ³•Teachersçš„å®Œæ•´è¯„ä¼°å™¨ - æœ€ç»ˆä¿®å¤ç‰ˆ"""
    
    def __init__(self, output_dir: str = "traditional_evaluation_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # æ¨èè¯„ä¼°æŒ‡æ ‡
        self.metrics = RecommendationMetrics()
        
        # æ•°æ®è·¯å¾„
        self.data_path = Path("../../data/sample")
        
        # ä¼ ç»Ÿæ¨¡å‹åˆ—è¡¨ (ä½¿ç”¨æ­£ç¡®çš„å°å†™åç§°)
        self.algorithms = [
            "deepfm",
            "autoint", 
            "transformer4rec",
            "xdeepfm",
            "din",
            "dcnv2"
        ]
        
        # è¯„ä¼°é…ç½®
        self.config = {
            "test_users_count": 20,        # æµ‹è¯•ç”¨æˆ·æ•°é‡
            "min_ratings_per_user": 10,    # æœ€å°‘è¯„åˆ†æ•°
            "train_ratio": 0.8,            # è®­ç»ƒé›†æ¯”ä¾‹
            "top_k": 10,                   # æ¨èæ•°é‡
            "relevance_threshold": 4.0     # ç›¸å…³æ€§é˜ˆå€¼
        }
        
        print(f"ğŸ¯ Traditional Teachersè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç®—æ³•æ•°é‡: {len(self.algorithms)}")
        print(f"   æ•°æ®è·¯å¾„: {self.data_path}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_movielens_data(self) -> Dict:
        """åŠ è½½MovieLensæ•°æ®"""
        print("ğŸ“ åŠ è½½MovieLensæ•°æ®...")
        
        try:
            # åŠ è½½æ•°æ®æ–‡ä»¶
            ratings_df = pd.read_csv(self.data_path / "ratings.csv")
            movies_df = pd.read_csv(self.data_path / "movies.csv") 
            users_df = pd.read_csv(self.data_path / "users.csv")
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
            print(f"   è¯„åˆ†è®°å½•: {len(ratings_df):,}")
            print(f"   ç”µå½±æ•°é‡: {len(movies_df):,}")
            print(f"   ç”¨æˆ·æ•°é‡: {len(users_df):,}")
            print(f"   è¯„åˆ†èŒƒå›´: {ratings_df['rating'].min():.1f} - {ratings_df['rating'].max():.1f}")
            print(f"   å¹³å‡è¯„åˆ†: {ratings_df['rating'].mean():.2f}")
            print(f"   æ•°æ®åˆ—å: {list(ratings_df.columns)}")
            
            return {
                "ratings": ratings_df,
                "movies": movies_df,
                "users": users_df
            }
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def prepare_train_test_data(self, data: Dict) -> Dict:
        """å‡†å¤‡è®­ç»ƒæµ‹è¯•æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡è®­ç»ƒæµ‹è¯•æ•°æ®...")
        
        ratings_df = data["ratings"]
        
        # é€‰æ‹©æ´»è·ƒç”¨æˆ· (è¯„åˆ†æ•°é‡è¶³å¤Ÿçš„ç”¨æˆ·)
        user_counts = ratings_df['user_id'].value_counts()
        active_users = user_counts[user_counts >= self.config["min_ratings_per_user"]].index.tolist()
        
        # è¿‡æ»¤æ•°æ®
        filtered_ratings = ratings_df[ratings_df['user_id'].isin(active_users)]
        
        # éšæœºé€‰æ‹©æµ‹è¯•ç”¨æˆ·
        test_users = np.random.choice(
            active_users, 
            size=min(self.config["test_users_count"], len(active_users)),
            replace=False
        ).tolist()
        
        # ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨æˆ·åˆ†å‰²è®­ç»ƒæµ‹è¯•æ•°æ®
        train_data = []
        test_data = {}
        
        for user_id in filtered_ratings['user_id'].unique():
            user_ratings = filtered_ratings[filtered_ratings['user_id'] == user_id]
            
            if user_id in test_users:
                # æµ‹è¯•ç”¨æˆ·ï¼š80%è®­ç»ƒï¼Œ20%æµ‹è¯•
                n_train = int(len(user_ratings) * self.config["train_ratio"])
                user_train = user_ratings.sample(n=n_train, random_state=42)
                user_test = user_ratings.drop(user_train.index)
                
                train_data.append(user_train)
                test_data[user_id] = user_test
            else:
                # éæµ‹è¯•ç”¨æˆ·ï¼šå…¨éƒ¨ç”¨äºè®­ç»ƒ
                train_data.append(user_ratings)
        
        train_df = pd.concat(train_data, ignore_index=True)
        
        print(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(train_df):,} æ¡è®°å½•")
        print(f"   æµ‹è¯•ç”¨æˆ·: {len(test_users)} ä¸ª")
        print(f"   æµ‹è¯•è®°å½•: {sum(len(df) for df in test_data.values())} æ¡")
        print(f"   è®­ç»ƒæ•°æ®åˆ—å: {list(train_df.columns)}")
        
        return {
            "train_df": train_df,
            "test_data": test_data,
            "test_users": test_users,
            "active_users": active_users
        }
    
    def train_single_model(self, algorithm: str, train_df: pd.DataFrame) -> Any:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"  ğŸ”§ è®­ç»ƒ {algorithm} æ¨¡å‹...")
        
        try:
            # æ¨¡å‹é…ç½®
            config = {
                "embedding_dim": 64,
                "hidden_dims": [512, 256, 128],
                "dropout": 0.2,
                "learning_rate": 0.001,
                "batch_size": 1024,
                "epochs": 5,  # å‡å°‘epochsåŠ å¿«è®­ç»ƒ
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            }
            
            # åˆ›å»ºæ¨¡å‹
            model = create_recommender(algorithm, **config)
            
            # æ•°æ®é¢„å¤„ç† - ç¡®ä¿æ ¼å¼æ­£ç¡®
            train_data = train_df.copy()
            
            print(f"    ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data.shape}")
            print(f"    ğŸ“Š è®­ç»ƒæ•°æ®åˆ—: {list(train_data.columns)}")
            
            # æ£€æŸ¥å¿…éœ€åˆ—
            required_columns = ['user_id', 'item_id', 'rating']
            for col in required_columns:
                if col not in train_data.columns:
                    print(f"    âŒ ç¼ºå°‘å¿…è¦åˆ— {col}")
                    print(f"    å¯ç”¨åˆ—: {list(train_data.columns)}")
                    return None
            
            # è®­ç»ƒæ¨¡å‹
            print(f"    ğŸƒ å¼€å§‹è®­ç»ƒ...")
            model.fit(train_data)
            
            print(f"    âœ… {algorithm} è®­ç»ƒå®Œæˆ")
            return model
            
        except Exception as e:
            print(f"    âŒ {algorithm} è®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    def generate_recommendations(self, model: Any, algorithm: str, 
                               test_users: List[int], train_df: pd.DataFrame) -> Dict:
        """ç”Ÿæˆæ¨èç»“æœ"""
        print(f"  ğŸ“‹ ç”Ÿæˆ {algorithm} æ¨è...")
        
        recommendations = {}
        
        try:
            # è·å–æ‰€æœ‰ç‰©å“
            all_items = train_df['item_id'].unique()
            print(f"    ğŸ“Š æ€»ç‰©å“æ•°: {len(all_items)}")
            
            for user_id in test_users:
                # è·å–ç”¨æˆ·å·²è¯„åˆ†ç‰©å“
                user_items = train_df[train_df['user_id'] == user_id]['item_id'].unique()
                
                # å€™é€‰ç‰©å“ (æ’é™¤å·²è¯„åˆ†)
                candidate_items = [item for item in all_items if item not in user_items]
                
                if len(candidate_items) > 0:
                    # ç”Ÿæˆæ¨è
                    try:
                        if hasattr(model, 'predict'):
                            # é¢„æµ‹è¯„åˆ†
                            predictions = []
                            for item_id in candidate_items[:100]:  # é™åˆ¶å€™é€‰æ•°é‡
                                try:
                                    score = model.predict(user_id, item_id)
                                    predictions.append((item_id, score))
                                except:
                                    continue
                            
                            # æ’åºå¹¶é€‰æ‹©top-k
                            if predictions:
                                predictions.sort(key=lambda x: x[1], reverse=True)
                                recommended_items = [item_id for item_id, _ in predictions[:self.config["top_k"]]]
                                recommendations[user_id] = recommended_items
                        
                        elif hasattr(model, 'recommend'):
                            # ç›´æ¥æ¨è
                            recommended_items = model.recommend(user_id, self.config["top_k"])
                            recommendations[user_id] = recommended_items
                        
                        else:
                            print(f"    âš ï¸ {algorithm} æ²¡æœ‰é¢„æµ‹æˆ–æ¨èæ–¹æ³•")
                            # å°è¯•éšæœºæ¨èä½œä¸ºfallback
                            random_items = np.random.choice(candidate_items, 
                                                          size=min(self.config["top_k"], len(candidate_items)), 
                                                          replace=False)
                            recommendations[user_id] = random_items.tolist()
                            
                    except Exception as e:
                        print(f"    âš ï¸ ç”¨æˆ·{user_id}æ¨èå¤±è´¥: {str(e)}")
                        continue
        
        except Exception as e:
            print(f"    âŒ {algorithm} æ¨èç”Ÿæˆå¤±è´¥: {str(e)}")
        
        success_rate = len(recommendations) / len(test_users) * 100 if test_users else 0
        print(f"    âœ… {algorithm} æ¨èå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1f}%")
        print(f"    ğŸ“Š æˆåŠŸæ¨èç”¨æˆ·æ•°: {len(recommendations)}")
        
        return recommendations
    
    def evaluate_model_performance(self, algorithm: str, recommendations: Dict, 
                                 test_data: Dict, train_df: pd.DataFrame) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"  ğŸ“Š è¯„ä¼° {algorithm} æ€§èƒ½...")
        
        try:
            # å‡†å¤‡è¯„ä¼°æ•°æ®
            eval_data = self._prepare_evaluation_data(recommendations, test_data, train_df)
            
            if not eval_data or not eval_data.get("true_ratings"):
                print(f"    âŒ {algorithm} è¯„ä¼°æ•°æ®å‡†å¤‡å¤±è´¥")
                return {}
            
            print(f"    ğŸ“Š è¯„ä¼°æ•°æ®ç»Ÿè®¡:")
            print(f"       çœŸå®è¯„åˆ†æ•°: {len(eval_data['true_ratings'])}")
            print(f"       é¢„æµ‹è¯„åˆ†æ•°: {len(eval_data['predicted_ratings'])}")
            print(f"       æµ‹è¯•ç”¨æˆ·æ•°: {len(eval_data['test_users_relevant'])}")
            
            # æ‰§è¡Œç»¼åˆè¯„ä¼°
            results = self.metrics.comprehensive_evaluation(
                eval_data["true_ratings"],
                eval_data["predicted_ratings"],
                eval_data["test_users_relevant"],  # è¿™æ˜¯å…³é”® - ç”¨æˆ·åˆ°ç›¸å…³ç‰©å“çš„æ˜ å°„
                eval_data["recommendations"],
                eval_data["item_features"],
                eval_data["item_popularity"],
                eval_data["all_items"]
            )
            
            print(f"    âœ… {algorithm} æ€§èƒ½è¯„ä¼°å®Œæˆ")
            return results
            
        except Exception as e:
            print(f"    âŒ {algorithm} æ€§èƒ½è¯„ä¼°å¤±è´¥: {str(e)}")
            import traceback
            print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
    def _prepare_evaluation_data(self, recommendations: Dict, test_data: Dict, 
                               train_df: pd.DataFrame) -> Dict:
        """å‡†å¤‡è¯„ä¼°æ•°æ®æ ¼å¼"""
        
        # çœŸå®è¯„åˆ†
        true_ratings = []
        for user_id, user_test_df in test_data.items():
            for _, row in user_test_df.iterrows():
                true_ratings.append((user_id, row['item_id'], row['rating']))
        
        # é¢„æµ‹è¯„åˆ† (ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨æ¨èæ’åºä½œä¸ºè¯„åˆ†)
        predicted_ratings = []
        for user_id, rec_items in recommendations.items():
            for i, item_id in enumerate(rec_items):
                # æ ¹æ®æ¨èæ’åºç”Ÿæˆä¼ªè¯„åˆ† (5åˆ°1)
                score = max(1, 5 - i // 2)
                predicted_ratings.append((user_id, item_id, score))
        
        # æµ‹è¯•ç”¨æˆ·ç›¸å…³ç‰©å“ (å…³é”®ä¿®å¤ï¼)
        test_users_relevant = {}
        for user_id, user_test_df in test_data.items():
            # è·å–è¯„åˆ† >= é˜ˆå€¼çš„ç‰©å“ä½œä¸ºç›¸å…³ç‰©å“
            relevant_items = user_test_df[
                user_test_df['rating'] >= self.config['relevance_threshold']
            ]['item_id'].tolist()
            test_users_relevant[user_id] = relevant_items
        
        # ç‰©å“ç‰¹å¾ (ç®€åŒ–)
        all_items = train_df['item_id'].unique()
        item_features = {item_id: np.random.randn(10) for item_id in all_items}
        
        # ç‰©å“æµè¡Œåº¦
        item_popularity = train_df['item_id'].value_counts().to_dict()
        
        return {
            "true_ratings": true_ratings,
            "predicted_ratings": predicted_ratings,
            "test_users_relevant": test_users_relevant,  # è¿™æ˜¯å…³é”®ä¿®å¤
            "recommendations": recommendations,
            "item_features": item_features,
            "item_popularity": item_popularity,
            "all_items": all_items.tolist()
        }
    
    def run_complete_evaluation(self) -> Dict:
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹Traditional Teacherså®Œæ•´è¯„ä¼°...")
        print("=" * 60)
        
        start_time = datetime.now()
        results = {
            "experiment_info": {
                "start_time": start_time.isoformat(),
                "algorithms": self.algorithms,
                "config": self.config
            },
            "models": {},
            "summary": {}
        }
        
        try:
            # 1. åŠ è½½æ•°æ®
            data = self.load_movielens_data()
            
            # 2. å‡†å¤‡è®­ç»ƒæµ‹è¯•æ•°æ®
            train_test_data = self.prepare_train_test_data(data)
            
            # 3. é€ä¸ªè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
            successful_models = 0
            
            for algorithm in self.algorithms:
                print(f"\nğŸ“ˆ å¤„ç† {algorithm} ç®—æ³•...")
                print("-" * 40)
                
                model_results = {
                    "algorithm": algorithm,
                    "training_status": "failed",
                    "evaluation_results": {},
                    "recommendations_count": 0
                }
                
                # è®­ç»ƒæ¨¡å‹
                model = self.train_single_model(algorithm, train_test_data["train_df"])
                
                if model is not None:
                    model_results["training_status"] = "success"
                    
                    # ç”Ÿæˆæ¨è
                    recommendations = self.generate_recommendations(
                        model, algorithm, train_test_data["test_users"], train_test_data["train_df"]
                    )
                    
                    model_results["recommendations_count"] = len(recommendations)
                    
                    if recommendations:
                        # è¯„ä¼°æ€§èƒ½
                        evaluation = self.evaluate_model_performance(
                            algorithm, recommendations, train_test_data["test_data"], train_test_data["train_df"]
                        )
                        
                        model_results["evaluation_results"] = evaluation
                        
                        if evaluation:
                            successful_models += 1
                
                results["models"][algorithm] = model_results
            
            # 4. ç”Ÿæˆæ€»ç»“
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            results["summary"] = {
                "total_algorithms": len(self.algorithms),
                "successful_trainings": sum(1 for r in results["models"].values() if r["training_status"] == "success"),
                "successful_evaluations": successful_models,
                "total_duration_seconds": duration,
                "end_time": end_time.isoformat()
            }
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(results)
            self.print_summary(results)
            
            return results
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            results["error"] = str(e)
            return results
    
    def save_results(self, results: Dict):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.output_dir / f"traditional_evaluation_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_file}")
    
    def print_summary(self, results: Dict):
        """æ‰“å°è¯„ä¼°æ€»ç»“"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Traditional Teachersè¯„ä¼°æ€»ç»“")
        print("=" * 60)
        
        summary = results["summary"]
        print(f"ğŸ¯ ç®—æ³•æ€»æ•°: {summary['total_algorithms']}")
        print(f"âœ… è®­ç»ƒæˆåŠŸ: {summary['successful_trainings']}")
        print(f"ğŸ“ˆ è¯„ä¼°æˆåŠŸ: {summary['successful_evaluations']}")
        print(f"â±ï¸  æ€»è€—æ—¶: {summary['total_duration_seconds']:.1f} ç§’")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for algorithm, model_result in results["models"].items():
            status = "âœ…" if model_result["training_status"] == "success" else "âŒ"
            rec_count = model_result["recommendations_count"]
            print(f"   {status} {algorithm}: æ¨èæ•° {rec_count}")
            
            if model_result["evaluation_results"]:
                eval_results = model_result["evaluation_results"]
                if "accuracy" in eval_results:
                    acc = eval_results["accuracy"]
                    if "rmse" in acc:
                        print(f"       RMSE: {acc['rmse']:.4f}")
                if "ranking" in eval_results:
                    rank = eval_results["ranking"]
                    if "ndcg_10" in rank:
                        print(f"       NDCG@10: {rank['ndcg_10']:.4f}")

def main():
    """ä¸»å‡½æ•°"""
    
    # å¯¼å…¥torch (éœ€è¦åœ¨è¿™é‡Œå¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ–æ—¶çš„ä¾èµ–é—®é¢˜)
    global torch
    try:
        import torch
        print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"ğŸ® CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸš€ GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œ
    evaluator = TraditionalTeachersEvaluator()
    results = evaluator.run_complete_evaluation()
    
    # æ£€æŸ¥ç»“æœ
    if "error" not in results:
        print(f"\nğŸ‰ Traditional Teachersè¯„ä¼°å®Œæˆ!")
        print(f"ğŸ¯ æˆåŠŸè¯„ä¼°ç®—æ³•æ•°: {results['summary']['successful_evaluations']}")
    else:
        print(f"\nğŸ’¥ è¯„ä¼°å¤±è´¥: {results['error']}")

if __name__ == "__main__":
    main()
