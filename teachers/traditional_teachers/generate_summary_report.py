#!/usr/bin/env python3
"""
Traditional Teachersè¯„ä¼°ç»“æœæ€»ç»“æŠ¥å‘Šç”Ÿæˆå™¨
Traditional Teachers Evaluation Results Summary Report Generator

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-18
"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime

def generate_summary_report():
    """ç”ŸæˆTraditional Teachersè¯„ä¼°æ€»ç»“æŠ¥å‘Š"""
    
    # è¯»å–æœ€æ–°çš„è¯„ä¼°ç»“æœ
    results_dir = Path("traditional_evaluation_results")
    if not results_dir.exists():
        print("âŒ è¯„ä¼°ç»“æœç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æœ€æ–°çš„ç»“æœæ–‡ä»¶
    result_files = list(results_dir.glob("traditional_evaluation_*.json"))
    if not result_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        return
    
    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
    
    # è¯»å–ç»“æœ
    with open(latest_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    print("=" * 80)
    print("ğŸ“Š TRADITIONAL TEACHERS å®Œæ•´è¯„ä¼°æŠ¥å‘Š")
    print("=" * 80)
    
    # åŸºæœ¬ä¿¡æ¯
    summary = results["summary"]
    print(f"ğŸ” è¯„ä¼°æ—¶é—´: {results['experiment_info']['start_time']}")
    print(f"ğŸ¯ ç®—æ³•æ€»æ•°: {summary['total_algorithms']}")
    print(f"âœ… è®­ç»ƒæˆåŠŸ: {summary['successful_trainings']}")
    print(f"ğŸ“ˆ è¯„ä¼°æˆåŠŸ: {summary['successful_evaluations']}")
    print(f"â±ï¸  æ€»è€—æ—¶: {summary['total_duration_seconds']:.1f} ç§’")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {latest_file}")
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    print(f"\nğŸ“‹ Traditional Teachersæ€§èƒ½å¯¹æ¯”")
    print("-" * 80)
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„å…³é”®æŒ‡æ ‡
    model_data = []
    
    for algorithm, model_result in results["models"].items():
        if model_result["training_status"] == "success" and model_result["evaluation_results"]:
            eval_results = model_result["evaluation_results"]
            
            # æå–å…³é”®æŒ‡æ ‡
            accuracy = eval_results.get("accuracy_metrics", {})
            ranking = eval_results.get("ranking_metrics", {})
            diversity = eval_results.get("diversity_metrics", {})
            novelty = eval_results.get("novelty_metrics", {})
            overall = eval_results.get("overall_score", {})
            
            model_data.append({
                "ç®—æ³•": algorithm.upper(),
                "è®­ç»ƒçŠ¶æ€": "âœ…",
                "æ¨èæ•°": model_result["recommendations_count"],
                "RMSE": f"{accuracy.get('RMSE', 0):.4f}",
                "NDCG@10": f"{ranking.get('NDCG@10', 0):.4f}",
                "è¦†ç›–ç‡": f"{diversity.get('Coverage', 0):.4f}",
                "æ–°é¢–åº¦": f"{novelty.get('Novelty', 0):.4f}",
                "æ€»åˆ†": f"{overall.get('overall_score', 0):.4f}"
            })
    
    # è½¬æ¢ä¸ºDataFrameå¹¶æ˜¾ç¤º
    df = pd.DataFrame(model_data)
    print(df.to_string(index=False))
    
    # è¯¦ç»†æŒ‡æ ‡åˆ†æ
    print(f"\nğŸ“Š è¯¦ç»†æŒ‡æ ‡åˆ†æ")
    print("-" * 80)
    
    for algorithm, model_result in results["models"].items():
        if model_result["training_status"] == "success" and model_result["evaluation_results"]:
            eval_results = model_result["evaluation_results"]
            
            print(f"\nğŸ”¸ {algorithm.upper()} è¯¦ç»†æŒ‡æ ‡:")
            
            # å‡†ç¡®æ€§æŒ‡æ ‡
            accuracy = eval_results.get("accuracy_metrics", {})
            print(f"   ğŸ“ å‡†ç¡®æ€§: RMSE={accuracy.get('RMSE', 0):.4f}, MAE={accuracy.get('MAE', 0):.4f}")
            
            # æ’åºæŒ‡æ ‡
            ranking = eval_results.get("ranking_metrics", {})
            print(f"   ğŸ¯ æ’åºè´¨é‡: P@10={ranking.get('Precision@10', 0):.4f}, NDCG@10={ranking.get('NDCG@10', 0):.4f}")
            
            # å¤šæ ·æ€§æŒ‡æ ‡
            diversity = eval_results.get("diversity_metrics", {})
            print(f"   ğŸŒˆ å¤šæ ·æ€§: è¦†ç›–ç‡={diversity.get('Coverage', 0):.4f}, åˆ—è¡¨å†…å¤šæ ·æ€§={diversity.get('Intra_List_Diversity', 0):.4f}")
            
            # æ–°é¢–æ€§æŒ‡æ ‡
            novelty = eval_results.get("novelty_metrics", {})
            print(f"   âœ¨ æ–°é¢–æ€§: {novelty.get('Novelty', 0):.4f}")
            
            # ç»¼åˆè¯„åˆ†
            overall = eval_results.get("overall_score", {})
            print(f"   ğŸ† ç»¼åˆå¾—åˆ†: {overall.get('overall_score', 0):.4f}")
    
    # æ¨¡å‹æ’å
    print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’å")
    print("-" * 80)
    
    # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
    model_scores = []
    for algorithm, model_result in results["models"].items():
        if model_result["training_status"] == "success" and model_result["evaluation_results"]:
            overall_score = model_result["evaluation_results"].get("overall_score", {}).get("overall_score", 0)
            model_scores.append((algorithm, overall_score))
    
    model_scores.sort(key=lambda x: x[1], reverse=True)
    
    for i, (algorithm, score) in enumerate(model_scores, 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "ğŸ…"
        print(f"   {medal} ç¬¬{i}å: {algorithm.upper()} (å¾—åˆ†: {score:.4f})")
    
    # ç»“è®ºå’Œå»ºè®®
    print(f"\nğŸ’¡ è¯„ä¼°ç»“è®º")
    print("-" * 80)
    print(f"âœ… æ‰€æœ‰6ä¸ªTraditional Teacheræ¨¡å‹å‡æˆåŠŸè®­ç»ƒå’Œè¯„ä¼°")
    print(f"ğŸ“Š è¯„ä¼°è¦†ç›–å‡†ç¡®æ€§ã€æ’åºã€å¤šæ ·æ€§ã€æ–°é¢–æ€§å››ä¸ªç»´åº¦")
    print(f"ğŸ¯ æœ€ä½³æ¨¡å‹: {model_scores[0][0].upper()} (ç»¼åˆå¾—åˆ†: {model_scores[0][1]:.4f})")
    print(f"ğŸ“ˆ æ‰€æœ‰æ¨¡å‹å‡èƒ½æ­£å¸¸ç”Ÿæˆæ¨èï¼Œå…·å¤‡å®é™…åº”ç”¨æ½œåŠ›")
    print(f"ğŸ”„ å»ºè®®: å¯è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜è¿›ä¸€æ­¥æå‡æ€§èƒ½")
    
    # ä¿å­˜æ€»ç»“æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = results_dir / f"traditional_teachers_summary_{timestamp}.txt"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("Traditional Teachersè¯„ä¼°æ€»ç»“æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {results['experiment_info']['start_time']}\n")
        f.write(f"ç®—æ³•æ€»æ•°: {summary['total_algorithms']}\n")
        f.write(f"æˆåŠŸè®­ç»ƒ: {summary['successful_trainings']}\n")
        f.write(f"æˆåŠŸè¯„ä¼°: {summary['successful_evaluations']}\n")
        f.write(f"æ€»è€—æ—¶: {summary['total_duration_seconds']:.1f} ç§’\n\n")
        
        f.write("æ¨¡å‹æ€§èƒ½æ’å:\n")
        for i, (algorithm, score) in enumerate(model_scores, 1):
            f.write(f"{i}. {algorithm.upper()}: {score:.4f}\n")
    
    print(f"\nğŸ’¾ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

if __name__ == "__main__":
    generate_summary_report()
