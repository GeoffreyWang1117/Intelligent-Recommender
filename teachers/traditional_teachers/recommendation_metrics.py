#!/usr/bin/env python3
"""
æ¨èç³»ç»Ÿè¯„ä»·æŒ‡æ ‡æ¨¡å—
Recommendation System Evaluation Metrics

å®ç°æ ‡å‡†çš„æ¨èç³»ç»Ÿè¯„ä»·æŒ‡æ ‡ï¼š
- å‡†ç¡®æ€§æŒ‡æ ‡: RMSE, MAE, Precision, Recall, F1-Score
- æ’åºæŒ‡æ ‡: NDCG, MAP, MRR
- å¤šæ ·æ€§æŒ‡æ ‡: Coverage, Diversity, Novelty
- é²æ£’æ€§æŒ‡æ ‡: Serendipity, Popularity Bias

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-08-18
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Set, Any
from collections import defaultdict, Counter
import math
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy import sparse
from scipy.spatial.distance import cosine
import warnings
warnings.filterwarnings('ignore')

class RecommendationMetrics:
    """æ¨èç³»ç»Ÿè¯„ä»·æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self):
        self.metrics_results = {}
    
    def calculate_accuracy_metrics(self, true_ratings: List[Tuple[int, int, float]], 
                                 predicted_ratings: List[Tuple[int, int, float]]) -> Dict[str, float]:
        """
        è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡
        
        Args:
            true_ratings: [(user_id, item_id, rating), ...]
            predicted_ratings: [(user_id, item_id, predicted_rating), ...]
        
        Returns:
            Dict with RMSE, MAE, etc.
        """
        # åˆ›å»ºè¯„åˆ†æ˜ å°„
        true_dict = {(user, item): rating for user, item, rating in true_ratings}
        pred_dict = {(user, item): rating for user, item, rating in predicted_ratings}
        
        # æ‰¾åˆ°äº¤é›†
        common_pairs = set(true_dict.keys()) & set(pred_dict.keys())
        
        if not common_pairs:
            return {"error": "æ²¡æœ‰å…±åŒçš„ç”¨æˆ·-ç‰©å“å¯¹"}
        
        true_values = [true_dict[pair] for pair in common_pairs]
        pred_values = [pred_dict[pair] for pair in common_pairs]
        
        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(true_values, pred_values))
        mae = mean_absolute_error(true_values, pred_values)
        
        # ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(true_values, pred_values)[0, 1] if len(true_values) > 1 else 0
        
        return {
            "RMSE": round(rmse, 4),
            "MAE": round(mae, 4),
            "Correlation": round(correlation, 4),
            "Coverage": len(common_pairs)
        }
    
    def calculate_ranking_metrics(self, test_users: Dict[int, List[int]], 
                                 recommendations: Dict[int, List[int]], 
                                 relevance_threshold: float = 4.0,
                                 k_values: List[int] = [5, 10, 20]) -> Dict[str, Dict[int, float]]:
        """
        è®¡ç®—æ’åºæŒ‡æ ‡
        
        Args:
            test_users: {user_id: [relevant_item_ids]}
            recommendations: {user_id: [recommended_item_ids]}
            relevance_threshold: ç›¸å…³æ€§é˜ˆå€¼
            k_values: è®¡ç®—top-kæŒ‡æ ‡çš„kå€¼åˆ—è¡¨
        
        Returns:
            Dict with Precision@K, Recall@K, NDCG@K, MAP@K
        """
        results = {}
        
        for k in k_values:
            precision_scores = []
            recall_scores = []
            ndcg_scores = []
            ap_scores = []
            
            for user_id in test_users:
                if user_id not in recommendations:
                    continue
                
                relevant_items = set(test_users[user_id])
                recommended_items = recommendations[user_id][:k]
                
                # Precision@K
                if recommended_items:
                    relevant_recommended = len(set(recommended_items) & relevant_items)
                    precision = relevant_recommended / len(recommended_items)
                    precision_scores.append(precision)
                
                # Recall@K
                if relevant_items:
                    relevant_recommended = len(set(recommended_items) & relevant_items)
                    recall = relevant_recommended / len(relevant_items)
                    recall_scores.append(recall)
                
                # NDCG@K
                ndcg = self._calculate_ndcg(recommended_items, relevant_items, k)
                ndcg_scores.append(ndcg)
                
                # Average Precision
                ap = self._calculate_average_precision(recommended_items, relevant_items)
                ap_scores.append(ap)
            
            results[f"Precision@{k}"] = np.mean(precision_scores) if precision_scores else 0
            results[f"Recall@{k}"] = np.mean(recall_scores) if recall_scores else 0
            results[f"NDCG@{k}"] = np.mean(ndcg_scores) if ndcg_scores else 0
            results[f"MAP@{k}"] = np.mean(ap_scores) if ap_scores else 0
            results[f"F1@{k}"] = self._calculate_f1(
                results[f"Precision@{k}"], results[f"Recall@{k}"]
            )
        
        return {metric: round(value, 4) for metric, value in results.items()}
    
    def _calculate_ndcg(self, recommended_items: List[int], 
                       relevant_items: Set[int], k: int) -> float:
        """è®¡ç®—NDCG@K"""
        if not recommended_items or not relevant_items:
            return 0.0
        
        # DCG
        dcg = 0.0
        for i, item in enumerate(recommended_items[:k]):
            if item in relevant_items:
                dcg += 1.0 / math.log2(i + 2)  # i+2 because log2(1) is undefined
        
        # IDCG (ç†æƒ³æƒ…å†µä¸‹çš„DCG)
        idcg = 0.0
        for i in range(min(k, len(relevant_items))):
            idcg += 1.0 / math.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def _calculate_average_precision(self, recommended_items: List[int], 
                                   relevant_items: Set[int]) -> float:
        """è®¡ç®—Average Precision"""
        if not recommended_items or not relevant_items:
            return 0.0
        
        relevant_count = 0
        precision_sum = 0.0
        
        for i, item in enumerate(recommended_items):
            if item in relevant_items:
                relevant_count += 1
                precision_at_i = relevant_count / (i + 1)
                precision_sum += precision_at_i
        
        return precision_sum / len(relevant_items) if relevant_items else 0.0
    
    def _calculate_f1(self, precision: float, recall: float) -> float:
        """è®¡ç®—F1åˆ†æ•°"""
        if precision + recall == 0:
            return 0.0
        return 2 * precision * recall / (precision + recall)
    
    def calculate_diversity_metrics(self, recommendations: Dict[int, List[int]], 
                                  item_features: Dict[int, List[str]], 
                                  all_items: Set[int]) -> Dict[str, float]:
        """
        è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡
        
        Args:
            recommendations: {user_id: [recommended_item_ids]}
            item_features: {item_id: [feature_list]} (e.g., genres)
            all_items: æ‰€æœ‰å¯æ¨èçš„ç‰©å“é›†åˆ
        
        Returns:
            Dict with Coverage, Intra-list Diversity, Inter-list Diversity
        """
        results = {}
        
        # 1. Coverage (æ¨èè¦†ç›–ç‡)
        recommended_items = set()
        for user_recs in recommendations.values():
            recommended_items.update(user_recs)
        
        coverage = len(recommended_items) / len(all_items) if all_items else 0
        results["Coverage"] = round(coverage, 4)
        
        # 2. Intra-list Diversity (åˆ—è¡¨å†…å¤šæ ·æ€§)
        intra_diversities = []
        for user_id, recs in recommendations.items():
            if len(recs) > 1:
                diversity = self._calculate_intra_list_diversity(recs, item_features)
                intra_diversities.append(diversity)
        
        results["Intra_List_Diversity"] = round(np.mean(intra_diversities), 4) if intra_diversities else 0
        
        # 3. Gini Coefficient (æ¨èåˆ†å¸ƒå‡åŒ€æ€§)
        item_counts = Counter()
        for user_recs in recommendations.values():
            item_counts.update(user_recs)
        
        gini = self._calculate_gini_coefficient(list(item_counts.values()))
        results["Gini_Coefficient"] = round(gini, 4)
        
        return results
    
    def _calculate_intra_list_diversity(self, items: List[int], 
                                      item_features: Dict[int, List[str]]) -> float:
        """è®¡ç®—æ¨èåˆ—è¡¨å†…çš„å¤šæ ·æ€§"""
        if len(items) < 2:
            return 0.0
        
        similarities = []
        for i in range(len(items)):
            for j in range(i + 1, len(items)):
                item1, item2 = items[i], items[j]
                if item1 in item_features and item2 in item_features:
                    sim = self._jaccard_similarity(
                        set(item_features[item1]), 
                        set(item_features[item2])
                    )
                    similarities.append(sim)
        
        # å¤šæ ·æ€§ = 1 - å¹³å‡ç›¸ä¼¼åº¦
        avg_similarity = np.mean(similarities) if similarities else 0
        return 1 - avg_similarity
    
    def _jaccard_similarity(self, set1: Set, set2: Set) -> float:
        """è®¡ç®—Jaccardç›¸ä¼¼åº¦"""
        if not set1 and not set2:
            return 1.0
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0.0
    
    def _calculate_gini_coefficient(self, values: List[int]) -> float:
        """è®¡ç®—åŸºå°¼ç³»æ•° (è¡¡é‡åˆ†å¸ƒä¸å‡åŒ€ç¨‹åº¦)"""
        if not values:
            return 0.0
        
        values = sorted(values)
        n = len(values)
        cumsum = np.cumsum(values)
        
        return (n + 1 - 2 * sum((n + 1 - i) * y for i, y in enumerate(values))) / (n * sum(values))
    
    def calculate_novelty_metrics(self, recommendations: Dict[int, List[int]], 
                                item_popularity: Dict[int, float],
                                popularity_threshold: float = 0.1) -> Dict[str, float]:
        """
        è®¡ç®—æ–°é¢–æ€§æŒ‡æ ‡
        
        Args:
            recommendations: {user_id: [recommended_item_ids]}
            item_popularity: {item_id: popularity_score} (0-1)
            popularity_threshold: æµè¡Œåº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è¢«è®¤ä¸ºæ˜¯æ–°é¢–çš„
        
        Returns:
            Dict with Novelty, Serendipity metrics
        """
        results = {}
        
        novelty_scores = []
        serendipity_scores = []
        
        for user_id, recs in recommendations.items():
            # Novelty: æ¨èéæµè¡Œç‰©å“çš„æ¯”ä¾‹
            novel_items = [item for item in recs 
                          if item_popularity.get(item, 0) < popularity_threshold]
            novelty = len(novel_items) / len(recs) if recs else 0
            novelty_scores.append(novelty)
            
            # Serendipity: å¹³å‡è´Ÿå¯¹æ•°æµè¡Œåº¦
            if recs:
                pop_scores = [item_popularity.get(item, 0.01) for item in recs]  # é¿å…log(0)
                serendipity = -np.mean([math.log(max(pop, 0.001)) for pop in pop_scores])
                serendipity_scores.append(serendipity)
        
        results["Novelty"] = round(np.mean(novelty_scores), 4) if novelty_scores else 0
        results["Serendipity"] = round(np.mean(serendipity_scores), 4) if serendipity_scores else 0
        
        return results
    
    def comprehensive_evaluation(self, 
                               true_ratings: List[Tuple[int, int, float]],
                               predicted_ratings: List[Tuple[int, int, float]],
                               test_users: Dict[int, List[int]],
                               recommendations: Dict[int, List[int]],
                               item_features: Dict[int, List[str]],
                               item_popularity: Dict[int, float],
                               all_items: Set[int]) -> Dict[str, Any]:
        """
        ç»¼åˆè¯„ä¼°æ¨èç³»ç»Ÿæ€§èƒ½
        
        Returns:
            Dict containing all evaluation metrics
        """
        print("ğŸ“Š å¼€å§‹ç»¼åˆè¯„ä¼°...")
        
        results = {
            "evaluation_timestamp": pd.Timestamp.now().isoformat(),
            "dataset_info": {
                "num_users": len(set([x[0] for x in true_ratings])),
                "num_items": len(set([x[1] for x in true_ratings])),
                "num_ratings": len(true_ratings),
                "num_recommendations": sum(len(recs) for recs in recommendations.values())
            }
        }
        
        # 1. å‡†ç¡®æ€§æŒ‡æ ‡
        print("  è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡...")
        accuracy_metrics = self.calculate_accuracy_metrics(true_ratings, predicted_ratings)
        results["accuracy_metrics"] = accuracy_metrics
        
        # 2. æ’åºæŒ‡æ ‡
        print("  è®¡ç®—æ’åºæŒ‡æ ‡...")
        ranking_metrics = self.calculate_ranking_metrics(test_users, recommendations)
        results["ranking_metrics"] = ranking_metrics
        
        # 3. å¤šæ ·æ€§æŒ‡æ ‡
        print("  è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡...")
        diversity_metrics = self.calculate_diversity_metrics(recommendations, item_features, all_items)
        results["diversity_metrics"] = diversity_metrics
        
        # 4. æ–°é¢–æ€§æŒ‡æ ‡
        print("  è®¡ç®—æ–°é¢–æ€§æŒ‡æ ‡...")
        novelty_metrics = self.calculate_novelty_metrics(recommendations, item_popularity)
        results["novelty_metrics"] = novelty_metrics
        
        # 5. ç»¼åˆæŒ‡æ ‡
        results["overall_score"] = self._calculate_overall_score(
            accuracy_metrics, ranking_metrics, diversity_metrics, novelty_metrics
        )
        
        print("âœ… ç»¼åˆè¯„ä¼°å®Œæˆ")
        return results
    
    def _calculate_overall_score(self, accuracy: Dict, ranking: Dict, 
                               diversity: Dict, novelty: Dict) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        
        # æå–å…³é”®æŒ‡æ ‡
        rmse = accuracy.get("RMSE", 5.0)  # è¶Šå°è¶Šå¥½
        precision_10 = ranking.get("Precision@10", 0)  # è¶Šå¤§è¶Šå¥½
        recall_10 = ranking.get("Recall@10", 0)  # è¶Šå¤§è¶Šå¥½
        ndcg_10 = ranking.get("NDCG@10", 0)  # è¶Šå¤§è¶Šå¥½
        coverage = diversity.get("Coverage", 0)  # è¶Šå¤§è¶Šå¥½
        intra_diversity = diversity.get("Intra_List_Diversity", 0)  # è¶Šå¤§è¶Šå¥½
        novelty_score = novelty.get("Novelty", 0)  # è¶Šå¤§è¶Šå¥½
        
        # æ ‡å‡†åŒ–åˆ†æ•° (0-1)
        accuracy_score = max(0, 1 - rmse / 5.0)  # RMSEæ ‡å‡†åŒ–
        relevance_score = (precision_10 + recall_10 + ndcg_10) / 3  # ç›¸å…³æ€§å¹³å‡
        diversity_score = (coverage + intra_diversity) / 2  # å¤šæ ·æ€§å¹³å‡
        novelty_score = novelty_score  # å·²ç»æ˜¯0-1èŒƒå›´
        
        # åŠ æƒç»¼åˆåˆ†æ•°
        overall = (
            0.4 * relevance_score +      # ç›¸å…³æ€§æƒé‡40%
            0.3 * accuracy_score +       # å‡†ç¡®æ€§æƒé‡30%
            0.2 * diversity_score +      # å¤šæ ·æ€§æƒé‡20%
            0.1 * novelty_score          # æ–°é¢–æ€§æƒé‡10%
        )
        
        return {
            "overall_score": round(overall, 4),
            "accuracy_score": round(accuracy_score, 4),
            "relevance_score": round(relevance_score, 4),
            "diversity_score": round(diversity_score, 4),
            "novelty_score": round(novelty_score, 4)
        }
    
    def print_evaluation_report(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ¨èç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š")
        print("="*60)
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = results["dataset_info"]
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   ç”¨æˆ·æ•°: {dataset_info['num_users']}")
        print(f"   ç‰©å“æ•°: {dataset_info['num_items']}")  
        print(f"   è¯„åˆ†æ•°: {dataset_info['num_ratings']}")
        print(f"   æ¨èæ•°: {dataset_info['num_recommendations']}")
        
        # å‡†ç¡®æ€§æŒ‡æ ‡
        print(f"\nğŸ¯ å‡†ç¡®æ€§æŒ‡æ ‡:")
        accuracy = results["accuracy_metrics"]
        for metric, value in accuracy.items():
            if metric != "error":
                print(f"   {metric}: {value}")
        
        # æ’åºæŒ‡æ ‡
        print(f"\nğŸ“ˆ æ’åºæŒ‡æ ‡:")
        ranking = results["ranking_metrics"]
        for metric, value in ranking.items():
            print(f"   {metric}: {value}")
        
        # å¤šæ ·æ€§æŒ‡æ ‡
        print(f"\nğŸŒˆ å¤šæ ·æ€§æŒ‡æ ‡:")
        diversity = results["diversity_metrics"]
        for metric, value in diversity.items():
            print(f"   {metric}: {value}")
        
        # æ–°é¢–æ€§æŒ‡æ ‡
        print(f"\nğŸ’« æ–°é¢–æ€§æŒ‡æ ‡:")
        novelty = results["novelty_metrics"]
        for metric, value in novelty.items():
            print(f"   {metric}: {value}")
        
        # ç»¼åˆè¯„åˆ†
        print(f"\nğŸ† ç»¼åˆè¯„åˆ†:")
        overall = results["overall_score"]
        for metric, value in overall.items():
            print(f"   {metric}: {value}")
        
        # æ€§èƒ½ç­‰çº§è¯„å®š
        overall_score = overall["overall_score"]
        if overall_score >= 0.8:
            grade = "ğŸ¥‡ ä¼˜ç§€"
        elif overall_score >= 0.6:
            grade = "ğŸ¥ˆ è‰¯å¥½"
        elif overall_score >= 0.4:
            grade = "ğŸ¥‰ ä¸€èˆ¬"
        else:
            grade = "âŒ å¾…æ”¹è¿›"
        
        print(f"\nğŸ–ï¸  æ€»ä½“è¯„çº§: {grade} (ç»¼åˆå¾—åˆ†: {overall_score:.3f})")
        print("="*60)


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•
    metrics = RecommendationMetrics()
    
    # ç¤ºä¾‹è¯„åˆ†æ•°æ®
    true_ratings = [(1, 101, 4.0), (1, 102, 3.0), (2, 101, 5.0), (2, 103, 2.0)]
    predicted_ratings = [(1, 101, 4.2), (1, 102, 3.1), (2, 101, 4.8), (2, 103, 2.5)]
    
    # ç¤ºä¾‹æ¨èæ•°æ®
    test_users = {1: [101, 102], 2: [101]}
    recommendations = {1: [101, 103, 104], 2: [101, 102, 105]}
    
    # ç¤ºä¾‹ç‰©å“ç‰¹å¾
    item_features = {
        101: ["Action", "Sci-Fi"],
        102: ["Drama", "Romance"],  
        103: ["Comedy"],
        104: ["Action", "Thriller"],
        105: ["Drama"]
    }
    
    # ç¤ºä¾‹ç‰©å“æµè¡Œåº¦
    item_popularity = {101: 0.8, 102: 0.3, 103: 0.6, 104: 0.2, 105: 0.4}
    
    all_items = {101, 102, 103, 104, 105, 106, 107}
    
    # ç»¼åˆè¯„ä¼°
    results = metrics.comprehensive_evaluation(
        true_ratings, predicted_ratings, test_users, recommendations,
        item_features, item_popularity, all_items
    )
    
    # æ‰“å°æŠ¥å‘Š
    metrics.print_evaluation_report(results)
