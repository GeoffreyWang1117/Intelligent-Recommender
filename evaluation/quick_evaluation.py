"""
å¿«é€Ÿè¯„ä¼°å·²è®­ç»ƒçš„SOTAæ¨èæ¨¡å‹
ä½¿ç”¨ç®€åŒ–çš„è¯„ä¼°æµç¨‹ï¼Œç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡å’Œå¯è§†åŒ–å›¾è¡¨
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import pickle
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict
import time
import logging
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class QuickEvaluator:
    """å¿«é€Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, models_dir: str = "models/saved"):
        self.models_dir = models_dir
        self.results = {}
        
    def load_training_results(self) -> Dict[str, Dict[str, float]]:
        """åŠ è½½è®­ç»ƒæ—¶çš„è¯„ä¼°ç»“æœ"""
        results_path = os.path.join(self.models_dir, "real_data_training_results.json")
        
        if not os.path.exists(results_path):
            logger.error(f"è®­ç»ƒç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_path}")
            return {}
        
        try:
            with open(results_path, 'r', encoding='utf-8') as f:
                training_results = json.load(f)
            
            # æå–è¯„ä¼°æŒ‡æ ‡
            evaluation_results = {}
            for model_name, data in training_results.items():
                if 'evaluation' in data:
                    eval_data = data['evaluation']
                    evaluation_results[model_name] = {
                        'RMSE': eval_data.get('rmse', 0.0),
                        'MAE': eval_data.get('mae', 0.0),
                        'Coverage': eval_data.get('coverage', 0.0),
                        'Training_Time': eval_data.get('training_time', 0.0),
                        'Sample_Size': eval_data.get('sample_size', 0),
                        'Error_Count': eval_data.get('error_count', 0)
                    }
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(evaluation_results)} ä¸ªæ¨¡å‹çš„è®­ç»ƒè¯„ä¼°ç»“æœ")
            return evaluation_results
            
        except Exception as e:
            logger.error(f"åŠ è½½è®­ç»ƒç»“æœå¤±è´¥: {e}")
            return {}
    
    def calculate_derived_metrics(self, results):
        """è®¡ç®—è¡ç”ŸæŒ‡æ ‡"""
        enhanced_results = {}
        
        for model_name, metrics in results.items():
            enhanced_metrics = metrics.copy()
            
            # æ€§èƒ½è¯„çº§ (åŸºäºRMSEï¼Œè¶Šä½è¶Šå¥½)
            rmse = metrics.get('RMSE', 10.0)
            if rmse < 1.0:
                performance_grade = 'A'
                performance_score = 95
            elif rmse < 1.5:
                performance_grade = 'B'
                performance_score = 85
            elif rmse < 2.0:
                performance_grade = 'C'
                performance_score = 75
            elif rmse < 3.0:
                performance_grade = 'D'
                performance_score = 65
            else:
                performance_grade = 'F'
                performance_score = 50
            
            enhanced_metrics['Performance_Grade'] = performance_grade
            enhanced_metrics['Performance_Score'] = performance_score
            
            # æ•ˆç‡è¯„çº§ (åŸºäºè®­ç»ƒæ—¶é—´)
            training_time = metrics.get('Training_Time', 0)
            if training_time < 30:
                efficiency_grade = 'A'
            elif training_time < 60:
                efficiency_grade = 'B'
            elif training_time < 120:
                efficiency_grade = 'C'
            elif training_time < 300:
                efficiency_grade = 'D'
            else:
                efficiency_grade = 'F'
            
            enhanced_metrics['Efficiency_Grade'] = efficiency_grade
            
            # é¢„æµ‹å‡†ç¡®ç‡ (åŸºäºé”™è¯¯æ•°é‡)
            error_count = metrics.get('Error_Count', 0)
            sample_size = metrics.get('Sample_Size', 1)
            accuracy = max(0, (sample_size - error_count) / sample_size * 100)
            enhanced_metrics['Prediction_Accuracy'] = accuracy
            
            enhanced_results[model_name] = enhanced_metrics
        
        return enhanced_results
    
    def create_performance_comparison_chart(self, results, output_dir: str = "evaluation_results"):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾"""
        os.makedirs(output_dir, exist_ok=True)
        
        models = list(results.keys())
        rmse_values = [results[model].get('RMSE', 0) for model in models]
        mae_values = [results[model].get('MAE', 0) for model in models]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # RMSEå¯¹æ¯”
        bars1 = ax1.bar(models, rmse_values, alpha=0.7, color='skyblue', edgecolor='navy')
        ax1.set_title('RMSE Performance Comparison', fontweight='bold', fontsize=14)
        ax1.set_ylabel('RMSE (Lower is Better)')
        ax1.set_xlabel('Models')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars1, rmse_values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # MAEå¯¹æ¯”
        bars2 = ax2.bar(models, mae_values, alpha=0.7, color='lightcoral', edgecolor='darkred')
        ax2.set_title('MAE Performance Comparison', fontweight='bold', fontsize=14)
        ax2.set_ylabel('MAE (Lower is Better)')
        ax2.set_xlabel('Models')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars2, mae_values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜")
    
    def create_training_efficiency_chart(self, results, output_dir: str = "evaluation_results"):
        """åˆ›å»ºè®­ç»ƒæ•ˆç‡å›¾"""
        models = list(results.keys())
        training_times = [results[model].get('Training_Time', 0) for model in models]
        performance_scores = [results[model].get('Performance_Score', 0) for model in models]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # è®­ç»ƒæ—¶é—´å¯¹æ¯”
        bars1 = ax1.bar(models, training_times, alpha=0.7, color='lightgreen', edgecolor='darkgreen')
        ax1.set_title('Training Time Comparison', fontweight='bold', fontsize=14)
        ax1.set_ylabel('Training Time (seconds)')
        ax1.set_xlabel('Models')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars1, training_times):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{value:.1f}s', ha='center', va='bottom', fontweight='bold')
        
        # æ•ˆç‡vsæ€§èƒ½æ•£ç‚¹å›¾
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink'][:len(models)]
        scatter = ax2.scatter(training_times, performance_scores, 
                            c=colors, s=100, alpha=0.7, edgecolors='black')
        
        for i, model in enumerate(models):
            ax2.annotate(model, (training_times[i], performance_scores[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=10)
        
        ax2.set_title('Training Efficiency vs Performance', fontweight='bold', fontsize=14)
        ax2.set_xlabel('Training Time (seconds)')
        ax2.set_ylabel('Performance Score')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'training_efficiency.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("è®­ç»ƒæ•ˆç‡å›¾å·²ä¿å­˜")
    
    def create_model_ranking_chart(self, results, output_dir: str = "evaluation_results"):
        """åˆ›å»ºæ¨¡å‹æ’åå›¾"""
        # æŒ‰æ€§èƒ½è¯„åˆ†æ’åº
        sorted_models = sorted(results.items(), 
                             key=lambda x: x[1].get('Performance_Score', 0), 
                             reverse=True)
        
        models = [item[0] for item in sorted_models]
        performance_scores = [item[1].get('Performance_Score', 0) for item in sorted_models]
        rmse_values = [item[1].get('RMSE', 0) for item in sorted_models]
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # åˆ›å»ºåŒYè½´å›¾
        ax2 = ax.twinx()
        
        # æ€§èƒ½è¯„åˆ†æ¡å½¢å›¾
        bars = ax.bar(models, performance_scores, alpha=0.7, color='lightblue', 
                     label='Performance Score', edgecolor='navy')
        
        # RMSEæŠ˜çº¿å›¾
        line = ax2.plot(models, rmse_values, color='red', marker='o', linewidth=2, 
                       markersize=8, label='RMSE')
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        ax.set_title('Model Performance Ranking', fontweight='bold', fontsize=16)
        ax.set_xlabel('Models (Ranked by Performance)', fontsize=12)
        ax.set_ylabel('Performance Score', color='blue', fontsize=12)
        ax2.set_ylabel('RMSE (Lower is Better)', color='red', fontsize=12)
        
        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, alpha=0.3)
        
        # æ—‹è½¬Xè½´æ ‡ç­¾
        ax.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ å›¾ä¾‹
        lines, labels = ax.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax.legend(lines + lines2, labels + labels2, loc='center right')
        
        # æ·»åŠ æ’åæ ‡æ³¨
        for i, (model, score) in enumerate(zip(models, performance_scores)):
            ax.text(i, score + 1, f'#{i+1}', ha='center', va='bottom', 
                   fontweight='bold', fontsize=12, color='darkblue')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'model_ranking.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("æ¨¡å‹æ’åå›¾å·²ä¿å­˜")
    
    def create_comprehensive_summary_table(self, results, output_dir: str = "evaluation_results"):
        """åˆ›å»ºç»¼åˆæ€»ç»“è¡¨"""
        # å‡†å¤‡è¡¨æ ¼æ•°æ®
        table_data = []
        for model, metrics in results.items():
            table_data.append({
                'Model': model,
                'Performance_Grade': metrics.get('Performance_Grade', 'N/A'),
                'RMSE': f"{metrics.get('RMSE', 0):.4f}",
                'MAE': f"{metrics.get('MAE', 0):.4f}",
                'Training_Time': f"{metrics.get('Training_Time', 0):.1f}s",
                'Efficiency_Grade': metrics.get('Efficiency_Grade', 'N/A'),
                'Prediction_Accuracy': f"{metrics.get('Prediction_Accuracy', 0):.1f}%",
                'Coverage': f"{metrics.get('Coverage', 0):.2f}"
            })
        
        # æŒ‰æ€§èƒ½è¯„åˆ†æ’åº
        table_data.sort(key=lambda x: results[x['Model']].get('Performance_Score', 0), reverse=True)
        
        # ä¿å­˜ä¸ºCSV
        df = pd.DataFrame(table_data)
        csv_path = os.path.join(output_dir, 'model_evaluation_summary.csv')
        df.to_csv(csv_path, index=False)
        
        # ä¿å­˜ä¸ºJSON
        json_path = os.path.join(output_dir, 'model_evaluation_summary.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºHTMLè¡¨æ ¼
        html_path = os.path.join(output_dir, 'model_evaluation_summary.html')
        html_content = df.to_html(index=False, table_id='evaluation_table', 
                                 classes='table table-striped table-hover')
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(f"""
<!DOCTYPE html>
<html>
<head>
    <title>SOTAæ¨èæ¨¡å‹è¯„ä¼°æŠ¥å‘Š</title>
    <meta charset="utf-8">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        .container {{ margin-top: 20px; }}
        .grade-A {{ color: #28a745; font-weight: bold; }}
        .grade-B {{ color: #17a2b8; font-weight: bold; }}
        .grade-C {{ color: #ffc107; font-weight: bold; }}
        .grade-D {{ color: #fd7e14; font-weight: bold; }}
        .grade-F {{ color: #dc3545; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="container">
        <h1 class="text-center mb-4">SOTAæ¨èæ¨¡å‹è¯„ä¼°æŠ¥å‘Š</h1>
        <p class="text-muted text-center">åŸºäºçœŸå®MovieLensæ•°æ®çš„7ä¸ªSOTAæ¨¡å‹æ€§èƒ½è¯„ä¼°</p>
        {html_content}
        <div class="mt-4">
            <h5>è¯„çº§è¯´æ˜ï¼š</h5>
            <ul>
                <li><span class="grade-A">Açº§</span>: ä¼˜ç§€ (RMSE < 1.0)</li>
                <li><span class="grade-B">Bçº§</span>: è‰¯å¥½ (RMSE < 1.5)</li>
                <li><span class="grade-C">Cçº§</span>: ä¸­ç­‰ (RMSE < 2.0)</li>
                <li><span class="grade-D">Dçº§</span>: ä¸€èˆ¬ (RMSE < 3.0)</li>
                <li><span class="grade-F">Fçº§</span>: è¾ƒå·® (RMSE â‰¥ 3.0)</li>
            </ul>
        </div>
    </div>
</body>
</html>
            """)
        
        logger.info(f"ç»¼åˆæ€»ç»“å·²ä¿å­˜: {csv_path}, {json_path}, {html_path}")
    
    def generate_quick_report(self, output_dir: str = "evaluation_results"):
        """ç”Ÿæˆå¿«é€Ÿè¯„ä¼°æŠ¥å‘Š"""
        logger.info("å¼€å§‹ç”Ÿæˆå¿«é€Ÿè¯„ä¼°æŠ¥å‘Š...")
        
        # åŠ è½½è®­ç»ƒç»“æœ
        results = self.load_training_results()
        if not results:
            logger.error("æ— æ³•åŠ è½½è®­ç»ƒç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        enhanced_results = self.calculate_derived_metrics(results)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆå„ç§å›¾è¡¨
        self.create_performance_comparison_chart(enhanced_results, output_dir)
        self.create_training_efficiency_chart(enhanced_results, output_dir)
        self.create_model_ranking_chart(enhanced_results, output_dir)
        self.create_comprehensive_summary_table(enhanced_results, output_dir)
        
        # è¾“å‡ºæ€»ç»“
        logger.info("\n" + "="*80)
        logger.info("å¿«é€Ÿè¯„ä¼°æŠ¥å‘Šæ€»ç»“")
        logger.info("="*80)
        
        # æŒ‰æ€§èƒ½æ’åº
        sorted_models = sorted(enhanced_results.items(), 
                             key=lambda x: x[1].get('Performance_Score', 0), 
                             reverse=True)
        
        for i, (model, metrics) in enumerate(sorted_models):
            logger.info(f"#{i+1} {model}:")
            logger.info(f"  æ€§èƒ½ç­‰çº§: {metrics.get('Performance_Grade', 'N/A')}")
            logger.info(f"  RMSE: {metrics.get('RMSE', 0):.4f}")
            logger.info(f"  MAE: {metrics.get('MAE', 0):.4f}")
            logger.info(f"  è®­ç»ƒæ—¶é—´: {metrics.get('Training_Time', 0):.1f}ç§’")
            logger.info(f"  æ•ˆç‡ç­‰çº§: {metrics.get('Efficiency_Grade', 'N/A')}")
            logger.info(f"  é¢„æµ‹å‡†ç¡®ç‡: {metrics.get('Prediction_Accuracy', 0):.1f}%")
        
        logger.info(f"\nâœ… å¿«é€Ÿè¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        logger.info(f"ğŸ“Š æŠ¥å‘Šæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")
        logger.info(f"ğŸ“ˆ åŒ…å«æ€§èƒ½å¯¹æ¯”å›¾ã€æ•ˆç‡åˆ†æå›¾ã€æ’åå›¾å’Œè¯¦ç»†æ•°æ®è¡¨")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹å¿«é€Ÿè¯„ä¼°SOTAæ¨èæ¨¡å‹...")
    
    evaluator = QuickEvaluator()
    evaluator.generate_quick_report()


if __name__ == "__main__":
    main()
