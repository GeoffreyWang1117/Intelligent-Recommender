"""
å…¨é¢è¯„ä¼°å·²è®­ç»ƒçš„SOTAæ¨èæ¨¡å‹
ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import pickle
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import time
import logging
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

from utils.data_loader import MovieLensLoader
from evaluation.metrics import RecommendationMetrics

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ComprehensiveEvaluator:
    """å…¨é¢è¯„ä¼°å™¨"""
    
    def __init__(self, models_dir: str = "models/saved"):
        self.models_dir = models_dir
        self.metrics_calculator = RecommendationMetrics()
        self.results = {}
        
    def load_trained_models(self) -> Dict[str, Any]:
        """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
        models = {}
        model_files = [
            "SVD_real_movielens.pkl",
            "DeepFM_real_movielens.pkl", 
            "DCNv2_real_movielens.pkl",
            "AutoInt_real_movielens.pkl",
            "xDeepFM_real_movielens.pkl",
            "Transformer4Rec_real_movielens.pkl",
            "DIN_real_movielens.pkl"
        ]
        
        for model_file in model_files:
            model_path = os.path.join(self.models_dir, model_file)
            if os.path.exists(model_path):
                try:
                    with open(model_path, 'rb') as f:
                        model = pickle.load(f)
                    model_name = model_file.replace('_real_movielens.pkl', '')
                    models[model_name] = model
                    logger.info(f"âœ“ åŠ è½½æ¨¡å‹: {model_name}")
                except Exception as e:
                    logger.error(f"âœ— åŠ è½½æ¨¡å‹å¤±è´¥ {model_file}: {e}")
            else:
                logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        return models
    
    def generate_recommendations(self, model, model_name: str, test_users: List[int], 
                               top_k: int = 20) -> Dict[int, List[int]]:
        """ä¸ºæµ‹è¯•ç”¨æˆ·ç”Ÿæˆæ¨è"""
        recommendations = {}
        successful_users = 0
        
        logger.info(f"ä¸º {len(test_users)} ä¸ªç”¨æˆ·ç”Ÿæˆæ¨è...")
        
        for i, user_id in enumerate(test_users):
            if i % 50 == 0:
                logger.info(f"  è¿›åº¦: {i}/{len(test_users)} ({i/len(test_users)*100:.1f}%)")
            
            try:
                # è®¾ç½®è¶…æ—¶æœºåˆ¶
                import signal
                def timeout_handler(signum, frame):
                    raise TimeoutError("æ¨èç”Ÿæˆè¶…æ—¶")
                
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(10)  # 10ç§’è¶…æ—¶
                
                try:
                    user_recs = model.get_user_recommendations(user_id, top_k=top_k)
                    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    
                    if user_recs:
                        if isinstance(user_recs[0], tuple):
                            # (item_id, score) æ ¼å¼
                            rec_items = [item for item, _ in user_recs]
                        elif isinstance(user_recs[0], dict):
                            # å­—å…¸æ ¼å¼
                            rec_items = [rec.get('item_id', rec.get('movieId', 0)) for rec in user_recs]
                        else:
                            # ç›´æ¥æ˜¯item_idåˆ—è¡¨
                            rec_items = user_recs
                        
                        # è¿‡æ»¤æ— æ•ˆæ¨è
                        rec_items = [item for item in rec_items if item is not None and item > 0]
                        recommendations[user_id] = rec_items[:top_k]
                        successful_users += 1
                    else:
                        recommendations[user_id] = []
                        
                except TimeoutError:
                    recommendations[user_id] = []
                    signal.alarm(0)
                    
            except Exception as e:
                recommendations[user_id] = []
        
        success_rate = successful_users / len(test_users) * 100
        logger.info(f"æ¨èç”Ÿæˆå®Œæˆ: {successful_users}/{len(test_users)} ({success_rate:.1f}%) æˆåŠŸ")
        
        return recommendations
    
    def calculate_comprehensive_metrics(self, recommendations: Dict[int, List[int]], 
                                      model_name: str) -> Dict[str, float]:
        """è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡"""
        logger.info(f"è®¡ç®— {model_name} çš„è¯„ä¼°æŒ‡æ ‡...")
        
        metrics = {}
        k_values = [5, 10, 20]
        
        # åŸºç¡€æ’åºæŒ‡æ ‡
        for k in k_values:
            try:
                recall = self.metrics_calculator.recall_at_k(recommendations, k)
                precision = self.metrics_calculator.precision_at_k(recommendations, k)
                ndcg = self.metrics_calculator.ndcg_at_k(recommendations, k=k)
                hit_rate = self.metrics_calculator.hit_rate_at_k(recommendations, k)
                
                metrics[f'Recall@{k}'] = recall
                metrics[f'Precision@{k}'] = precision
                metrics[f'NDCG@{k}'] = ndcg
                metrics[f'HitRate@{k}'] = hit_rate
                
                # F1 Score
                if precision + recall > 0:
                    f1 = 2 * precision * recall / (precision + recall)
                    metrics[f'F1@{k}'] = f1
                else:
                    metrics[f'F1@{k}'] = 0.0
                    
            except Exception as e:
                logger.warning(f"è®¡ç®—K={k}æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                for metric_name in [f'Recall@{k}', f'Precision@{k}', f'NDCG@{k}', f'HitRate@{k}', f'F1@{k}']:
                    metrics[metric_name] = 0.0
        
        # é«˜çº§æŒ‡æ ‡
        try:
            metrics['MAP'] = self.metrics_calculator.mean_average_precision(recommendations)
            metrics['MRR'] = self.metrics_calculator.mean_reciprocal_rank(recommendations)
            metrics['Coverage'] = self.metrics_calculator.coverage(recommendations)
            # ç®€åŒ–å¤šæ ·æ€§è®¡ç®—
            all_items = set()
            for recs in recommendations.values():
                all_items.update(recs[:10])  # åªè€ƒè™‘å‰10ä¸ªæ¨è
            metrics['Diversity'] = len(all_items) / max(len(recommendations), 1)
            metrics['Novelty'] = self.metrics_calculator.novelty(recommendations)
        except Exception as e:
            logger.warning(f"è®¡ç®—é«˜çº§æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            for metric_name in ['MAP', 'MRR', 'Coverage', 'Diversity', 'Novelty']:
                metrics[metric_name] = 0.0
        
        # æ¨èè¦†ç›–ç»Ÿè®¡
        total_users = len(recommendations)
        users_with_recs = sum(1 for recs in recommendations.values() if len(recs) > 0)
        metrics['User_Coverage'] = users_with_recs / total_users if total_users > 0 else 0.0
        
        avg_rec_length = np.mean([len(recs) for recs in recommendations.values()])
        metrics['Avg_Rec_Length'] = avg_rec_length
        
        return metrics
    
    def evaluate_all_models(self, train_data: pd.DataFrame, test_data: pd.DataFrame) -> Dict[str, Dict[str, float]]:
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        logger.info("å¼€å§‹å…¨é¢è¯„ä¼°æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹...")
        
        # å‡†å¤‡è¯„ä¼°æ•°æ®
        self.metrics_calculator.prepare_data(train_data, test_data)
        
        # åŠ è½½æ¨¡å‹
        models = self.load_trained_models()
        if not models:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è®­ç»ƒæ¨¡å‹")
            return {}
        
        # é€‰æ‹©æµ‹è¯•ç”¨æˆ·ï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜æ•ˆç‡ï¼‰
        test_users = list(test_data['user_id'].unique()[:200])  # è¯„ä¼°200ä¸ªç”¨æˆ·
        logger.info(f"é€‰æ‹© {len(test_users)} ä¸ªæµ‹è¯•ç”¨æˆ·è¿›è¡Œè¯„ä¼°")
        
        results = {}
        
        for model_name, model in models.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"è¯„ä¼°æ¨¡å‹: {model_name}")
            logger.info(f"{'='*60}")
            
            try:
                start_time = time.time()
                
                # ç”Ÿæˆæ¨è
                recommendations = self.generate_recommendations(model, model_name, test_users)
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = self.calculate_comprehensive_metrics(recommendations, model_name)
                metrics['Evaluation_Time'] = time.time() - start_time
                
                results[model_name] = metrics
                
                logger.info(f"âœ“ {model_name} è¯„ä¼°å®Œæˆ")
                logger.info(f"  ä¸»è¦æŒ‡æ ‡: Recall@10={metrics.get('Recall@10', 0):.4f}, "
                          f"Precision@10={metrics.get('Precision@10', 0):.4f}, "
                          f"NDCG@10={metrics.get('NDCG@10', 0):.4f}")
                
            except Exception as e:
                logger.error(f"âœ— {model_name} è¯„ä¼°å¤±è´¥: {e}")
                results[model_name] = {'error': str(e)}
        
        return results
    
    def create_visualization_reports(self, results: Dict[str, Dict[str, float]], 
                                   output_dir: str = "evaluation_results"):
        """åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š"""
        os.makedirs(output_dir, exist_ok=True)
        
        # å‡†å¤‡æ•°æ®
        models = list(results.keys())
        valid_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if not valid_results:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœå¯ç”¨äºå¯è§†åŒ–")
            return
        
        # 1. ä¸»è¦æŒ‡æ ‡å¯¹æ¯”å›¾
        self._create_main_metrics_chart(valid_results, output_dir)
        
        # 2. Recall@K å’Œ Precision@K å¯¹æ¯”
        self._create_recall_precision_chart(valid_results, output_dir)
        
        # 3. æ’åºè´¨é‡æŒ‡æ ‡å¯¹æ¯”
        self._create_ranking_quality_chart(valid_results, output_dir)
        
        # 4. è¦†ç›–ç‡å’Œå¤šæ ·æ€§å¯¹æ¯”
        self._create_coverage_diversity_chart(valid_results, output_dir)
        
        # 5. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        self._create_radar_chart(valid_results, output_dir)
        
        # 6. è¯¦ç»†æ•°æ®è¡¨æ ¼
        self._create_detailed_table(valid_results, output_dir)
        
        logger.info(f"å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _create_main_metrics_chart(self, results: Dict[str, Dict[str, float]], output_dir: str):
        """åˆ›å»ºä¸»è¦æŒ‡æ ‡å¯¹æ¯”å›¾"""
        models = list(results.keys())
        metrics = ['Recall@10', 'Precision@10', 'NDCG@10', 'F1@10']
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(models))
        width = 0.2
        
        for i, metric in enumerate(metrics):
            values = [results[model].get(metric, 0) for model in models]
            ax.bar(x + i * width, values, width, label=metric, alpha=0.8)
        
        ax.set_xlabel('æ¨¡å‹')
        ax.set_ylabel('æŒ‡æ ‡å€¼')
        ax.set_title('ä¸»è¦æ¨èæŒ‡æ ‡å¯¹æ¯” (K=10)', fontsize=14, fontweight='bold')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'main_metrics_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_recall_precision_chart(self, results: Dict[str, Dict[str, float]], output_dir: str):
        """åˆ›å»ºRecall@Kå’ŒPrecision@Kå¯¹æ¯”å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        models = list(results.keys())
        k_values = [5, 10, 20]
        
        # Recall@K
        for model in models:
            recall_values = [results[model].get(f'Recall@{k}', 0) for k in k_values]
            ax1.plot(k_values, recall_values, marker='o', linewidth=2, label=model)
        
        ax1.set_xlabel('Kå€¼')
        ax1.set_ylabel('Recall@K')
        ax1.set_title('Recall@K å¯¹æ¯”', fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Precision@K
        for model in models:
            precision_values = [results[model].get(f'Precision@{k}', 0) for k in k_values]
            ax2.plot(k_values, precision_values, marker='s', linewidth=2, label=model)
        
        ax2.set_xlabel('Kå€¼')
        ax2.set_ylabel('Precision@K')
        ax2.set_title('Precision@K å¯¹æ¯”', fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'recall_precision_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_ranking_quality_chart(self, results: Dict[str, Dict[str, float]], output_dir: str):
        """åˆ›å»ºæ’åºè´¨é‡æŒ‡æ ‡å¯¹æ¯”å›¾"""
        models = list(results.keys())
        metrics = ['NDCG@10', 'MAP', 'MRR']
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        x = np.arange(len(models))
        width = 0.25
        
        for i, metric in enumerate(metrics):
            values = [results[model].get(metric, 0) for model in models]
            ax.bar(x + i * width, values, width, label=metric, alpha=0.8)
        
        ax.set_xlabel('æ¨¡å‹')
        ax.set_ylabel('æŒ‡æ ‡å€¼')
        ax.set_title('æ’åºè´¨é‡æŒ‡æ ‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax.set_xticks(x + width)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'ranking_quality_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_coverage_diversity_chart(self, results: Dict[str, Dict[str, float]], output_dir: str):
        """åˆ›å»ºè¦†ç›–ç‡å’Œå¤šæ ·æ€§å¯¹æ¯”å›¾"""
        models = list(results.keys())
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # è¦†ç›–ç‡æŒ‡æ ‡
        coverage_metrics = ['Coverage', 'User_Coverage']
        x = np.arange(len(models))
        width = 0.35
        
        for i, metric in enumerate(coverage_metrics):
            values = [results[model].get(metric, 0) for model in models]
            ax1.bar(x + i * width, values, width, label=metric, alpha=0.8)
        
        ax1.set_xlabel('æ¨¡å‹')
        ax1.set_ylabel('è¦†ç›–ç‡')
        ax1.set_title('è¦†ç›–ç‡æŒ‡æ ‡å¯¹æ¯”', fontweight='bold')
        ax1.set_xticks(x + width / 2)
        ax1.set_xticklabels(models, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å¤šæ ·æ€§å’Œæ–°é¢–æ€§
        diversity_values = [results[model].get('Diversity', 0) for model in models]
        novelty_values = [results[model].get('Novelty', 0) for model in models]
        
        ax2_twin = ax2.twinx()
        
        bars1 = ax2.bar(x - width/2, diversity_values, width, label='Diversity', alpha=0.8, color='skyblue')
        bars2 = ax2_twin.bar(x + width/2, novelty_values, width, label='Novelty', alpha=0.8, color='lightcoral')
        
        ax2.set_xlabel('æ¨¡å‹')
        ax2.set_ylabel('å¤šæ ·æ€§', color='skyblue')
        ax2_twin.set_ylabel('æ–°é¢–æ€§', color='lightcoral')
        ax2.set_title('å¤šæ ·æ€§å’Œæ–°é¢–æ€§å¯¹æ¯”', fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(models, rotation=45, ha='right')
        
        # æ·»åŠ å›¾ä¾‹
        lines1, labels1 = ax2.get_legend_handles_labels()
        lines2, labels2 = ax2_twin.get_legend_handles_labels()
        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'coverage_diversity_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_radar_chart(self, results: Dict[str, Dict[str, float]], output_dir: str):
        """åˆ›å»ºç»¼åˆæ€§èƒ½é›·è¾¾å›¾"""
        models = list(results.keys())
        metrics = ['Recall@10', 'Precision@10', 'NDCG@10', 'Coverage', 'Diversity', 'Novelty']
        
        # å½’ä¸€åŒ–æŒ‡æ ‡å€¼åˆ°0-1èŒƒå›´
        normalized_results = {}
        for metric in metrics:
            values = [results[model].get(metric, 0) for model in models]
            max_val = max(values) if max(values) > 0 else 1
            for i, model in enumerate(models):
                if model not in normalized_results:
                    normalized_results[model] = {}
                normalized_results[model][metric] = values[i] / max_val
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        # åˆ›å»ºé¢œè‰²åˆ—è¡¨
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink'][:len(models)]
        
        for i, model in enumerate(models):
            values = [normalized_results[model].get(metric, 0) for metric in metrics]
            values += values[:1]  # é—­åˆå›¾å½¢
            
            ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])
            ax.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 1)
        ax.set_title('æ¨¡å‹ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', size=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'comprehensive_radar_chart.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_detailed_table(self, results: Dict[str, Dict[str, float]], output_dir: str):
        """åˆ›å»ºè¯¦ç»†æ•°æ®è¡¨æ ¼"""
        # åˆ›å»ºDataFrame
        df_data = []
        for model, metrics in results.items():
            row = {'Model': model}
            for key, value in metrics.items():
                row[key] = value  # type: ignore
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        
        # ä¿å­˜ä¸ºCSV
        csv_path = os.path.join(output_dir, 'detailed_evaluation_results.csv')
        df.to_csv(csv_path, index=False, float_format='%.4f')
        
        # ä¿å­˜ä¸ºJSON
        json_path = os.path.join(output_dir, 'detailed_evaluation_results.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {csv_path}, {json_path}")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹å…¨é¢è¯„ä¼°SOTAæ¨èæ¨¡å‹...")
    
    # 1. åŠ è½½æ•°æ®
    logger.info("åŠ è½½MovieLensæ•°æ®...")
    try:
        # ç›´æ¥è¯»å–CSVæ ¼å¼çš„MovieLens smallæ•°æ®é›†
        ratings_path = "data/movielens/small/ratings.csv"
        movies_path = "data/movielens/small/movies.csv"
        
        ratings_df = pd.read_csv(ratings_path)
        movies_df = pd.read_csv(movies_path)
        
        # é‡å‘½ååˆ—ä»¥åŒ¹é…æˆ‘ä»¬çš„æ ¼å¼
        if 'movieId' in ratings_df.columns:
            ratings_df = ratings_df.rename(columns={'movieId': 'item_id', 'userId': 'user_id'})
        
        # ç®€å•çš„è®­ç»ƒæµ‹è¯•åˆ†å‰²
        from sklearn.model_selection import train_test_split
        train_data, test_data = train_test_split(ratings_df, test_size=0.2, random_state=42)
        
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_data)} æ¡, æµ‹è¯•é›† {len(test_data)} æ¡")
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 2. åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ComprehensiveEvaluator()
    
    # 3. æ‰§è¡Œè¯„ä¼°
    logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    results = evaluator.evaluate_all_models(train_data, test_data)
    
    if not results:
        logger.error("è¯„ä¼°å¤±è´¥ï¼Œæ²¡æœ‰è·å¾—ä»»ä½•ç»“æœ")
        return
    
    # 4. ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
    logger.info("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨...")
    evaluator.create_visualization_reports(results)
    
    # 5. è¾“å‡ºæ€»ç»“
    logger.info("\n" + "="*80)
    logger.info("è¯„ä¼°æ€»ç»“")
    logger.info("="*80)
    
    for model, metrics in results.items():
        if 'error' in metrics:
            logger.error(f"{model}: è¯„ä¼°å¤±è´¥ - {metrics['error']}")
        else:
            logger.info(f"{model}:")
            logger.info(f"  Recall@10: {metrics.get('Recall@10', 0):.4f}")
            logger.info(f"  Precision@10: {metrics.get('Precision@10', 0):.4f}")
            logger.info(f"  NDCG@10: {metrics.get('NDCG@10', 0):.4f}")
            logger.info(f"  è¦†ç›–ç‡: {metrics.get('Coverage', 0):.4f}")
            logger.info(f"  ç”¨æˆ·è¦†ç›–ç‡: {metrics.get('User_Coverage', 0):.4f}")
    
    logger.info("\nâœ… å…¨é¢è¯„ä¼°å®Œæˆï¼")
    logger.info("ğŸ“Š è¯„ä¼°æŠ¥å‘Šå’Œå›¾è¡¨å·²ç”Ÿæˆåœ¨ evaluation_results/ ç›®å½•")


if __name__ == "__main__":
    main()
