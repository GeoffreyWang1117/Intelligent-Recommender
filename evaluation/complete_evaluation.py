#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆå®Œæ•´æ¨èç³»ç»Ÿè¯„ä¼°è„šæœ¬

ä¿®å¤äº†æ¨èç”ŸæˆæˆåŠŸç‡ä¸º0%çš„å…³é”®é—®é¢˜ï¼š
1. æ”¹è¿›ç”¨æˆ·æ¨èç”Ÿæˆé€»è¾‘
2. å¢å¼ºé”™è¯¯å¤„ç†å’Œè°ƒè¯•ä¿¡æ¯
3. ä¼˜åŒ–æ¨¡å‹å…¼å®¹æ€§
4. æä¾›æ›´å¥å£®çš„è¯„ä¼°æµç¨‹
"""

import os
import sys
import logging
import pandas as pd
import numpy as np
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

# ç®€åŒ–å¯¼å…¥ï¼Œé¿å…å¤æ‚ä¾èµ–
import pickle

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('evaluation_results/complete_evaluation.log')
    ]
)
logger = logging.getLogger(__name__)


class FixedCompleteEvaluator:
    """ä¿®å¤ç‰ˆå®Œæ•´è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.results = {}
        self.visualization_data = {}
        self.output_dir = Path("evaluation_results")
        self.output_dir.mkdir(exist_ok=True)
        
        # ç¼“å­˜æ•°æ®
        self.train_data = None
        self.test_data = None
        self.test_user_items = {}
        self.total_items = 0
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        logger.info("åŠ è½½MovieLensæ•°æ®...")
        
        # ç›´æ¥åŠ è½½æ•°æ®æ–‡ä»¶
        data_path = Path("data/movielens/small/ratings.csv")
        if not data_path.exists():
            logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return
        
        # åŠ è½½æ•°æ®
        data = pd.read_csv(data_path)
        # é‡å‘½ååˆ—ä»¥åŒ¹é…ç³»ç»Ÿæ ¼å¼
        if 'movieId' in data.columns:
            data = data.rename(columns={'userId': 'user_id', 'movieId': 'item_id'})
        
        # åˆ†å‰²æ•°æ®
        from sklearn.model_selection import train_test_split
        train_data, test_data = train_test_split(
            data, test_size=0.2, random_state=42, stratify=data['user_id']
        )
        
        self.train_data = train_data.reset_index(drop=True)
        self.test_data = test_data.reset_index(drop=True)
        
        # æ„å»ºæµ‹è¯•ç”¨æˆ·-ç‰©å“æ˜ å°„
        for _, row in self.test_data.iterrows():
            user_id = row['user_id']
            item_id = row['item_id']
            if user_id not in self.test_user_items:
                self.test_user_items[user_id] = set()
            self.test_user_items[user_id].add(item_id)
        
        self.total_items = len(data['item_id'].unique())
        
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(self.train_data)} æ¡, æµ‹è¯•é›† {len(self.test_data)} æ¡")
    
    def load_models(self) -> Dict[str, Any]:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        models = {}
        saved_dir = Path("models/saved")
        
        if not saved_dir.exists():
            logger.error("æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return models
        
        # æ¨¡å‹æ˜ å°„
        model_files = {
            'SVD': 'SVD_real_movielens.pkl',
            'DeepFM': 'DeepFM_real_movielens.pkl', 
            'DCNv2': 'DCNv2_real_movielens.pkl',
            'AutoInt': 'AutoInt_real_movielens.pkl',
            'xDeepFM': 'xDeepFM_real_movielens.pkl',
            'Transformer4Rec': 'Transformer4Rec_real_movielens.pkl',
            'DIN': 'DIN_real_movielens.pkl'
        }
        
        for model_name, filename in model_files.items():
            model_path = saved_dir / filename
            try:
                if model_path.exists():
                    import pickle
                    with open(model_path, 'rb') as f:
                        model = pickle.load(f)
                    models[model_name] = model
                    logger.info(f"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
                else:
                    logger.warning(f"âœ— æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            except Exception as e:
                logger.error(f"âœ— åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {str(e)}")
        
        return models
    
    def safe_get_recommendations(self, model, user_id: int, top_k: int = 10) -> List[int]:
        """å®‰å…¨è·å–ç”¨æˆ·æ¨è"""
        try:
            # æ–¹æ³•1: å°è¯•æ ‡å‡†æ¨èæ–¹æ³•
            if hasattr(model, 'get_user_recommendations'):
                try:
                    recs = model.get_user_recommendations(user_id, top_k)
                    if recs:
                        # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                        if isinstance(recs[0], tuple):
                            return [int(item) for item, _ in recs if item is not None and item > 0]
                        elif isinstance(recs[0], dict):
                            return [int(rec.get('item_id', 0)) for rec in recs if rec.get('item_id', 0) > 0]
                        elif isinstance(recs[0], (int, float)):
                            return [int(item) for item in recs if item is not None and item > 0]
                except Exception as e:
                    logger.debug(f"æ ‡å‡†æ¨èæ–¹æ³•å¤±è´¥ (user={user_id}): {str(e)}")
            
            # æ–¹æ³•2: åŸºäºé¢„æµ‹åˆ†æ•°çš„æ¨è
            if hasattr(model, 'predict'):
                try:
                    item_scores = []
                    
                    # è·å–è®­ç»ƒæ•°æ®ä¸­çš„æ‰€æœ‰ç‰©å“
                    if hasattr(model, 'item_encoder') and hasattr(model.item_encoder, 'classes_'):
                        available_items = list(model.item_encoder.classes_)
                    elif self.train_data is not None:
                        available_items = list(self.train_data['item_id'].unique())
                    else:
                        available_items = list(range(1, min(1000, self.total_items + 1)))
                    
                    # è·å–ç”¨æˆ·å·²è¯„åˆ†ç‰©å“
                    rated_items = set()
                    if self.train_data is not None:
                        user_ratings = self.train_data[self.train_data['user_id'] == user_id]
                        if not user_ratings.empty:
                            rated_items = set(user_ratings['item_id'].values)
                    
                    # ä¸ºæœªè¯„åˆ†ç‰©å“è®¡ç®—é¢„æµ‹åˆ†æ•°
                    for item_id in available_items:
                        if item_id not in rated_items:
                            try:
                                score = model.predict(user_id, item_id)
                                if score is not None and not np.isnan(score):
                                    item_scores.append((item_id, float(score)))
                            except:
                                continue
                    
                    # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top-k
                    if item_scores:
                        item_scores.sort(key=lambda x: x[1], reverse=True)
                        return [int(item) for item, _ in item_scores[:top_k]]
                
                except Exception as e:
                    logger.debug(f"åŸºäºé¢„æµ‹çš„æ¨èå¤±è´¥ (user={user_id}): {str(e)}")
            
            # æ–¹æ³•3: è¿”å›çƒ­é—¨ç‰©å“
            if self.train_data is not None:
                popular_items = self.train_data.groupby('item_id')['rating'].agg(['mean', 'count'])
                popular_items['score'] = popular_items['mean'] * np.log(1 + popular_items['count'])
                top_items = popular_items.nlargest(top_k, 'score').index.tolist()
                return [int(item) for item in top_items]
            
        except Exception as e:
            logger.debug(f"æ¨èç”Ÿæˆå®Œå…¨å¤±è´¥ (user={user_id}): {str(e)}")
        
        return []
    
    def generate_recommendations(self, model, test_users: List[int], top_k: int = 10) -> Dict[int, List[int]]:
        """ç”Ÿæˆæ¨èåˆ—è¡¨"""
        recommendations = {}
        successful_users = 0
        
        logger.info(f"ä¸º {len(test_users)} ä¸ªç”¨æˆ·ç”Ÿæˆæ¨è...")
        
        def get_user_recs(user_id):
            try:
                rec_items = self.safe_get_recommendations(model, user_id, top_k)
                return user_id, rec_items
            except Exception as e:
                logger.debug(f"ç”¨æˆ· {user_id} æ¨èå¤±è´¥: {str(e)}")
                return user_id, []
        
        # é™åˆ¶æµ‹è¯•ç”¨æˆ·æ•°é‡ä»¥åŠ å¿«è¯„ä¼°
        test_users_sample = test_users[:min(100, len(test_users))]
        
        # å¹¶è¡Œç”Ÿæˆæ¨è
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(get_user_recs, user_id): user_id 
                      for user_id in test_users_sample}
            
            for i, future in enumerate(as_completed(futures)):
                if i % 25 == 0:
                    progress = int(i / len(test_users_sample) * 100)
                    logger.info(f"  è¿›åº¦: {i}/{len(test_users_sample)} ({progress}%)")
                
                try:
                    user_id, rec_items = future.result(timeout=10)
                    recommendations[user_id] = rec_items
                    if len(rec_items) > 0:
                        successful_users += 1
                except Exception as e:
                    logger.debug(f"è·å–ç»“æœå¤±è´¥: {str(e)}")
        
        success_rate = successful_users / len(recommendations) * 100 if recommendations else 0
        logger.info(f"æ¨èç”Ÿæˆå®Œæˆ: {successful_users}/{len(recommendations)} ({success_rate:.1f}%) æˆåŠŸ")
        
        return recommendations
    
    def calculate_rating_metrics(self, model, test_data: pd.DataFrame, 
                               sample_size: int = 1000) -> Dict[str, float]:
        """è®¡ç®—è¯„åˆ†é¢„æµ‹æŒ‡æ ‡"""
        test_sample = test_data.sample(n=min(sample_size, len(test_data)), random_state=42)
        
        true_ratings = []
        pred_ratings = []
        error_count = 0
        
        logger.info(f"è®¡ç®—è¯„åˆ†æŒ‡æ ‡ï¼Œæ ·æœ¬æ•°: {len(test_sample)}")
        
        for _, row in test_sample.iterrows():
            try:
                user_id = int(row['user_id'])
                item_id = int(row['item_id'])
                true_rating = float(row['rating'])
                
                # é¢„æµ‹è¯„åˆ†
                pred_rating = model.predict(user_id, item_id)
                
                if pred_rating is not None and not np.isnan(pred_rating):
                    true_ratings.append(true_rating)
                    pred_ratings.append(float(pred_rating))
                else:
                    error_count += 1
                    
            except Exception as e:
                error_count += 1
                logger.debug(f"é¢„æµ‹å¤±è´¥: {str(e)}")
        
        # è®¡ç®—æŒ‡æ ‡
        if len(true_ratings) > 0:
            true_ratings = np.array(true_ratings)
            pred_ratings = np.array(pred_ratings)
            
            rmse = float(np.sqrt(np.mean((true_ratings - pred_ratings) ** 2)))
            mae = float(np.mean(np.abs(true_ratings - pred_ratings)))
        else:
            rmse = 0.0
            mae = 0.0
        
        coverage = (len(true_ratings) / len(test_sample)) if len(test_sample) > 0 else 0.0
        
        return {
            'rmse': rmse,
            'mae': mae,
            'coverage': coverage,
            'sample_size': len(test_sample),
            'error_count': error_count,
            'valid_predictions': len(true_ratings)
        }
    
    def evaluate_single_model(self, model_name: str, model) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        logger.info(f"å¼€å§‹å®Œæ•´è¯„ä¼°: {model_name}")
        start_time = time.time()
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½
        if self.test_data is None:
            logger.error("æµ‹è¯•æ•°æ®æœªåŠ è½½")
            return {}
        
        # è®¡ç®—è¯„åˆ†é¢„æµ‹æŒ‡æ ‡
        logger.info("  è®¡ç®—è¯„åˆ†é¢„æµ‹æŒ‡æ ‡...")
        rating_metrics = self.calculate_rating_metrics(model, self.test_data)
        
        # ç”Ÿæˆæ¨èå¹¶è®¡ç®—æ’åºæŒ‡æ ‡
        logger.info("  ç”Ÿæˆæ¨èå¹¶è®¡ç®—æ’åºæŒ‡æ ‡...")
        test_users = list(self.test_user_items.keys())
        recommendations = self.generate_recommendations(model, test_users)
        
        # è®¡ç®—æ’åºæŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå†…ç½®å®ç°ï¼‰
        ranking_metrics = {}
        if recommendations:
            for k in [5, 10, 20]:
                try:
                    recall = self.calculate_recall_at_k(recommendations, k)
                    precision = self.calculate_precision_at_k(recommendations, k)
                    hit_rate = self.calculate_hit_rate_at_k(recommendations, k)
                    ndcg = self.calculate_ndcg_at_k(recommendations, k)
                    f1 = (2 * recall * precision) / (recall + precision) if (recall + precision) > 0 else 0.0
                    
                    ranking_metrics.update({
                        f'Recall@{k}': float(recall),
                        f'Precision@{k}': float(precision),
                        f'HitRate@{k}': float(hit_rate),
                        f'NDCG@{k}': float(ndcg),
                        f'F1@{k}': float(f1)
                    })
                except Exception as e:
                    logger.debug(f"è®¡ç®—@{k}æŒ‡æ ‡å¤±è´¥: {str(e)}")
                    ranking_metrics.update({
                        f'Recall@{k}': 0.0,
                        f'Precision@{k}': 0.0,
                        f'HitRate@{k}': 0.0,
                        f'NDCG@{k}': 0.0,
                        f'F1@{k}': 0.0
                    })
            
            # è®¡ç®—é«˜çº§æŒ‡æ ‡
            try:
                map_score = self.calculate_map(recommendations)
                mrr_score = self.calculate_mrr(recommendations)
                coverage = self.calculate_coverage(recommendations)
                diversity = self.calculate_diversity(recommendations)
                novelty = self.calculate_novelty(recommendations)
                user_coverage = len(recommendations) / len(test_users) if test_users else 0.0
                
                ranking_metrics.update({
                    'MAP': float(map_score),
                    'MRR': float(mrr_score),
                    'Coverage': float(coverage),
                    'Diversity': float(diversity),
                    'Novelty': float(novelty),
                    'User_Coverage': float(user_coverage)
                })
            except Exception as e:
                logger.debug(f"è®¡ç®—é«˜çº§æŒ‡æ ‡å¤±è´¥: {str(e)}")
                ranking_metrics.update({
                    'MAP': 0.0, 'MRR': 0.0, 'Coverage': 0.0,
                    'Diversity': 0.0, 'Novelty': 0.0, 'User_Coverage': 0.0
                })
        else:
            # å¦‚æœæ²¡æœ‰æ¨èï¼Œæ‰€æœ‰æ’åºæŒ‡æ ‡ä¸º0
            for k in [5, 10, 20]:
                ranking_metrics.update({
                    f'Recall@{k}': 0.0, f'Precision@{k}': 0.0, f'HitRate@{k}': 0.0,
                    f'NDCG@{k}': 0.0, f'F1@{k}': 0.0
                })
            ranking_metrics.update({
                'MAP': 0.0, 'MRR': 0.0, 'Coverage': 0.0,
                'Diversity': 0.0, 'Novelty': 0.0, 'User_Coverage': 0.0
            })
        
        evaluation_time = time.time() - start_time
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        result = {
            **rating_metrics,
            **ranking_metrics,
            'evaluation_time': evaluation_time
        }
        
        logger.info(f"âœ“ {model_name} è¯„ä¼°å®Œæˆ ({evaluation_time:.1f}ç§’)")
        return result
    
    def calculate_recall_at_k(self, recommendations: Dict[int, List[int]], k: int) -> float:
        """è®¡ç®—Recall@K"""
        total_recall = 0.0
        user_count = 0
        
        for user_id, rec_items in recommendations.items():
            if user_id not in self.test_user_items:
                continue
                
            true_items = self.test_user_items[user_id]
            rec_items_k = rec_items[:k]
            
            if len(true_items) > 0:
                hits = len(set(rec_items_k) & true_items)
                recall = hits / len(true_items)
                total_recall += recall
                user_count += 1
        
        return total_recall / user_count if user_count > 0 else 0.0
    
    def calculate_precision_at_k(self, recommendations: Dict[int, List[int]], k: int) -> float:
        """è®¡ç®—Precision@K"""
        total_precision = 0.0
        user_count = 0
        
        for user_id, rec_items in recommendations.items():
            if user_id not in self.test_user_items:
                continue
                
            true_items = self.test_user_items[user_id]
            rec_items_k = rec_items[:k]
            
            if len(rec_items_k) > 0:
                hits = len(set(rec_items_k) & true_items)
                precision = hits / len(rec_items_k)
                total_precision += precision
                user_count += 1
        
        return total_precision / user_count if user_count > 0 else 0.0
    
    def calculate_hit_rate_at_k(self, recommendations: Dict[int, List[int]], k: int) -> float:
        """è®¡ç®—Hit Rate@K"""
        hits = 0
        total_users = 0
        
        for user_id, rec_items in recommendations.items():
            if user_id not in self.test_user_items:
                continue
                
            true_items = self.test_user_items[user_id]
            rec_items_k = rec_items[:k]
            
            if len(set(rec_items_k) & true_items) > 0:
                hits += 1
            total_users += 1
        
        return hits / total_users if total_users > 0 else 0.0
    
    def calculate_ndcg_at_k(self, recommendations: Dict[int, List[int]], k: int) -> float:
        """è®¡ç®—NDCG@K"""
        total_ndcg = 0.0
        user_count = 0
        
        for user_id, rec_items in recommendations.items():
            if user_id not in self.test_user_items:
                continue
                
            true_items = self.test_user_items[user_id]
            rec_items_k = rec_items[:k]
            
            # è®¡ç®—DCG
            dcg = 0.0
            for i, item_id in enumerate(rec_items_k):
                if item_id in true_items:
                    dcg += 1.0 / np.log2(i + 2)
            
            # è®¡ç®—IDCG (ç†æƒ³æƒ…å†µä¸‹çš„DCG)
            idcg = 0.0
            for i in range(min(len(true_items), k)):
                idcg += 1.0 / np.log2(i + 2)
            
            # è®¡ç®—NDCG
            if idcg > 0:
                ndcg = dcg / idcg
                total_ndcg += ndcg
                user_count += 1
        
        return total_ndcg / user_count if user_count > 0 else 0.0
    
    def calculate_map(self, recommendations: Dict[int, List[int]]) -> float:
        """è®¡ç®—MAP (Mean Average Precision)"""
        total_ap = 0.0
        user_count = 0
        
        for user_id, rec_items in recommendations.items():
            if user_id not in self.test_user_items:
                continue
                
            true_items = self.test_user_items[user_id]
            
            # è®¡ç®—AP
            ap = 0.0
            hits = 0
            for i, item_id in enumerate(rec_items):
                if item_id in true_items:
                    hits += 1
                    precision_at_i = hits / (i + 1)
                    ap += precision_at_i
            
            if len(true_items) > 0:
                ap /= len(true_items)
                total_ap += ap
                user_count += 1
        
        return total_ap / user_count if user_count > 0 else 0.0
    
    def calculate_mrr(self, recommendations: Dict[int, List[int]]) -> float:
        """è®¡ç®—MRR (Mean Reciprocal Rank)"""
        total_rr = 0.0
        user_count = 0
        
        for user_id, rec_items in recommendations.items():
            if user_id not in self.test_user_items:
                continue
                
            true_items = self.test_user_items[user_id]
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå‘½ä¸­çš„ä½ç½®
            for i, item_id in enumerate(rec_items):
                if item_id in true_items:
                    rr = 1.0 / (i + 1)
                    total_rr += rr
                    break
            
            user_count += 1
        
        return total_rr / user_count if user_count > 0 else 0.0
    
    def calculate_coverage(self, recommendations: Dict[int, List[int]]) -> float:
        """è®¡ç®—ç‰©å“è¦†ç›–ç‡"""
        all_recommended_items = set()
        for rec_items in recommendations.values():
            all_recommended_items.update(rec_items)
        
        return len(all_recommended_items) / self.total_items if self.total_items > 0 else 0.0
    
    def calculate_diversity(self, recommendations: Dict[int, List[int]], top_k: int = 10) -> float:
        """è®¡ç®—æ¨èå¤šæ ·æ€§"""
        diversities = []
        
        for user_id, rec_items in recommendations.items():
            rec_items_k = rec_items[:top_k]
            n_items = len(rec_items_k)
            
            if n_items < 2:
                diversities.append(0.0)
                continue
            
            # è®¡ç®—ç‰©å“é—´çš„å¹³å‡è·ç¦»ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºä¸åŒç‰©å“çš„æ¯”ä¾‹ï¼‰
            unique_items = len(set(rec_items_k))
            diversity = unique_items / n_items if n_items > 0 else 0.0
            diversities.append(diversity)
        
        return float(np.mean(diversities)) if diversities else 0.0
    
    def calculate_novelty(self, recommendations: Dict[int, List[int]]) -> float:
        """è®¡ç®—æ¨èæ–°é¢–æ€§ï¼ˆåŸºäºç‰©å“æµè¡Œåº¦ï¼‰"""
        if self.train_data is None:
            return 0.0
        
        # è®¡ç®—ç‰©å“æµè¡Œåº¦
        item_popularity = self.train_data['item_id'].value_counts()
        total_interactions = len(self.train_data)
        
        novelties = []
        for user_id, rec_items in recommendations.items():
            user_novelty = 0.0
            for item_id in rec_items:
                # è®¡ç®—ç‰©å“çš„æµè¡Œåº¦
                popularity = item_popularity.get(item_id, 0) / total_interactions
                # æ–°é¢–æ€§ = -log(æµè¡Œåº¦)ï¼Œæµè¡Œåº¦è¶Šä½æ–°é¢–æ€§è¶Šé«˜
                if popularity > 0:
                    novelty = -np.log2(popularity)
                else:
                    novelty = 10.0  # ç»™æœªè§è¿‡çš„ç‰©å“é«˜æ–°é¢–æ€§
                user_novelty += novelty
            
            avg_novelty = user_novelty / len(rec_items) if rec_items else 0.0
            novelties.append(avg_novelty)
        
        return float(np.mean(novelties)) if novelties else 0.0
    
    def run_complete_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        logger.info("å¼€å§‹å®Œæ•´æ¨èç³»ç»Ÿè¯„ä¼°...")
        
        # åŠ è½½æ•°æ®
        self.load_data()
        
        # åŠ è½½æ¨¡å‹
        logger.info("å¼€å§‹å®Œæ•´æ¨¡å‹è¯„ä¼°...")
        models = self.load_models()
        
        if not models:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
            return
        
        logger.info("=" * 80)
        logger.info("å¼€å§‹å®Œæ•´æ¨èç³»ç»Ÿè¯„ä¼°")
        logger.info("=" * 80)
        
        # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
        for model_name, model in models.items():
            logger.info("")
            logger.info("=" * 60)
            logger.info(f"è¯„ä¼°æ¨¡å‹: {model_name}")
            logger.info("=" * 60)
            
            try:
                result = self.evaluate_single_model(model_name, model)
                self.results[model_name] = result
                
                # å­˜å‚¨å¯è§†åŒ–æ•°æ®
                self.visualization_data[model_name] = result
                
            except Exception as e:
                logger.error(f"è¯„ä¼°æ¨¡å‹ {model_name} å¤±è´¥: {str(e)}")
                # æ·»åŠ ç©ºç»“æœé¿å…åç»­å¤„ç†å‡ºé”™
                self.results[model_name] = {
                    'rmse': 0.0, 'mae': 0.0, 'coverage': 0.0,
                    'Recall@10': 0.0, 'Precision@10': 0.0, 'NDCG@10': 0.0,
                    'evaluation_time': 0.0, 'error': str(e)
                }
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.generate_visualizations()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
    
    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜JSONæ ¼å¼
        json_file = self.output_dir / "complete_evaluation_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼
        csv_file = self.output_dir / "complete_evaluation_results.csv"
        df = pd.DataFrame.from_dict(self.results, orient='index')
        df.index.name = 'Model'
        df.to_csv(csv_file)
        
        logger.info(f"å®Œæ•´è¯„ä¼°ç»“æœå·²ä¿å­˜: {json_file}, {csv_file}")
    
    def generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        logger.info("ç”Ÿæˆç»¼åˆå¯è§†åŒ–æŠ¥å‘Š...")
        
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
            plt.rcParams['axes.unicode_minus'] = False
            sns.set_style("whitegrid")
            
            # å‡†å¤‡æ•°æ®
            df = pd.DataFrame.from_dict(self.results, orient='index')
            
            # 1. ç»¼åˆæ€§èƒ½å¯¹æ¯”
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            
            # RMSEå¯¹æ¯”
            models = df.index.tolist()
            rmse_values = df['rmse'].values
            bars1 = ax1.bar(models, rmse_values, color='skyblue', alpha=0.7)
            ax1.set_title('RMSE Comparison', fontsize=14, fontweight='bold')
            ax1.set_ylabel('RMSE')
            ax1.tick_params(axis='x', rotation=45)
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars1, rmse_values):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{val:.3f}', ha='center', va='bottom')
            
            # Recall@10å¯¹æ¯”
            recall_values = df['Recall@10'].values
            bars2 = ax2.bar(models, recall_values, color='lightcoral', alpha=0.7)
            ax2.set_title('Recall@10 Comparison', fontsize=14, fontweight='bold')
            ax2.set_ylabel('Recall@10')
            ax2.tick_params(axis='x', rotation=45)
            for bar, val in zip(bars2, recall_values):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                        f'{val:.3f}', ha='center', va='bottom')
            
            # NDCG@10å¯¹æ¯”
            ndcg_values = df['NDCG@10'].values
            bars3 = ax3.bar(models, ndcg_values, color='lightgreen', alpha=0.7)
            ax3.set_title('NDCG@10 Comparison', fontsize=14, fontweight='bold')
            ax3.set_ylabel('NDCG@10')
            ax3.tick_params(axis='x', rotation=45)
            for bar, val in zip(bars3, ndcg_values):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                        f'{val:.3f}', ha='center', va='bottom')
            
            # è¯„ä¼°æ—¶é—´å¯¹æ¯”
            time_values = df['evaluation_time'].values
            bars4 = ax4.bar(models, time_values, color='gold', alpha=0.7)
            ax4.set_title('Evaluation Time Comparison', fontsize=14, fontweight='bold')
            ax4.set_ylabel('Time (seconds)')
            ax4.tick_params(axis='x', rotation=45)
            for bar, val in zip(bars4, time_values):
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{val:.1f}s', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / "fixed_comprehensive_performance.png", 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            # 2. æ’åºæŒ‡æ ‡çƒ­åŠ›å›¾
            fig, ax = plt.subplots(figsize=(12, 8))
            ranking_cols = [col for col in df.columns if any(k in col for k in ['Recall', 'Precision', 'NDCG', 'F1'])]
            heatmap_data = df[ranking_cols].T
            
            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                       cbar_kws={'label': 'Score'}, ax=ax)
            ax.set_title('Ranking Metrics Heatmap', fontsize=16, fontweight='bold')
            ax.set_xlabel('Models')
            ax.set_ylabel('Metrics')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / "fixed_ranking_metrics_heatmap.png",
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ç»¼åˆå¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir}")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {str(e)}")
    
    def print_summary(self):
        """æ‰“å°è¯„ä¼°æ€»ç»“"""
        logger.info("")
        logger.info("=" * 80)
        logger.info("å®Œæ•´è¯„ä¼°æ€»ç»“")
        logger.info("=" * 80)
        
        for model_name, result in self.results.items():
            logger.info(f"\n{model_name}:")
            logger.info(f"  è¯„åˆ†é¢„æµ‹ - RMSE: {result.get('rmse', 0.0):.4f}, MAE: {result.get('mae', 0.0):.4f}")
            logger.info(f"  æ’åºè´¨é‡ - Recall@10: {result.get('Recall@10', 0.0):.4f}, Precision@10: {result.get('Precision@10', 0.0):.4f}")
            logger.info(f"  é«˜çº§æŒ‡æ ‡ - NDCG@10: {result.get('NDCG@10', 0.0):.4f}, MAP: {result.get('MAP', 0.0):.4f}")
            logger.info(f"  è¯„ä¼°æ—¶é—´: {result.get('evaluation_time', 0.0):.1f}ç§’")
        
        logger.info("\nâœ… å®Œæ•´è¯„ä¼°å®Œæˆï¼")
        logger.info(f"ğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜åœ¨ {self.output_dir}/ ç›®å½•")


def main():
    """ä¸»å‡½æ•°"""
    evaluator = FixedCompleteEvaluator()
    evaluator.run_complete_evaluation()


if __name__ == "__main__":
    main()
