#!/usr/bin/env python3
"""
å®Œæ•´çš„6ä¸ªTeacheræ¨¡å‹CUDAå®éªŒ - æœ€ç»ˆä¿®å¤ç‰ˆæœ¬
ç¡®ä¿æ‰€æœ‰æ¨¡å‹éƒ½èƒ½æ­£ç¡®ç”Ÿæˆæ¨èå¹¶å‚ä¸ä¸€è‡´æ€§åˆ†æ
åŒ…å«æœ€æ–°çš„é”™è¯¯ä¿®å¤å’Œæ¥å£ç»Ÿä¸€
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import time
import logging
from typing import Dict, List
from collections import defaultdict
import traceback

# CUDAä¼˜åŒ–è®¾ç½®
import torch
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

from models import create_recommender

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import time
import logging
from typing import Dict, List
from collections import defaultdict
import traceback

# CUDAä¼˜åŒ–è®¾ç½®
import torch
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

from models import create_recommender

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def simple_jaccard_similarity(set1: set, set2: set) -> float:
    """è®¡ç®—Jaccardç›¸ä¼¼åº¦"""
    if not set1 and not set2:
        return 1.0
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    return intersection / union if union > 0 else 0.0


def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥CUDAç¯å¢ƒ...")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   è®¾å¤‡ {i}: {torch.cuda.get_device_name(i)}")
            print(f"   æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
        
        # è®¾ç½®æ˜¾å­˜åˆ†é…ç­–ç•¥
        torch.cuda.empty_cache()
        
        # æµ‹è¯•CUDAåŸºæœ¬æ“ä½œ
        try:
            test_tensor = torch.randn(100, 100).cuda()
            result = torch.mm(test_tensor, test_tensor.t())
            del test_tensor, result
            torch.cuda.empty_cache()
            print("   âœ… CUDAåŸºæœ¬æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"   âŒ CUDAæµ‹è¯•å¤±è´¥: {e}")
            return False
    else:
        print("   âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        return False
    
    return True


def run_complete_teacher_experiment():
    """è¿è¡Œå®Œæ•´çš„6ä¸ªTeacheræ¨¡å‹å®éªŒ"""
    print("ğŸ“ å®Œæ•´6ä¸ªTeacheræ¨¡å‹CUDAå®éªŒ")
    print("=" * 60)
    
    # æ£€æŸ¥CUDAç¯å¢ƒ
    cuda_available = check_cuda_environment()
    device = torch.device('cuda' if cuda_available else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®
    print("\\nğŸ“Š åˆ›å»ºæ¨¡æ‹Ÿæ¨èæ•°æ®...")
    np.random.seed(42)
    
    # å¢å¤§æ•°æ®é›†ä»¥æ›´å¥½åœ°åˆ©ç”¨GPU
    num_interactions = 8000 if cuda_available else 3000
    num_users = 300 if cuda_available else 200
    num_items = 250 if cuda_available else 150
    
    interactions = []
    for _ in range(num_interactions):
        user_id = np.random.randint(1, num_users + 1)
        item_id = np.random.randint(1, num_items + 1)
        rating = np.random.choice([3, 4, 5], p=[0.3, 0.4, 0.3])
        timestamp = int(time.time()) + np.random.randint(-86400*30, 86400*30)
        
        interactions.append({
            'user_id': user_id,
            'item_id': item_id,
            'rating': rating,
            'timestamp': timestamp
        })
    
    train_data = pd.DataFrame(interactions)
    train_data = train_data.drop_duplicates(['user_id', 'item_id'])
    
    print(f"âœ… ç”Ÿæˆäº† {len(train_data)} æ¡å”¯ä¸€äº¤äº’æ•°æ®")
    print(f"   ç”¨æˆ·æ•°: {train_data['user_id'].nunique()}")
    print(f"   ç‰©å“æ•°: {train_data['item_id'].nunique()}")
    
    # 6ä¸ªTeacheræ¨¡å‹é…ç½® - CUDAä¼˜åŒ–ç‰ˆæœ¬
    algorithms = ['deepfm', 'autoint', 'transformer4rec', 'xdeepfm', 'din', 'dcnv2']
    
    if cuda_available:
        # CUDAä¼˜åŒ–é…ç½®
        model_configs = {
            'deepfm': {
                'embedding_dim': 32,
                'learning_rate': 0.001,
                'epochs': 20,
                'batch_size': 128,
                'early_stopping': True,
                'patience': 3
            },
            'autoint': {
                'embedding_dim': 32,
                'learning_rate': 0.001,
                'epochs': 20,
                'num_heads': 8,
                'num_layers': 4,
                'batch_size': 128,
                'early_stopping': True,
                'patience': 3
            },
            'transformer4rec': {
                'embedding_dim': 64,
                'num_heads': 8,
                'num_layers': 4,
                'learning_rate': 0.001,
                'epochs': 15,
                'batch_size': 128,
                'early_stopping': True,
                'patience': 3
            },
            'xdeepfm': {
                'embedding_dim': 32,
                'cin_layer_sizes': [128, 64],
                'dnn_hidden_dims': [256, 128],
                'learning_rate': 0.001,
                'epochs': 15,
                'batch_size': 128,
                'early_stopping': True,
                'patience': 3
            },
            'din': {
                'embedding_dim': 32,
                'hidden_dims': [256, 128, 64],
                'attention_hidden_dim': 128,
                'learning_rate': 0.001,
                'epochs': 15,
                'batch_size': 128,
                'early_stopping': True,
                'patience': 3
            },
            'dcnv2': {
                'embedding_dim': 32,
                'cross_layers': 4,
                'deep_layers': [256, 128, 64],
                'learning_rate': 0.001,
                'epochs': 15,
                'batch_size': 128,
                'early_stopping': True,
                'patience': 3
            }
        }
    else:
        # CPUä¼˜åŒ–é…ç½®
        model_configs = {
            'deepfm': {
                'embedding_dim': 16,
                'learning_rate': 0.001,
                'epochs': 12,
                'batch_size': 64,
                'early_stopping': False
            },
            'autoint': {
                'embedding_dim': 16,
                'learning_rate': 0.001,
                'epochs': 12,
                'num_heads': 4,
                'num_layers': 3,
                'early_stopping': False
            },
            'transformer4rec': {
                'embedding_dim': 32,
                'num_heads': 4,
                'num_layers': 3,
                'learning_rate': 0.001,
                'epochs': 10,
                'batch_size': 64,
                'early_stopping': False
            },
            'xdeepfm': {
                'embedding_dim': 16,
                'cin_layer_sizes': [64, 32],
                'dnn_hidden_dims': [128, 64],
                'learning_rate': 0.001,
                'epochs': 10,
                'batch_size': 64,
                'early_stopping': False
            },
            'din': {
                'embedding_dim': 16,
                'hidden_dims': [128, 64],
                'attention_hidden_dim': 64,
                'learning_rate': 0.001,
                'epochs': 10,
                'batch_size': 64,
                'early_stopping': False
            },
            'dcnv2': {
                'embedding_dim': 16,
                'cross_layers': 3,
                'deep_layers': [128, 64],
                'learning_rate': 0.001,
                'epochs': 10,
                'batch_size': 64,
                'early_stopping': False
            }
        }
    
    print(f"\\nğŸ¯ å°†è®­ç»ƒ {len(algorithms)} ä¸ªTeacheræ¨¡å‹:")
    for algo in algorithms:
        print(f"   - {algo}")
    
    print(f"   æ¨¡å¼: {'CUDAåŠ é€Ÿ' if cuda_available else 'CPUæ¨¡å¼'}")
    
    # è®­ç»ƒæ¨¡å‹
    trained_models = {}
    training_times = {}
    training_errors = {}
    
    for algo_name in algorithms:
        print(f"\\nğŸ“š è®­ç»ƒTeacheræ¨¡å‹: {algo_name}")
        config = model_configs[algo_name]
        print(f"   é…ç½®: embedding_dim={config.get('embedding_dim', 16)}, "
              f"epochs={config.get('epochs', 10)}, "
              f"batch_size={config.get('batch_size', 64)}")
        
        try:
            start_time = time.time()
            
            # æ¸…ç†GPUå†…å­˜
            if cuda_available:
                torch.cuda.empty_cache()
            
            model = create_recommender(algo_name, **config)
            
            if model is None:
                print(f"âŒ æ— æ³•åˆ›å»ºç®—æ³•: {algo_name}")
                training_errors[algo_name] = "ç®—æ³•åˆ›å»ºå¤±è´¥"
                continue
            
            # è®¾ç½®è®¾å¤‡
            if hasattr(model, 'device'):
                model.device = device
            
            # è®­ç»ƒæ¨¡å‹
            try:
                model.fit(train_data, **config)
                end_time = time.time()
                
                training_time = end_time - start_time
                training_times[algo_name] = training_time
                
                if model.is_trained:
                    trained_models[algo_name] = model
                    print(f"âœ… {algo_name} è®­ç»ƒæˆåŠŸ! è€—æ—¶: {training_time:.2f}ç§’")
                    
                    # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                    if cuda_available:
                        memory_allocated = torch.cuda.memory_allocated() / 1024**3
                        memory_reserved = torch.cuda.memory_reserved() / 1024**3
                        print(f"   GPUå†…å­˜: å·²åˆ†é… {memory_allocated:.2f}GB, å·²ä¿ç•™ {memory_reserved:.2f}GB")
                else:
                    print(f"âŒ {algo_name} è®­ç»ƒå¤±è´¥ - æ¨¡å‹æœªå®Œæˆè®­ç»ƒ")
                    training_errors[algo_name] = "è®­ç»ƒæœªå®Œæˆ"
                    
            except RuntimeError as e:
                if "CUDA" in str(e) or "out of memory" in str(e):
                    print(f"âš ï¸ {algo_name} CUDAé”™è¯¯ï¼Œå°è¯•CPUæ¨¡å¼: {str(e)[:100]}...")
                    
                    # æ¸…ç†GPUå†…å­˜å¹¶åˆ‡æ¢åˆ°CPU
                    if cuda_available:
                        torch.cuda.empty_cache()
                    
                    # é‡æ–°åˆ›å»ºæ¨¡å‹å¹¶è®¾ç½®ä¸ºCPU
                    cpu_config = config.copy()
                    cpu_config['batch_size'] = min(cpu_config.get('batch_size', 64), 32)
                    cpu_config['epochs'] = min(cpu_config.get('epochs', 10), 8)
                    
                    model = create_recommender(algo_name, **cpu_config)
                    if hasattr(model, 'device'):
                        model.device = torch.device('cpu')
                    
                    model.fit(train_data, **cpu_config)
                    end_time = time.time()
                    
                    training_time = end_time - start_time
                    training_times[algo_name] = training_time
                    
                    if model.is_trained:
                        trained_models[algo_name] = model
                        print(f"âœ… {algo_name} CPUæ¨¡å¼è®­ç»ƒæˆåŠŸ! è€—æ—¶: {training_time:.2f}ç§’")
                    else:
                        print(f"âŒ {algo_name} CPUæ¨¡å¼ä¹Ÿå¤±è´¥")
                        training_errors[algo_name] = f"CUDAå’ŒCPUéƒ½å¤±è´¥: {str(e)[:50]}"
                else:
                    raise e
                    
        except Exception as e:
            training_time = time.time() - start_time
            training_times[algo_name] = training_time
            training_errors[algo_name] = f"{type(e).__name__}: {str(e)[:100]}"
            print(f"âŒ è®­ç»ƒ {algo_name} æ—¶å‡ºé”™: {str(e)[:100]}...")
            print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # æ¸…ç†GPUå†…å­˜
            if cuda_available:
                torch.cuda.empty_cache()
    
    print(f"\\nğŸ¯ è®­ç»ƒé˜¶æ®µå®Œæˆ!")
    print(f"   æˆåŠŸè®­ç»ƒ: {len(trained_models)} ä¸ªTeacheræ¨¡å‹")
    print(f"   æˆåŠŸçš„æ¨¡å‹: {list(trained_models.keys())}")
    
    if training_errors:
        print(f"   å¤±è´¥çš„æ¨¡å‹: {list(training_errors.keys())}")
        for model_name, error in training_errors.items():
            print(f"     {model_name}: {error}")
    
    if len(trained_models) < 2:
        print("âŒ è®­ç»ƒæˆåŠŸçš„æ¨¡å‹å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡Œä¸€è‡´æ€§åˆ†æ")
        return None
    
    # ç”Ÿæˆæ¨èå¹¶åˆ†æä¸€è‡´æ€§
    print("\\nğŸ“Š ç”Ÿæˆæ¨èå¹¶åˆ†æä¸€è‡´æ€§...")
    test_users = list(train_data['user_id'].unique())[:100 if cuda_available else 50]
    
    all_recommendations = {}
    performance_stats = {}
    
    for algo_name, model in trained_models.items():
        print(f"ğŸ” ç”Ÿæˆ {algo_name} çš„æ¨è...")
        
        recommendations = {}
        success_count = 0
        total_rec_count = 0
        error_count = 0
        
        for user_id in test_users:
            try:
                recs = model.get_user_recommendations(user_id, top_k=10)
                if recs and len(recs) > 0:
                    rec_items = [rec['item_id'] for rec in recs]
                    recommendations[user_id] = rec_items
                    success_count += 1
                    total_rec_count += len(rec_items)
            except Exception as e:
                error_count += 1
                if error_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"   âš ï¸ ç”¨æˆ·{user_id}æ¨èå¤±è´¥: {str(e)[:50]}")
                continue
        
        if recommendations:
            all_recommendations[algo_name] = recommendations
            performance_stats[algo_name] = {
                'success_rate': success_count / len(test_users),
                'avg_rec_length': total_rec_count / success_count if success_count > 0 else 0,
                'total_recommendations': len(recommendations),
                'training_time': training_times.get(algo_name, 0),
                'error_count': error_count
            }
            print(f"   âœ… æˆåŠŸä¸º {len(recommendations)} ä¸ªç”¨æˆ·ç”Ÿæˆæ¨è")
            print(f"   ğŸ“ˆ æˆåŠŸç‡: {success_count/len(test_users):.2%}")
            if error_count > 0:
                print(f"   âš ï¸ é”™è¯¯æ•°: {error_count}")
    
    # è®¡ç®—ä¸€è‡´æ€§çŸ©é˜µ
    print("\\nğŸ” è®¡ç®—Teacheræ¨¡å‹é—´ä¸€è‡´æ€§...")
    
    if len(all_recommendations) < 2:
        print("âŒ æ— æ³•è¿›è¡Œä¸€è‡´æ€§åˆ†æ - æ¨èç”Ÿæˆå¤±è´¥")
        return None
    
    algorithms_list = list(all_recommendations.keys())
    jaccard_matrix = {}
    
    print(f"ğŸ“Š åˆ†æ {len(algorithms_list)} ä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§:")
    
    for i, algo1 in enumerate(algorithms_list):
        jaccard_matrix[algo1] = {}
        
        for j, algo2 in enumerate(algorithms_list):
            if algo1 == algo2:
                jaccard_matrix[algo1][algo2] = 1.0
            else:
                similarities = []
                
                # æ‰¾åˆ°ä¸¤ä¸ªç®—æ³•éƒ½æœ‰æ¨èçš„ç”¨æˆ·
                common_users = set(all_recommendations[algo1].keys()) & set(all_recommendations[algo2].keys())
                
                for user_id in common_users:
                    recs1 = set(all_recommendations[algo1][user_id][:10])
                    recs2 = set(all_recommendations[algo2][user_id][:10])
                    
                    sim = simple_jaccard_similarity(recs1, recs2)
                    similarities.append(sim)
                
                avg_similarity = np.mean(similarities) if similarities else 0.0
                jaccard_matrix[algo1][algo2] = avg_similarity
        
        print(f"   âœ… å®Œæˆ {algo1} çš„ä¸€è‡´æ€§è®¡ç®—")
    
    # åˆ†æç»“æœ
    print("\\nğŸ“ˆ åˆ†æTeacheræ¨¡å‹ä¸€è‡´æ€§ç»“æœ...")
    
    min_jaccard = float('inf')
    max_jaccard = 0.0
    min_pair = None
    max_pair = None
    
    all_similarities = []
    
    for algo1 in algorithms_list:
        for algo2 in algorithms_list:
            if algo1 != algo2:
                similarity = jaccard_matrix[algo1][algo2]
                all_similarities.append(similarity)
                
                if similarity < min_jaccard:
                    min_jaccard = similarity
                    min_pair = (algo1, algo2)
                if similarity > max_jaccard:
                    max_jaccard = similarity
                    max_pair = (algo1, algo2)
    
    avg_jaccard = np.mean(all_similarities) if all_similarities else 0.0
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\\nğŸ“ ç”Ÿæˆå®Œæ•´Teacheræ¨¡å‹åˆ†ææŠ¥å‘Š...")
    
    report = []
    report.append("# ğŸ“ å®Œæ•´6ä¸ªTeacheræ¨¡å‹ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š")
    report.append(f"**å®éªŒæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"**è¿è¡Œè®¾å¤‡**: {device}")
    report.append("")
    
    report.append("## ğŸ“Š å®éªŒæ¦‚è¿°")
    report.append(f"- **ç›®æ ‡Teacheræ¨¡å‹**: {', '.join(algorithms)}")
    report.append(f"- **æˆåŠŸè®­ç»ƒæ¨¡å‹**: {', '.join(trained_models.keys())}")
    report.append(f"- **å¤±è´¥æ¨¡å‹**: {', '.join(training_errors.keys()) if training_errors else 'æ— '}")
    report.append(f"- **æ•°æ®é›†è§„æ¨¡**: {len(train_data)} æ¡äº¤äº’")
    report.append(f"- **æµ‹è¯•ç”¨æˆ·æ•°**: {len(test_users)}")
    report.append(f"- **è¿è¡Œæ¨¡å¼**: {'CUDAåŠ é€Ÿ' if cuda_available else 'CPUæ¨¡å¼'}")
    report.append("")
    
    # è®­ç»ƒæ€§èƒ½
    report.append("## â±ï¸ Teacheræ¨¡å‹è®­ç»ƒæ€§èƒ½")
    report.append("")
    report.append("| æ¨¡å‹ | è®­ç»ƒæ—¶é—´(ç§’) | çŠ¶æ€ | å¤‡æ³¨ |")
    report.append("|------|-------------|------|------|")
    
    for algo in algorithms:
        if algo in trained_models:
            train_time = training_times.get(algo, 0)
            report.append(f"| {algo} | {train_time:.2f} | âœ… æˆåŠŸ | - |")
        else:
            error_msg = training_errors.get(algo, "æœªçŸ¥é”™è¯¯")
            report.append(f"| {algo} | - | âŒ å¤±è´¥ | {error_msg[:30]}... |")
    report.append("")
    
    # æ¨èæ€§èƒ½
    if performance_stats:
        report.append("## ğŸ† Teacheræ¨¡å‹æ¨èæ€§èƒ½")
        report.append("")
        report.append("| æ¨¡å‹ | æˆåŠŸç‡ | å¹³å‡æ¨èæ•° | æ€»æ¨èæ•° | è®­ç»ƒæ—¶é—´(ç§’) | é”™è¯¯æ•° |")
        report.append("|------|--------|------------|----------|-------------|--------|")
        
        for algo_name, stats in performance_stats.items():
            report.append(f"| {algo_name} | {stats['success_rate']:.2%} | {stats['avg_rec_length']:.1f} | {stats['total_recommendations']} | {stats['training_time']:.2f} | {stats['error_count']} |")
        report.append("")
    
    # ä¸€è‡´æ€§åˆ†æ
    if len(algorithms_list) >= 2:
        report.append("## ğŸ”„ Teacheræ¨¡å‹ä¸€è‡´æ€§åˆ†æ")
        report.append("")
        
        report.append("### ğŸ“Š å…³é”®æŒ‡æ ‡")
        report.append(f"- **å¹³å‡Jaccardç›¸ä¼¼åº¦**: {avg_jaccard:.4f}")
        if min_pair:
            report.append(f"- **æœ€å¼ºäº’è¡¥ç»„åˆ**: {min_pair[0]} + {min_pair[1]} (Jaccard={min_jaccard:.4f})")
        if max_pair:
            report.append(f"- **æœ€ç›¸ä¼¼ç»„åˆ**: {max_pair[0]} + {max_pair[1]} (Jaccard={max_jaccard:.4f})")
        report.append("")
        
        # Jaccardç›¸ä¼¼åº¦çŸ©é˜µ
        report.append("### ğŸ”¢ Jaccardç›¸ä¼¼åº¦çŸ©é˜µ")
        report.append("")
        
        header = "| ç®—æ³• |" + "".join([f" {algo} |" for algo in algorithms_list])
        separator = "|" + "".join(["------|" for _ in range(len(algorithms_list) + 1)])
        
        report.append(header)
        report.append(separator)
        
        for algo1 in algorithms_list:
            row = f"| **{algo1}** |"
            for algo2 in algorithms_list:
                if algo1 == algo2:
                    row += " 1.0000 |"
                else:
                    similarity = jaccard_matrix[algo1][algo2]
                    row += f" {similarity:.4f} |"
            report.append(row)
        
        report.append("")
        
        # Ensembleå»ºè®®
        report.append("## ğŸ¯ 6ä¸ªTeacheræ¨¡å‹Ensembleç­–ç•¥å»ºè®®")
        report.append("")
        
        if min_pair and max_pair:
            report.append("### ğŸ¥‡ æœ€ä½³Teacher Ensembleç»„åˆ")
            report.append("")
            report.append(f"**æœ€å¼ºäº’è¡¥ç­–ç•¥**: {min_pair[0]} + {min_pair[1]}")
            report.append(f"- **Jaccardç›¸ä¼¼åº¦**: {min_jaccard:.4f}")
            report.append(f"- **ç‰¹ç‚¹**: æ¨èé‡å åº¦æœ€ä½ï¼Œå¤šæ ·æ€§æœ€é«˜")
            report.append("")
            
            report.append(f"**æœ€å¼ºä¸€è‡´ç­–ç•¥**: {max_pair[0]} + {max_pair[1]}")
            report.append(f"- **Jaccardç›¸ä¼¼åº¦**: {max_jaccard:.4f}")  
            report.append(f"- **ç‰¹ç‚¹**: æ¨èé«˜åº¦ä¸€è‡´ï¼Œç¨³å®šæ€§æœ€é«˜")
            report.append("")
    
    # å¤±è´¥åˆ†æ
    if training_errors:
        report.append("## âŒ å¤±è´¥æ¨¡å‹åˆ†æ")
        report.append("")
        
        for model_name, error in training_errors.items():
            report.append(f"### {model_name}")
            report.append(f"- **é”™è¯¯**: {error}")
            
            if "CUDA" in error or "memory" in error:
                report.append("- **å»ºè®®**: GPUå†…å­˜ä¸è¶³ï¼Œå»ºè®®å‡å°batch_sizeæˆ–ä½¿ç”¨CPUæ¨¡å¼")
            elif "Target" in error and "out of bounds" in error:
                report.append("- **å»ºè®®**: æ ‡ç­¾ç´¢å¼•é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥æ•°æ®é¢„å¤„ç†")
            elif "tensor" in error and "dtype" in error:
                report.append("- **å»ºè®®**: æ•°æ®ç±»å‹ä¸åŒ¹é…ï¼Œéœ€è¦è½¬æ¢ä¸ºfloatç±»å‹")
            else:
                report.append("- **å»ºè®®**: æ£€æŸ¥æ¨¡å‹å®ç°å’Œå‚æ•°é…ç½®")
            
            report.append("")
    
    # å®æ–½å»ºè®®
    report.append("## ğŸš€ å®æ–½å»ºè®®")
    report.append("")
    
    successful_count = len(trained_models)
    total_count = len(algorithms)
    
    if successful_count == total_count:
        report.append("### âœ… å®Œç¾æ‰§è¡Œ")
        report.append("- æ‰€æœ‰6ä¸ªTeacheræ¨¡å‹éƒ½è®­ç»ƒæˆåŠŸ")
        report.append("- å¯ä»¥ä½¿ç”¨å®Œæ•´çš„ensembleç­–ç•¥")
        if min_pair:
            report.append(f"- æ¨èä½¿ç”¨æœ€å¼ºäº’è¡¥ç»„åˆ: {min_pair[0]} + {min_pair[1]}")
    elif successful_count >= 4:
        report.append("### ğŸ¯ è‰¯å¥½æ‰§è¡Œ")
        report.append(f"- {successful_count}/{total_count} ä¸ªTeacheræ¨¡å‹è®­ç»ƒæˆåŠŸ")
        report.append("- å¯ä»¥ä½¿ç”¨éƒ¨åˆ†ensembleç­–ç•¥")
        report.append("- å»ºè®®ä¿®å¤å¤±è´¥æ¨¡å‹ä»¥è·å¾—æ›´å¥½æ•ˆæœ")
    else:
        report.append("### âš ï¸ éƒ¨åˆ†æ‰§è¡Œ")
        report.append(f"- ä»… {successful_count}/{total_count} ä¸ªTeacheræ¨¡å‹è®­ç»ƒæˆåŠŸ")
        report.append("- å»ºè®®ä¼˜å…ˆä¿®å¤å¤±è´¥æ¨¡å‹")
        report.append("- å¯èƒ½éœ€è¦è°ƒæ•´ç¡¬ä»¶é…ç½®æˆ–å‚æ•°è®¾ç½®")
    
    report.append("")
    report.append("### CUDAä¼˜åŒ–å»ºè®®")
    if cuda_available:
        report.append("- âœ… CUDAç¯å¢ƒå¯ç”¨ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨GPUåŠ é€Ÿ")
        report.append("- ğŸ’¡ å¯ä»¥å°è¯•å¢å¤§batch_sizeå’Œembedding_dimæå‡æ•ˆæœ")
        report.append("- ğŸ”§ ç›‘æ§GPUå†…å­˜ä½¿ç”¨ï¼Œå¿…è¦æ—¶è°ƒæ•´å‚æ•°")
    else:
        report.append("- âš ï¸ å½“å‰ä½¿ç”¨CPUæ¨¡å¼ï¼Œå»ºè®®å‡çº§åˆ°GPUç¯å¢ƒ")
        report.append("- ğŸ’¡ CPUæ¨¡å¼ä¸‹å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å‚æ•°")
        report.append("- ğŸ”§ è€ƒè™‘ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿ")
    
    # ä¿å­˜æŠ¥å‘Š
    report_content = "\\n".join(report)
    
    with open("COMPLETE_TEACHER_MODEL_ANALYSIS.md", "w", encoding='utf-8') as f:
        f.write(report_content)
    
    print("âœ… å®Œæ•´Teacheræ¨¡å‹åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° COMPLETE_TEACHER_MODEL_ANALYSIS.md")
    
    # æ˜¾ç¤ºå…³é”®ç»“æœ
    print("\\nğŸ‰ 6ä¸ªTeacheræ¨¡å‹å®éªŒå®Œæˆï¼")
    print("\\nğŸ“Š å…³é”®ç»“æœ:")
    print(f"ğŸ“ˆ æˆåŠŸè®­ç»ƒçš„Teacheræ¨¡å‹ ({len(trained_models)}/{len(algorithms)}): {list(trained_models.keys())}")
    
    if training_errors:
        print(f"âŒ å¤±è´¥çš„æ¨¡å‹: {list(training_errors.keys())}")
    
    if avg_jaccard is not None:
        print(f"ğŸ“Š å¹³å‡Jaccardç›¸ä¼¼åº¦: {avg_jaccard:.4f}")
    
    if min_pair:
        print(f"ğŸ† æœ€ä½³äº’è¡¥ç»„åˆ: {min_pair[0]} + {min_pair[1]} (Jaccard={min_jaccard:.4f})")
    
    if max_pair:
        print(f"âš ï¸  æœ€ç›¸ä¼¼ç»„åˆ: {max_pair[0]} + {max_pair[1]} (Jaccard={max_jaccard:.4f})")
    
    # æ˜¾ç¤ºä¸€è‡´æ€§çŸ©é˜µ
    if len(algorithms_list) >= 2:
        print("\\nğŸ“‹ Jaccardç›¸ä¼¼åº¦çŸ©é˜µ:")
        print("     ", end="")
        for algo in algorithms_list:
            print(f"{algo:>12}", end="")
        print()
        
        for algo1 in algorithms_list:
            print(f"{algo1:>8}", end="")
            for algo2 in algorithms_list:
                if algo1 == algo2:
                    print(f"{'1.0000':>12}", end="")
                else:
                    print(f"{jaccard_matrix[algo1][algo2]:>12.4f}", end="")
            print()
    
    # æ¸…ç†GPUå†…å­˜
    if cuda_available:
        torch.cuda.empty_cache()
    
    return {
        'trained_models': list(trained_models.keys()),
        'failed_models': list(training_errors.keys()),
        'jaccard_matrix': jaccard_matrix if len(algorithms_list) >= 2 else None,
        'best_complementary': min_pair,
        'most_similar': max_pair,
        'avg_jaccard': avg_jaccard,
        'performance_stats': performance_stats,
        'device': str(device)
    }


if __name__ == "__main__":
    result = run_complete_teacher_experiment()
    if result:
        success_rate = len(result['trained_models']) / 6 * 100
        print(f"\\nâœ… å®éªŒå®Œæˆï¼Teacheræ¨¡å‹æˆåŠŸç‡: {success_rate:.1f}% ({len(result['trained_models'])}/6)")
        print(f"ğŸ”§ è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹ COMPLETE_TEACHER_MODEL_ANALYSIS.md")
    else:
        print(f"\\nâŒ å®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
